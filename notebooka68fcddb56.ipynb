{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79789198",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-12T15:35:40.036171Z",
     "iopub.status.busy": "2025-08-12T15:35:40.035936Z",
     "iopub.status.idle": "2025-08-12T15:35:47.728343Z",
     "shell.execute_reply": "2025-08-12T15:35:47.727697Z"
    },
    "papermill": {
     "duration": 7.702514,
     "end_time": "2025-08-12T15:35:47.729757",
     "exception": false,
     "start_time": "2025-08-12T15:35:40.027243",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "from tqdm.notebook import tqdm  # For progress bars\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c256da2",
   "metadata": {
    "papermill": {
     "duration": 0.006447,
     "end_time": "2025-08-12T15:35:47.743581",
     "exception": false,
     "start_time": "2025-08-12T15:35:47.737134",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4056fefd",
   "metadata": {
    "papermill": {
     "duration": 0.006886,
     "end_time": "2025-08-12T15:35:47.756974",
     "exception": false,
     "start_time": "2025-08-12T15:35:47.750088",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c2d93f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-12T15:35:47.770772Z",
     "iopub.status.busy": "2025-08-12T15:35:47.770419Z",
     "iopub.status.idle": "2025-08-12T15:35:47.792916Z",
     "shell.execute_reply": "2025-08-12T15:35:47.792050Z"
    },
    "papermill": {
     "duration": 0.030739,
     "end_time": "2025-08-12T15:35:47.793963",
     "exception": false,
     "start_time": "2025-08-12T15:35:47.763224",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/mta-toy-task-dataset/mta_train.txt already exists. Skipping generation.\n",
      "/kaggle/input/mta-toy-task-dataset/mta_test.txt already exists. Skipping generation.\n",
      "Dataset already exists. Loading a sample...\n",
      "Sample Sequence:\n",
      "olbud.xdyxs.cudki.lhwzg.pwhlo.rljxi.ozgfm.ijhhx.gowsd.gamrs.tstxh.gjxcx.qkwht.awtzm.jhctg.aqyxx.bxpyl.pxdks.ryciw.rarbd.lghyj.gdbcm.aeyen.fxdcl.bbhpj.gqnxc.tuhgc.bnieb.krypx.bmiev.yrphk.damen.yqifs.wmpdo.fhkun.dslla.ncfie.buysa.dddbq.srvth.gqugr.owgip.hieqz.zydvi.cfwde.ogbrg.zyerl.ynrca.mdwsf.qphya#tm\n",
      "Target Block: awtzm\n",
      "Query Letters: tm\n"
     ]
    }
   ],
   "source": [
    "num_blocks = 50\n",
    "N = 5 #text string in sequence\n",
    "L = 2 #query string\n",
    "N_options=[5, 8]\n",
    "num_sequences_train = 1000000\n",
    "num_sequences_test = 1000\n",
    "sequence_len = (num_blocks *N) + (num_blocks-1)\n",
    "def contains_letters(main_str, target_str, case_sensitive=False):\n",
    "    if not case_sensitive:\n",
    "        main_str = main_str.lower()\n",
    "        target_str = target_str.lower()\n",
    "    \n",
    "    main_counts = Counter(main_str)\n",
    "    target_counts = Counter(target_str)\n",
    "    \n",
    "    return all(main_counts[k] >= target_counts[k] for k in target_counts)\n",
    "\n",
    "def generate_sequence(num_blocks,N, N_options, L):\n",
    "    # Randomly choose block lengths (N) and generate blocks\n",
    "    if N is None:\n",
    "        N = random.choice(N_options)\n",
    "    blocks = []\n",
    "    target_block_idx = random.randint(0, num_blocks - 1)\n",
    "    target_block = ''.join(random.choices(string.ascii_lowercase, k=N))\n",
    "\n",
    "    while True:\n",
    "        target_block = ''.join(random.choices(string.ascii_lowercase, k=N))\n",
    "        unique_block_letters = list(set(target_block))\n",
    "        if len(unique_block_letters) >3:  # Ensure we get at least 3 unique letters\n",
    "            break\n",
    "    # Generate 2 unique query letters that MUST appear in the target block\n",
    "    while True:\n",
    "        query_letters = random.sample(target_block, min(L, len(target_block)))\n",
    "        # print(target_block)\n",
    "        unique_letters = list(set(query_letters))\n",
    "        if len(unique_letters) == L:  # Ensure we get exactly 2 letters\n",
    "            break\n",
    "\n",
    "    for i,_ in enumerate(range(num_blocks)):\n",
    "        if i == target_block_idx:\n",
    "            blocks.append(target_block)\n",
    "        else:\n",
    "            while True:\n",
    "                block = ''.join(random.choices(string.ascii_lowercase, k=N))\n",
    "                if not contains_letters(block, ''.join(query_letters)): \n",
    "                    break\n",
    "            blocks.append(block)\n",
    "\n",
    "\n",
    "    \n",
    "    # Join blocks into sequence and append query\n",
    "    sequence = '.'.join(blocks) + '#' + ''.join(query_letters)\n",
    "    return sequence, target_block_idx\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def generate_dataset(num_sequences, filepath, force=False):\n",
    "    if os.path.exists(filepath) and not force:\n",
    "        print(f\"{filepath} already exists. Skipping generation.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Generating {num_sequences} sequences to {filepath}...\")\n",
    "    with open(filepath, 'w') as f:\n",
    "        for _ in tqdm(range(num_sequences), desc=f\"Generating {filepath}\"):\n",
    "            seq, target = generate_sequence(num_blocks,N, N_options, L)\n",
    "            f.write(f\"{seq}|{target}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Only generate files if they don't exist\n",
    "path_folder = \"/kaggle/input/mta-toy-task-dataset/\"\n",
    "generate_dataset(num_sequences_train, f'{path_folder}mta_train.txt')\n",
    "generate_dataset(num_sequences_test, f'{path_folder}mta_test.txt')\n",
    "# Verify a sample\n",
    "if not os.path.exists(f'{path_folder}mta_train.txt'):\n",
    "    sample_seq, target_idx = generate_sequence()\n",
    "    print(f\"Sample Sequence:\\n{sample_seq}\")\n",
    "    print(f\"Target Block: {sample_seq.split('.')[target_idx]}\")\n",
    "    print(f\"Query Letters: {sample_seq.split('#')[-1]}\")\n",
    "else:\n",
    "    print(\"Dataset already exists. Loading a sample...\")\n",
    "    with open(f'{path_folder}mta_train.txt') as f:\n",
    "        first_line = f.readline().strip()\n",
    "        seq, target = first_line.split('|')\n",
    "        print(f\"Sample Sequence:\\n{seq}\")\n",
    "        print(f\"Target Block: {seq.split('.')[int(target)]}\")\n",
    "        print(f\"Query Letters: {seq.split('#')[-1]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "641b4358",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-12T15:35:47.807462Z",
     "iopub.status.busy": "2025-08-12T15:35:47.807268Z",
     "iopub.status.idle": "2025-08-12T15:35:47.812099Z",
     "shell.execute_reply": "2025-08-12T15:35:47.811534Z"
    },
    "papermill": {
     "duration": 0.012721,
     "end_time": "2025-08-12T15:35:47.813187",
     "exception": false,
     "start_time": "2025-08-12T15:35:47.800466",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'olbud.xdyxs.cudki.lhwzg.pwhlo.rljxi.ozgfm.ijhhx.gowsd.gamrs.tstxh.gjxcx.qkwht.awtzm.jhctg.aqyxx.bxpyl.pxdks.ryciw.rarbd.lghyj.gdbcm.aeyen.fxdcl.bbhpj.gqnxc.tuhgc.bnieb.krypx.bmiev.yrphk.damen.yqifs.wmpdo.fhkun.dslla.ncfie.buysa.dddbq.srvth.gqugr.owgip.hieqz.zydvi.cfwde.ogbrg.zyerl.ynrca.mdwsf.qphya'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blocks_part, query_letters = seq.split('#')\n",
    "blocks = blocks_part.split('.')\n",
    "\n",
    "blocks_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4255346b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-12T15:35:47.826777Z",
     "iopub.status.busy": "2025-08-12T15:35:47.826577Z",
     "iopub.status.idle": "2025-08-12T15:35:47.832074Z",
     "shell.execute_reply": "2025-08-12T15:35:47.831526Z"
    },
    "papermill": {
     "duration": 0.013496,
     "end_time": "2025-08-12T15:35:47.833094",
     "exception": false,
     "start_time": "2025-08-12T15:35:47.819598",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "# use only lowercase letters as the character set\n",
    "chars = list(string.ascii_lowercase)\n",
    "chars.append(\".\")\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "encode = lambda s: [stoi[c] for c in s if c in stoi]  # ignore characters not in vocab\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "vocab_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c807236",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-12T15:35:47.846968Z",
     "iopub.status.busy": "2025-08-12T15:35:47.846745Z",
     "iopub.status.idle": "2025-08-12T15:35:47.882643Z",
     "shell.execute_reply": "2025-08-12T15:35:47.881972Z"
    },
    "papermill": {
     "duration": 0.044109,
     "end_time": "2025-08-12T15:35:47.883782",
     "exception": false,
     "start_time": "2025-08-12T15:35:47.839673",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([14, 11,  1, 20,  3, 26, 23,  3, 24, 23, 18, 26,  2, 20,  3, 10,  8, 26,\n",
       "        11,  7, 22, 25,  6, 26, 15, 22,  7, 11, 14, 26, 17, 11,  9, 23,  8, 26,\n",
       "        14, 25,  6,  5, 12, 26,  8,  9,  7,  7, 23, 26,  6, 14, 22, 18,  3, 26,\n",
       "         6,  0, 12, 17, 18, 26, 19, 18, 19, 23,  7, 26,  6,  9, 23,  2, 23, 26,\n",
       "        16, 10, 22,  7, 19, 26,  0, 22, 19, 25, 12, 26,  9,  7,  2, 19,  6, 26,\n",
       "         0, 16, 24, 23, 23, 26,  1, 23, 15, 24, 11, 26, 15, 23,  3, 10, 18, 26,\n",
       "        17, 24,  2,  8, 22, 26, 17,  0, 17,  1,  3, 26, 11,  6,  7, 24,  9, 26,\n",
       "         6,  3,  1,  2, 12, 26,  0,  4, 24,  4, 13, 26,  5, 23,  3,  2, 11, 26,\n",
       "         1,  1,  7, 15,  9, 26,  6, 16, 13, 23,  2, 26, 19, 20,  7,  6,  2, 26,\n",
       "         1, 13,  8,  4,  1, 26, 10, 17, 24, 15, 23, 26,  1, 12,  8,  4, 21, 26,\n",
       "        24, 17, 15,  7, 10, 26,  3,  0, 12,  4, 13, 26, 24, 16,  8,  5, 18, 26,\n",
       "        22, 12, 15,  3, 14, 26,  5,  7, 10, 20, 13, 26,  3, 18, 11, 11,  0, 26,\n",
       "        13,  2,  5,  8,  4, 26,  1, 20, 24, 18,  0, 26,  3,  3,  3,  1, 16, 26,\n",
       "        18, 17, 21, 19,  7, 26,  6, 16, 20,  6, 17, 26, 14, 22,  6,  8, 15, 26,\n",
       "         7,  8,  4, 16, 25, 26, 25, 24,  3, 21,  8, 26,  2,  5, 22,  3,  4, 26,\n",
       "        14,  6,  1, 17,  6, 26, 25, 24,  4, 17, 11, 26, 24, 13, 17,  2,  0, 26,\n",
       "        12,  3, 22, 18,  5, 26, 16, 15,  7, 24,  0])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(encode(blocks_part))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91589227",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-12T15:35:47.897743Z",
     "iopub.status.busy": "2025-08-12T15:35:47.897548Z",
     "iopub.status.idle": "2025-08-12T15:35:47.902210Z",
     "shell.execute_reply": "2025-08-12T15:35:47.901531Z"
    },
    "papermill": {
     "duration": 0.01275,
     "end_time": "2025-08-12T15:35:47.903229",
     "exception": false,
     "start_time": "2025-08-12T15:35:47.890479",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([19, 12])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(encode(query_letters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54faa516",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-12T15:35:47.917286Z",
     "iopub.status.busy": "2025-08-12T15:35:47.917079Z",
     "iopub.status.idle": "2025-08-12T15:35:47.923127Z",
     "shell.execute_reply": "2025-08-12T15:35:47.922440Z"
    },
    "papermill": {
     "duration": 0.014268,
     "end_time": "2025-08-12T15:35:47.924175",
     "exception": false,
     "start_time": "2025-08-12T15:35:47.909907",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(22)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(encode(blocks[int(target)]))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "348085f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-12T15:35:47.938259Z",
     "iopub.status.busy": "2025-08-12T15:35:47.938055Z",
     "iopub.status.idle": "2025-08-12T15:35:47.943687Z",
     "shell.execute_reply": "2025-08-12T15:35:47.943202Z"
    },
    "papermill": {
     "duration": 0.013963,
     "end_time": "2025-08-12T15:35:47.944769",
     "exception": false,
     "start_time": "2025-08-12T15:35:47.930806",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MTADataset(Dataset):\n",
    "    def __init__(self, file_path, variant = \"all\" ):#, tokenizer, max_length=256):\n",
    "        # self.tokenizer = tokenizer\n",
    "        # self.max_length = max_length\n",
    "        self.variant = variant\n",
    "        with open(file_path, 'r') as f:\n",
    "            self.sequences = [line.strip() for line in f.readlines()]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    def __getitem__(self, idx):\n",
    "        sequence, target_block_idx = self.sequences[idx].split('|')\n",
    "        blocks_part, query_letters = sequence.split('#')\n",
    "        blocks = blocks_part.split('.')\n",
    "        \n",
    "        # Find target block (contains all query letters)\n",
    "        target_block = torch.tensor(encode(blocks[int(target_block_idx)]))\n",
    "        if self.variant == \"all\":\n",
    "            target = target_block\n",
    "        elif self.variant == \"first\":\n",
    "            target = target_block[0]\n",
    "        elif self.variant == \"last\":\n",
    "            target = target_block[-1]\n",
    "        elif self.variant == \"position\":\n",
    "            target = torch.tensor(int(target_block_idx))\n",
    "\n",
    "        else:\n",
    "            raise f\"{self.variant} is not recognized as a supported variant type\"\n",
    "            \n",
    "        \n",
    "        # # Tokenize blocks and query SEPARATELY\n",
    "        blocks_tokens = torch.tensor(encode(blocks_part))\n",
    "        query_tokens = torch.tensor(encode(query_letters))\n",
    "\n",
    "        \n",
    "        return {\n",
    "            'blocks': blocks_tokens,\n",
    "            'query': query_tokens,\n",
    "            'target': target\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f26d9382",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-12T15:35:47.959208Z",
     "iopub.status.busy": "2025-08-12T15:35:47.958715Z",
     "iopub.status.idle": "2025-08-12T15:35:47.961826Z",
     "shell.execute_reply": "2025-08-12T15:35:47.961294Z"
    },
    "papermill": {
     "duration": 0.011317,
     "end_time": "2025-08-12T15:35:47.962764",
     "exception": false,
     "start_time": "2025-08-12T15:35:47.951447",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# i = 0\n",
    "# for batch in train_dataloader:\n",
    "#     blocks = batch['blocks']\n",
    "#     queries = batch['query']\n",
    "#     targets = batch['target']\n",
    "#     print(f\"blocks: {blocks.shape}\")\n",
    "#     print(f\"queries: {queries.shape}\")\n",
    "#     print(f\"targets: {targets.shape}\")\n",
    "#     i+=1\n",
    "#     if i == 5:\n",
    "#         break\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844e8aea",
   "metadata": {
    "papermill": {
     "duration": 0.006611,
     "end_time": "2025-08-12T15:35:47.976136",
     "exception": false,
     "start_time": "2025-08-12T15:35:47.969525",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59dcb35a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-12T15:35:47.990332Z",
     "iopub.status.busy": "2025-08-12T15:35:47.990141Z",
     "iopub.status.idle": "2025-08-12T15:35:47.999059Z",
     "shell.execute_reply": "2025-08-12T15:35:47.998577Z"
    },
    "papermill": {
     "duration": 0.017353,
     "end_time": "2025-08-12T15:35:48.000247",
     "exception": false,
     "start_time": "2025-08-12T15:35:47.982894",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "\n",
    "class MultiTokenAttention(nn.Module):\n",
    "    def __init__(self,n_heads, head_size):\n",
    "        self.n_heads = n_heads\n",
    "        self.head_size = head_size\n",
    "        super().__init__()\n",
    "        # Projections for blocks (keys/values) and queries (letters)\n",
    "        self.block_proj = nn.Linear(n_embd, n_embd * 2)  # K, V\n",
    "        self.query_proj = nn.Linear(n_embd, n_embd)      # Q\n",
    "        \n",
    "        # Key-Query Convolution\n",
    "        self.conv2d = nn.Conv2d(\n",
    "            in_channels=n_heads,\n",
    "            out_channels=n_heads,\n",
    "            kernel_size=(c_q, c_k),\n",
    "            padding= \"same\",#((c_q-1)//2, (c_k-1)//2),\n",
    "            # groups=n_heads\n",
    "        )\n",
    "        \n",
    "        # Head Mixing\n",
    "        self.head_conv = nn.Conv2d(\n",
    "            in_channels=n_heads,\n",
    "            out_channels=n_heads,\n",
    "            padding = \"same\",# ((c_h-1)//2, (c_h-1)//2),\n",
    "            kernel_size=c_h,\n",
    "            groups=n_heads // c_h\n",
    "        )\n",
    "        \n",
    "        # Output\n",
    "        self.group_norm = nn.GroupNorm(n_heads, n_embd)\n",
    "        self.gate = nn.Linear(n_embd, n_heads)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        # self.register_buffer('tril', torch.tril(torch.ones(L, block_size)))\n",
    "\n",
    "\n",
    "    def forward(self, blocks, queries):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            blocks:  (batch, block_len, n_embd)  # All blocks concatenated\n",
    "            queries: (batch, query_len, n_embd)  # Query letters\n",
    "        \"\"\"\n",
    "        # batch_size = blocks.size(0)\n",
    "        B,T,C = blocks.shape\n",
    "        _,Q_len,_ = queries.shape\n",
    "        \n",
    "        # 1. Project blocks to K/V and queries to Q\n",
    "        K, V = self.block_proj(blocks).chunk(2, dim=-1)  # Each (batch, block_len, n_embd)\n",
    "        Q = self.query_proj(queries)                     # (batch, query_len, n_embd)\n",
    "\n",
    "        \n",
    "        # 2. Split into heads\n",
    "        Q = Q.view(B, -1, self.n_heads, self.head_size).transpose(1, 2)  # [B, h, q_len, d]\n",
    "        K = K.view(B, -1, self.n_heads, self.head_size).transpose(1, 2)  # [B, h, b_len, d]\n",
    "        V = V.view(B, -1, self.n_heads, self.head_size).transpose(1, 2)  # [B, h, b_len, d]\n",
    "\n",
    "        \n",
    "        # 3. Attention scores between queries and blocks\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) *(self.head_size **-0.5)  # [B, h, q_len, b_len]\n",
    "        # Ensure q_len >= c_q (e.g., pad queries if needed)\n",
    "\n",
    "\n",
    "        # 4. Key-Query Convolution (mixes attention across nearby queries/blocks)\n",
    "        attn_scores = self.conv2d(attn_scores)  # [B, h, q_len, b_len]\n",
    "\n",
    "\n",
    "        \n",
    "        # attn_scores = attn_scores.masked_fill(self.tril[:Q_len, :T] == 0, float('-inf'))    \n",
    "        # attn_scores = attn_scores.masked_fill(mask == 0, float('-inf'))        \n",
    "\n",
    "\n",
    "        \n",
    "        # 5. Softmax + Head Mixing\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "        attn_weights = self.head_conv(attn_weights)  # [B, h, q_len, b_len]\n",
    "\n",
    "        \n",
    "        # 6. Weighted sum of block values\n",
    "        output = torch.matmul(attn_weights, V)  # [B, h, q_len, d]\n",
    "        output = output.transpose(1, 2).reshape(B, -1, self.n_heads * self.head_size)\n",
    "        # output = self.head_conv(output)  # [B, q_len, C]\n",
    "\n",
    "        \n",
    "        # 7. GroupNorm + Gating\n",
    "        output = output.transpose(1, 2)  # [B, C, seq_len]\n",
    "        output = self.group_norm(output)\n",
    "        output = output.transpose(1, 2)  # Back to [B, seq_len, C]\n",
    "        gates = torch.sigmoid(self.gate(output))  # [B, q_len, h]\n",
    "        gates = gates.unsqueeze(-1).expand(-1, -1, -1, self.head_size).reshape_as(output)\n",
    "        output = output * gates\n",
    "        output = self.dropout(self.proj(output))\n",
    "        \n",
    "        return output  # (batch, query_len, n_embd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad37db50",
   "metadata": {
    "papermill": {
     "duration": 0.006748,
     "end_time": "2025-08-12T15:35:48.013994",
     "exception": false,
     "start_time": "2025-08-12T15:35:48.007246",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74db74bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-12T15:35:48.028659Z",
     "iopub.status.busy": "2025-08-12T15:35:48.028456Z",
     "iopub.status.idle": "2025-08-12T16:34:28.321021Z",
     "shell.execute_reply": "2025-08-12T16:34:28.319751Z"
    },
    "papermill": {
     "duration": 3520.301524,
     "end_time": "2025-08-12T16:34:28.322221",
     "exception": false,
     "start_time": "2025-08-12T15:35:48.020697",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.943551 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8575f76204249b38e288850035918d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py:549: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at /pytorch/aten/src/ATen/native/Convolution.cpp:1036.)\n",
      "  return F.conv2d(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 3.3232, test loss 3.3180, lr 3.00e-04\n",
      "Saved checkpoint at iteration 0\n",
      "step 100: train loss 2.8587, test loss 2.8616, lr 3.00e-04\n",
      "Saved checkpoint at iteration 100\n",
      "step 200: train loss 2.8232, test loss 2.8169, lr 3.00e-04\n",
      "Saved checkpoint at iteration 200\n",
      "step 300: train loss 2.8164, test loss 2.8103, lr 3.00e-04\n",
      "Saved checkpoint at iteration 300\n",
      "step 400: train loss 2.8074, test loss 2.8060, lr 3.00e-04\n",
      "Saved checkpoint at iteration 400\n",
      "step 500: train loss 2.8133, test loss 2.8087, lr 3.00e-04\n",
      "step 600: train loss 2.8036, test loss 2.7988, lr 3.00e-04\n",
      "Saved checkpoint at iteration 600\n",
      "step 700: train loss 2.8001, test loss 2.7957, lr 3.00e-04\n",
      "Saved checkpoint at iteration 700\n",
      "step 800: train loss 2.7984, test loss 2.7974, lr 3.00e-04\n",
      "step 900: train loss 2.8000, test loss 2.7918, lr 3.00e-04\n",
      "Saved checkpoint at iteration 900\n",
      "step 1000: train loss 2.7973, test loss 2.7941, lr 3.00e-04\n",
      "Saved checkpoint at iteration 1000\n",
      "step 1100: train loss 2.7929, test loss 2.7906, lr 3.00e-04\n",
      "Saved checkpoint at iteration 1100\n",
      "step 1200: train loss 2.7947, test loss 2.7881, lr 3.00e-04\n",
      "Saved checkpoint at iteration 1200\n",
      "step 1300: train loss 2.7395, test loss 2.7369, lr 3.00e-04\n",
      "Saved checkpoint at iteration 1300\n",
      "step 1400: train loss 2.6335, test loss 2.6220, lr 3.00e-04\n",
      "Saved checkpoint at iteration 1400\n",
      "step 1500: train loss 2.5564, test loss 2.5559, lr 3.00e-04\n",
      "Saved checkpoint at iteration 1500\n",
      "step 1600: train loss 2.4760, test loss 2.4678, lr 3.00e-04\n",
      "Saved checkpoint at iteration 1600\n",
      "step 1700: train loss 2.4189, test loss 2.4082, lr 3.00e-04\n",
      "Saved checkpoint at iteration 1700\n",
      "step 1800: train loss 2.3750, test loss 2.3678, lr 3.00e-04\n",
      "Saved checkpoint at iteration 1800\n",
      "step 1900: train loss 2.3306, test loss 2.3118, lr 3.00e-04\n",
      "Saved checkpoint at iteration 1900\n",
      "step 2000: train loss 2.2797, test loss 2.2673, lr 3.00e-04\n",
      "Saved checkpoint at iteration 2000\n",
      "Saved checkpoint at iteration 2000\n",
      "step 2100: train loss 2.2496, test loss 2.2365, lr 3.00e-04\n",
      "Saved checkpoint at iteration 2100\n",
      "step 2200: train loss 2.2161, test loss 2.2072, lr 3.00e-04\n",
      "Saved checkpoint at iteration 2200\n",
      "step 2300: train loss 2.1854, test loss 2.1798, lr 3.00e-04\n",
      "Saved checkpoint at iteration 2300\n",
      "step 2400: train loss 2.1558, test loss 2.1458, lr 3.00e-04\n",
      "Saved checkpoint at iteration 2400\n",
      "step 2500: train loss 2.1267, test loss 2.1185, lr 3.00e-04\n",
      "Saved checkpoint at iteration 2500\n",
      "step 2600: train loss 2.1050, test loss 2.1011, lr 3.00e-04\n",
      "Saved checkpoint at iteration 2600\n",
      "step 2700: train loss 2.0637, test loss 2.0548, lr 3.00e-04\n",
      "Saved checkpoint at iteration 2700\n",
      "step 2800: train loss 2.0385, test loss 2.0400, lr 3.00e-04\n",
      "Saved checkpoint at iteration 2800\n",
      "step 2900: train loss 2.0068, test loss 2.0143, lr 3.00e-04\n",
      "Saved checkpoint at iteration 2900\n",
      "step 3000: train loss 1.9841, test loss 1.9867, lr 3.00e-04\n",
      "Saved checkpoint at iteration 3000\n",
      "Saved checkpoint at iteration 3000\n",
      "step 3100: train loss 1.9667, test loss 1.9793, lr 3.00e-04\n",
      "Saved checkpoint at iteration 3100\n",
      "step 3200: train loss 1.9329, test loss 1.9434, lr 3.00e-04\n",
      "Saved checkpoint at iteration 3200\n",
      "step 3300: train loss 1.9243, test loss 1.9280, lr 3.00e-04\n",
      "Saved checkpoint at iteration 3300\n",
      "step 3400: train loss 1.9088, test loss 1.9202, lr 3.00e-04\n",
      "Saved checkpoint at iteration 3400\n",
      "step 3500: train loss 1.9008, test loss 1.9210, lr 3.00e-04\n",
      "step 3600: train loss 1.8775, test loss 1.8832, lr 3.00e-04\n",
      "Saved checkpoint at iteration 3600\n",
      "step 3700: train loss 1.8647, test loss 1.8582, lr 3.00e-04\n",
      "Saved checkpoint at iteration 3700\n",
      "step 3800: train loss 1.8512, test loss 1.8487, lr 3.00e-04\n",
      "Saved checkpoint at iteration 3800\n",
      "step 3900: train loss 1.8401, test loss 1.8460, lr 3.00e-04\n",
      "Saved checkpoint at iteration 3900\n",
      "step 4000: train loss 1.8178, test loss 1.8256, lr 3.00e-04\n",
      "Saved checkpoint at iteration 4000\n",
      "Saved checkpoint at iteration 4000\n",
      "step 4100: train loss 1.7981, test loss 1.8106, lr 3.00e-04\n",
      "Saved checkpoint at iteration 4100\n",
      "step 4200: train loss 1.7745, test loss 1.7610, lr 3.00e-04\n",
      "Saved checkpoint at iteration 4200\n",
      "step 4300: train loss 1.6852, test loss 1.6886, lr 3.00e-04\n",
      "Saved checkpoint at iteration 4300\n",
      "step 4400: train loss 1.6254, test loss 1.6066, lr 3.00e-04\n",
      "Saved checkpoint at iteration 4400\n",
      "step 4500: train loss 1.5523, test loss 1.5382, lr 3.00e-04\n",
      "Saved checkpoint at iteration 4500\n",
      "step 4600: train loss 1.4714, test loss 1.4704, lr 3.00e-04\n",
      "Saved checkpoint at iteration 4600\n",
      "step 4700: train loss 1.4410, test loss 1.4296, lr 3.00e-04\n",
      "Saved checkpoint at iteration 4700\n",
      "step 4800: train loss 1.3957, test loss 1.3716, lr 3.00e-04\n",
      "Saved checkpoint at iteration 4800\n",
      "step 4900: train loss 1.3359, test loss 1.3407, lr 3.00e-04\n",
      "Saved checkpoint at iteration 4900\n",
      "step 5000: train loss 1.3050, test loss 1.2900, lr 3.00e-04\n",
      "Saved checkpoint at iteration 5000\n",
      "Saved checkpoint at iteration 5000\n",
      "step 5100: train loss 1.2599, test loss 1.2819, lr 3.00e-04\n",
      "Saved checkpoint at iteration 5100\n",
      "step 5200: train loss 1.2070, test loss 1.2047, lr 3.00e-04\n",
      "Saved checkpoint at iteration 5200\n",
      "step 5300: train loss 1.1668, test loss 1.1767, lr 3.00e-04\n",
      "Saved checkpoint at iteration 5300\n",
      "step 5400: train loss 1.1431, test loss 1.1435, lr 3.00e-04\n",
      "Saved checkpoint at iteration 5400\n",
      "step 5500: train loss 1.1136, test loss 1.0986, lr 3.00e-04\n",
      "Saved checkpoint at iteration 5500\n",
      "step 5600: train loss 1.0660, test loss 1.0609, lr 3.00e-04\n",
      "Saved checkpoint at iteration 5600\n",
      "step 5700: train loss 1.0251, test loss 1.0600, lr 3.00e-04\n",
      "Saved checkpoint at iteration 5700\n",
      "step 5800: train loss 1.0186, test loss 1.0504, lr 3.00e-04\n",
      "Saved checkpoint at iteration 5800\n",
      "step 5900: train loss 0.9601, test loss 0.9923, lr 3.00e-04\n",
      "Saved checkpoint at iteration 5900\n",
      "step 6000: train loss 0.9487, test loss 0.9721, lr 3.00e-04\n",
      "Saved checkpoint at iteration 6000\n",
      "Saved checkpoint at iteration 6000\n",
      "step 6100: train loss 0.9286, test loss 0.9466, lr 3.00e-04\n",
      "Saved checkpoint at iteration 6100\n",
      "step 6200: train loss 0.9060, test loss 0.9255, lr 3.00e-04\n",
      "Saved checkpoint at iteration 6200\n",
      "step 6300: train loss 0.8830, test loss 0.9025, lr 3.00e-04\n",
      "Saved checkpoint at iteration 6300\n",
      "step 6400: train loss 0.8735, test loss 0.8821, lr 3.00e-04\n",
      "Saved checkpoint at iteration 6400\n",
      "step 6500: train loss 0.8519, test loss 0.8810, lr 3.00e-04\n",
      "Saved checkpoint at iteration 6500\n",
      "step 6600: train loss 0.8393, test loss 0.8348, lr 3.00e-04\n",
      "Saved checkpoint at iteration 6600\n",
      "step 6700: train loss 0.8323, test loss 0.8601, lr 3.00e-04\n",
      "step 6800: train loss 0.8222, test loss 0.8460, lr 3.00e-04\n",
      "step 6900: train loss 0.7934, test loss 0.8208, lr 3.00e-04\n",
      "Saved checkpoint at iteration 6900\n",
      "step 7000: train loss 0.7863, test loss 0.7994, lr 3.00e-04\n",
      "Saved checkpoint at iteration 7000\n",
      "Saved checkpoint at iteration 7000\n",
      "step 7100: train loss 0.7794, test loss 0.8001, lr 3.00e-04\n",
      "step 7200: train loss 0.7654, test loss 0.7719, lr 3.00e-04\n",
      "Saved checkpoint at iteration 7200\n",
      "step 7300: train loss 0.7390, test loss 0.7675, lr 3.00e-04\n",
      "Saved checkpoint at iteration 7300\n",
      "step 7400: train loss 0.7601, test loss 0.7383, lr 3.00e-04\n",
      "Saved checkpoint at iteration 7400\n",
      "step 7500: train loss 0.7214, test loss 0.7351, lr 3.00e-04\n",
      "Saved checkpoint at iteration 7500\n",
      "step 7600: train loss 0.7449, test loss 0.7593, lr 3.00e-04\n",
      "step 7700: train loss 0.7018, test loss 0.7270, lr 3.00e-04\n",
      "Saved checkpoint at iteration 7700\n",
      "step 7800: train loss 0.6989, test loss 0.7022, lr 3.00e-04\n",
      "Saved checkpoint at iteration 7800\n",
      "step 7900: train loss 0.6929, test loss 0.7163, lr 3.00e-04\n",
      "step 8000: train loss 0.6950, test loss 0.6949, lr 3.00e-04\n",
      "Saved checkpoint at iteration 8000\n",
      "Saved checkpoint at iteration 8000\n",
      "step 8100: train loss 0.6882, test loss 0.7111, lr 3.00e-04\n",
      "step 8200: train loss 0.6721, test loss 0.7064, lr 3.00e-04\n",
      "step 8300: train loss 0.6690, test loss 0.6796, lr 3.00e-04\n",
      "Saved checkpoint at iteration 8300\n",
      "step 8400: train loss 0.6490, test loss 0.6769, lr 3.00e-04\n",
      "Saved checkpoint at iteration 8400\n",
      "step 8500: train loss 0.6501, test loss 0.6634, lr 3.00e-04\n",
      "Saved checkpoint at iteration 8500\n",
      "step 8600: train loss 0.6488, test loss 0.6621, lr 3.00e-04\n",
      "Saved checkpoint at iteration 8600\n",
      "step 8700: train loss 0.6435, test loss 0.6650, lr 3.00e-04\n",
      "step 8800: train loss 0.6569, test loss 0.6860, lr 3.00e-04\n",
      "step 8900: train loss 0.6272, test loss 0.6420, lr 3.00e-04\n",
      "Saved checkpoint at iteration 8900\n",
      "step 9000: train loss 0.6281, test loss 0.6304, lr 3.00e-04\n",
      "Saved checkpoint at iteration 9000\n",
      "Saved checkpoint at iteration 9000\n",
      "step 9100: train loss 0.6165, test loss 0.6199, lr 3.00e-04\n",
      "Saved checkpoint at iteration 9100\n",
      "step 9200: train loss 0.6227, test loss 0.6328, lr 3.00e-04\n",
      "step 9300: train loss 0.6156, test loss 0.6226, lr 3.00e-04\n",
      "step 9400: train loss 0.6092, test loss 0.5993, lr 3.00e-04\n",
      "Saved checkpoint at iteration 9400\n",
      "step 9500: train loss 0.6152, test loss 0.6050, lr 3.00e-04\n",
      "step 9600: train loss 0.5981, test loss 0.5994, lr 3.00e-04\n",
      "step 9700: train loss 0.5881, test loss 0.6097, lr 3.00e-04\n",
      "step 9800: train loss 0.5870, test loss 0.5865, lr 3.00e-04\n",
      "Saved checkpoint at iteration 9800\n",
      "step 9900: train loss 0.5825, test loss 0.5904, lr 3.00e-04\n",
      "step 10000: train loss 0.5848, test loss 0.5721, lr 3.00e-04\n",
      "Saved checkpoint at iteration 10000\n",
      "Saved checkpoint at iteration 10000\n",
      "step 10100: train loss 0.5945, test loss 0.5751, lr 3.00e-04\n",
      "step 10200: train loss 0.5744, test loss 0.6030, lr 3.00e-04\n",
      "step 10300: train loss 0.5542, test loss 0.5746, lr 3.00e-04\n",
      "step 10400: train loss 0.5650, test loss 0.5788, lr 3.00e-04\n",
      "step 10500: train loss 0.5708, test loss 0.5643, lr 3.00e-04\n",
      "Saved checkpoint at iteration 10500\n",
      "step 10600: train loss 0.5605, test loss 0.5534, lr 3.00e-04\n",
      "Saved checkpoint at iteration 10600\n",
      "step 10700: train loss 0.5545, test loss 0.5530, lr 3.00e-04\n",
      "Saved checkpoint at iteration 10700\n",
      "step 10800: train loss 0.5535, test loss 0.5466, lr 3.00e-04\n",
      "Saved checkpoint at iteration 10800\n",
      "step 10900: train loss 0.5608, test loss 0.5532, lr 3.00e-04\n",
      "step 11000: train loss 0.5308, test loss 0.5534, lr 3.00e-04\n",
      "Saved checkpoint at iteration 11000\n",
      "step 11100: train loss 0.5498, test loss 0.5550, lr 3.00e-04\n",
      "step 11200: train loss 0.5466, test loss 0.5461, lr 3.00e-04\n",
      "Saved checkpoint at iteration 11200\n",
      "step 11300: train loss 0.5446, test loss 0.5519, lr 3.00e-04\n",
      "step 11400: train loss 0.5391, test loss 0.5374, lr 3.00e-04\n",
      "Saved checkpoint at iteration 11400\n",
      "step 11500: train loss 0.5110, test loss 0.5123, lr 3.00e-05\n",
      "Saved checkpoint at iteration 11500\n",
      "step 11600: train loss 0.4838, test loss 0.4880, lr 3.00e-05\n",
      "Saved checkpoint at iteration 11600\n",
      "step 11700: train loss 0.4648, test loss 0.4747, lr 3.00e-05\n",
      "Saved checkpoint at iteration 11700\n",
      "step 11800: train loss 0.4587, test loss 0.4593, lr 3.00e-05\n",
      "Saved checkpoint at iteration 11800\n",
      "step 11900: train loss 0.4507, test loss 0.4545, lr 3.00e-05\n",
      "Saved checkpoint at iteration 11900\n",
      "step 12000: train loss 0.4557, test loss 0.4495, lr 3.00e-05\n",
      "Saved checkpoint at iteration 12000\n",
      "Saved checkpoint at iteration 12000\n",
      "step 12100: train loss 0.4428, test loss 0.4411, lr 3.00e-05\n",
      "Saved checkpoint at iteration 12100\n",
      "step 12200: train loss 0.4273, test loss 0.4377, lr 3.00e-05\n",
      "Saved checkpoint at iteration 12200\n",
      "step 12300: train loss 0.4363, test loss 0.4322, lr 3.00e-05\n",
      "Saved checkpoint at iteration 12300\n",
      "step 12400: train loss 0.4251, test loss 0.4308, lr 3.00e-05\n",
      "Saved checkpoint at iteration 12400\n",
      "step 12500: train loss 0.4122, test loss 0.4278, lr 3.00e-05\n",
      "Saved checkpoint at iteration 12500\n",
      "step 12600: train loss 0.4096, test loss 0.4264, lr 3.00e-05\n",
      "Saved checkpoint at iteration 12600\n",
      "step 12700: train loss 0.4164, test loss 0.4204, lr 3.00e-05\n",
      "Saved checkpoint at iteration 12700\n",
      "step 12800: train loss 0.4191, test loss 0.4190, lr 3.00e-05\n",
      "Saved checkpoint at iteration 12800\n",
      "step 12900: train loss 0.4051, test loss 0.4147, lr 3.00e-05\n",
      "Saved checkpoint at iteration 12900\n",
      "step 13000: train loss 0.4297, test loss 0.4165, lr 3.00e-05\n",
      "Saved checkpoint at iteration 13000\n",
      "step 13100: train loss 0.4017, test loss 0.4149, lr 3.00e-05\n",
      "step 13200: train loss 0.4056, test loss 0.4083, lr 3.00e-05\n",
      "Saved checkpoint at iteration 13200\n",
      "step 13300: train loss 0.4022, test loss 0.4063, lr 3.00e-05\n",
      "Saved checkpoint at iteration 13300\n",
      "step 13400: train loss 0.3918, test loss 0.4077, lr 3.00e-05\n",
      "step 13500: train loss 0.4011, test loss 0.4045, lr 3.00e-05\n",
      "Saved checkpoint at iteration 13500\n",
      "step 13600: train loss 0.3990, test loss 0.4002, lr 3.00e-05\n",
      "Saved checkpoint at iteration 13600\n",
      "step 13700: train loss 0.3996, test loss 0.3994, lr 3.00e-05\n",
      "Saved checkpoint at iteration 13700\n",
      "step 13800: train loss 0.4005, test loss 0.4013, lr 3.00e-05\n",
      "step 13900: train loss 0.3998, test loss 0.3991, lr 3.00e-05\n",
      "Saved checkpoint at iteration 13900\n",
      "step 14000: train loss 0.3878, test loss 0.3996, lr 3.00e-05\n",
      "Saved checkpoint at iteration 14000\n",
      "step 14100: train loss 0.3948, test loss 0.3956, lr 3.00e-05\n",
      "Saved checkpoint at iteration 14100\n",
      "step 14200: train loss 0.3995, test loss 0.3915, lr 3.00e-05\n",
      "Saved checkpoint at iteration 14200\n",
      "step 14300: train loss 0.3841, test loss 0.3938, lr 3.00e-05\n",
      "step 14400: train loss 0.4056, test loss 0.3966, lr 3.00e-05\n",
      "step 14500: train loss 0.3799, test loss 0.3937, lr 3.00e-05\n",
      "step 14600: train loss 0.3869, test loss 0.3899, lr 3.00e-05\n",
      "Saved checkpoint at iteration 14600\n",
      "step 14700: train loss 0.3886, test loss 0.3921, lr 3.00e-05\n",
      "step 14800: train loss 0.3727, test loss 0.3885, lr 3.00e-05\n",
      "Saved checkpoint at iteration 14800\n",
      "step 14900: train loss 0.3928, test loss 0.3905, lr 3.00e-05\n",
      "step 15000: train loss 0.3811, test loss 0.3840, lr 3.00e-06\n",
      "Saved checkpoint at iteration 15000\n",
      "Saved checkpoint at iteration 15000\n",
      "step 15100: train loss 0.3793, test loss 0.3838, lr 3.00e-06\n",
      "Saved checkpoint at iteration 15100\n",
      "step 15200: train loss 0.3782, test loss 0.3825, lr 3.00e-06\n",
      "Saved checkpoint at iteration 15200\n",
      "step 15300: train loss 0.3727, test loss 0.3812, lr 3.00e-06\n",
      "Saved checkpoint at iteration 15300\n",
      "step 15400: train loss 0.3762, test loss 0.3793, lr 3.00e-06\n",
      "Saved checkpoint at iteration 15400\n",
      "step 15500: train loss 0.3802, test loss 0.3826, lr 3.00e-06\n",
      "step 15600: train loss 0.3732, test loss 0.3817, lr 3.00e-06\n",
      "step 15700: train loss 0.3766, test loss 0.3814, lr 3.00e-06\n",
      "step 15800: train loss 0.3767, test loss 0.3808, lr 3.00e-06\n",
      "step 15900: train loss 0.3829, test loss 0.3815, lr 3.00e-06\n",
      "step 16000: train loss 0.3790, test loss 0.3803, lr 3.00e-07\n",
      "Saved checkpoint at iteration 16000\n",
      "step 16100: train loss 0.3725, test loss 0.3807, lr 3.00e-07\n",
      "step 16200: train loss 0.3779, test loss 0.3791, lr 3.00e-07\n",
      "Saved checkpoint at iteration 16200\n",
      "step 16300: train loss 0.3699, test loss 0.3802, lr 3.00e-07\n",
      "step 16400: train loss 0.3754, test loss 0.3814, lr 3.00e-07\n",
      "step 16500: train loss 0.3783, test loss 0.3798, lr 3.00e-07\n",
      "step 16600: train loss 0.3753, test loss 0.3803, lr 3.00e-07\n",
      "step 16700: train loss 0.3719, test loss 0.3802, lr 3.00e-07\n",
      "step 16800: train loss 0.3779, test loss 0.3804, lr 3.00e-07\n",
      "step 16900: train loss 0.3815, test loss 0.3797, lr 3.00e-07\n",
      "step 17000: train loss 0.3795, test loss 0.3798, lr 3.00e-07\n",
      "Saved checkpoint at iteration 17000\n",
      "step 17100: train loss 0.3628, test loss 0.3822, lr 3.00e-07\n",
      "step 17200: train loss 0.3771, test loss 0.3785, lr 3.00e-07\n",
      "Saved checkpoint at iteration 17200\n",
      "step 17300: train loss 0.3693, test loss 0.3801, lr 3.00e-07\n",
      "step 17400: train loss 0.3676, test loss 0.3780, lr 3.00e-07\n",
      "Saved checkpoint at iteration 17400\n",
      "step 17500: train loss 0.3627, test loss 0.3791, lr 3.00e-07\n",
      "step 17600: train loss 0.3752, test loss 0.3803, lr 3.00e-07\n",
      "step 17700: train loss 0.3778, test loss 0.3783, lr 3.00e-07\n",
      "step 17800: train loss 0.3748, test loss 0.3791, lr 3.00e-07\n",
      "step 17900: train loss 0.3743, test loss 0.3798, lr 3.00e-08\n",
      "step 18000: train loss 0.3764, test loss 0.3794, lr 3.00e-08\n",
      "Saved checkpoint at iteration 18000\n",
      "step 18100: train loss 0.3755, test loss 0.3792, lr 3.00e-08\n",
      "step 18200: train loss 0.3795, test loss 0.3799, lr 3.00e-08\n",
      "step 18300: train loss 0.3670, test loss 0.3804, lr 3.00e-08\n",
      "step 18400: train loss 0.3718, test loss 0.3810, lr 3.00e-08\n",
      "step 18500: train loss 0.3695, test loss 0.3801, lr 3.00e-08\n",
      "step 18600: train loss 0.3685, test loss 0.3806, lr 3.00e-08\n",
      "step 18700: train loss 0.3852, test loss 0.3804, lr 3.00e-08\n",
      "step 18800: train loss 0.3789, test loss 0.3793, lr 3.00e-08\n",
      "step 18900: train loss 0.3795, test loss 0.3824, lr 3.00e-09\n",
      "step 19000: train loss 0.3694, test loss 0.3795, lr 3.00e-09\n",
      "Saved checkpoint at iteration 19000\n",
      "step 19100: train loss 0.3755, test loss 0.3794, lr 3.00e-09\n",
      "step 19200: train loss 0.3662, test loss 0.3803, lr 3.00e-09\n",
      "step 19300: train loss 0.3914, test loss 0.3808, lr 3.00e-09\n",
      "step 19400: train loss 0.3760, test loss 0.3797, lr 3.00e-09\n",
      "step 19500: train loss 0.3758, test loss 0.3805, lr 3.00e-09\n",
      "step 19600: train loss 0.3738, test loss 0.3802, lr 3.00e-09\n",
      "step 19700: train loss 0.3788, test loss 0.3785, lr 3.00e-09\n",
      "step 19800: train loss 0.3717, test loss 0.3801, lr 3.00e-09\n",
      "step 19900: train loss 0.3792, test loss 0.3791, lr 3.00e-09\n",
      "step 19999: train loss 0.3680, test loss 0.3826, lr 3.00e-09\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 64 # how many independent sequences will we process in parallel?\n",
    "block_size = sequence_len#32 # what is the maximum context length for predictions?\n",
    "max_iters = 20000\n",
    "eval_interval = 100\n",
    "learning_rate = 3e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 256#//4\n",
    "n_head = 2\n",
    "n_layer = 4\n",
    "dropout = 0.1\n",
    "N = 5\n",
    "c_q=2\n",
    "c_k=2*N -1\n",
    "c_h=2\n",
    "\n",
    "# ------------\n",
    "SEED = 1337\n",
    "torch.manual_seed(SEED)\n",
    "# For maximum reproducibility\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.use_deterministic_algorithms(False) \n",
    "# 1. Set environment variables for CuBLAS determinism\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'  # or ':16:8'\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "\n",
    "# 2. Set Python, NumPy and PyTorch seeds\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# 3. Configure PyTorch for deterministic operations\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "# Add these near your hyperparameters\n",
    "checkpoint_dir = \"./checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "checkpoint_interval = 1000  # Save every 1000 iterations\n",
    "best_test_loss = float('inf')\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "g = torch.Generator()\n",
    "g.manual_seed(SEED)\n",
    "\n",
    "# Add this function to save checkpoints\n",
    "def save_checkpoint(iteration, model, optimizer, scheduler, best=False):\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    checkpoint = {\n",
    "        'iteration': iteration,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n",
    "        'loss': best_test_loss,\n",
    "        'timestamp': timestamp\n",
    "    }\n",
    "    \n",
    "    filename = f\"checkpoint_{iteration}.pt\" if not best else \"best_model.pt\"\n",
    "    torch.save(checkpoint, os.path.join(checkpoint_dir, filename))\n",
    "    print(f\"Saved checkpoint at iteration {iteration}\")\n",
    "\n",
    "# Add this function to load checkpoints\n",
    "def load_checkpoint(path, model, optimizer, scheduler):\n",
    "    checkpoint = torch.load(path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    if scheduler and checkpoint['scheduler_state_dict']:\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    return checkpoint['iteration']\n",
    "train_dataset = MTADataset(f\"{path_folder}mta_train.txt\", variant=\"all\")#, tokenizer)\n",
    "# Modified DataLoader with deterministic shuffling\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    worker_init_fn=seed_worker,\n",
    "    generator=g,\n",
    "    num_workers=4,\n",
    "    persistent_workers=True\n",
    ")\n",
    "test_dataset = MTADataset(f\"{path_folder}mta_test.txt\", variant=\"all\")#, tokenizer)\n",
    "# Modified DataLoader with deterministic shuffling\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    worker_init_fn=seed_worker,\n",
    "    generator=g,\n",
    "    num_workers=4,\n",
    "    persistent_workers=True\n",
    ")\n",
    "  \n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'test']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        if split == 'test':\n",
    "            dataloader = test_dataloader\n",
    "        else:\n",
    "            dataloader = train_dataloader\n",
    "        dataloader_iter = iter(dataloader)\n",
    "        for k in range(eval_iters):\n",
    "            try:\n",
    "                batch = next(dataloader_iter)\n",
    "                # process the batch\n",
    "            except StopIteration:\n",
    "                # Reinitialize the iterator if you reach the end\n",
    "                dataloader_iter = iter(dataloader)\n",
    "                batch = next(dataloader_iter)\n",
    "            blocks = batch['blocks'].to(device)\n",
    "            queries = batch['query'].to(device)\n",
    "            targets = batch['target'].to(device)\n",
    "            logits, loss = model(blocks, queries, targets)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiTokenAttention(n_head, head_size)\n",
    "        self.ffwd_blocks = FeedFoward(n_embd)\n",
    "        self.ffwd_queries = FeedFoward(n_embd)\n",
    "\n",
    "        self.blocks_proj  = nn.Linear(n_embd, n_embd) \n",
    "        self.ln_blocks = nn.RMSNorm(n_embd)\n",
    "        self.ln_queries = nn.RMSNorm(n_embd)\n",
    "\n",
    "        self.ln2 = nn.RMSNorm(n_embd)\n",
    "\n",
    "    def forward(self, blocks_pos_emb, queries_emb):\n",
    "        blocks_pos_emb = self.ln_blocks(blocks_pos_emb)\n",
    "        queries_emb = self.ln_queries(queries_emb)\n",
    "        attn_out = self.sa(blocks_pos_emb, queries_emb)\n",
    "        blocks_pos_emb_attn = self.blocks_proj(attn_out.mean(dim=1))\n",
    "        blocks_pos_emb = blocks_pos_emb + blocks_pos_emb_attn.unsqueeze(1)\n",
    "        queries_emb = queries_emb + attn_out\n",
    "        queries_emb = queries_emb + self.ffwd_queries(self.ln2(queries_emb))\n",
    "        blocks_pos_emb = blocks_pos_emb + self.ffwd_blocks(self.ln2(blocks_pos_emb))\n",
    "\n",
    "        return blocks_pos_emb,queries_emb\n",
    "\n",
    "# super simple bigram model\n",
    "class MTAAttentionRetrievalModel(nn.Module):\n",
    "\n",
    "    def __init__(self, variant= \"all\"):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(sequence_len, n_embd)\n",
    "        self.blocks = nn.ModuleList([Block(n_embd, n_head) for _ in range(n_layer)]) #nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.RMSNorm(n_embd) # final layer norm\n",
    "        self.variant = variant\n",
    "\n",
    "        if self.variant == \"position\":\n",
    "            self.lm_head = nn.Linear(L*n_embd, num_blocks)\n",
    "        elif self.variant == \"all\":\n",
    "            self.lm_head = nn.Sequential(\n",
    "                nn.Linear(n_embd*L, n_embd * 2),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_embd * 2, vocab_size * N)\n",
    "            )\n",
    "        elif self.variant in ['first', 'last']:\n",
    "            self.lm_head = nn.Linear(n_embd*L, vocab_size)\n",
    "        else:\n",
    "            raise f\"{self.variant} is not recognized as a supported variant type\"\n",
    "\n",
    "    def forward(self, blocks, queries, targets=None):\n",
    "        B, T = blocks.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        blocks_emb = self.token_embedding_table(blocks) # (B,T,C)\n",
    "        queries_emb = self.token_embedding_table(queries) # (B,T,C)\n",
    "\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        blocks_pos_emb = blocks_emb + pos_emb # (B,T,C)\n",
    "        for block in self.blocks:\n",
    "            blocks_pos_emb, queries_emb = block(blocks_pos_emb, queries_emb)\n",
    "        # x = self.blocks(blocks_pos_emb,queries_emb) # (B,T,C)\n",
    "        x = self.ln_f(queries_emb) # (B,T,C)\n",
    "        # print(x.shape)\n",
    "        logits = self.lm_head(x.view(B,-1)) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, C = logits.shape\n",
    "            if self.variant == \"all\":\n",
    "                # logits = logits.view(-1, vocab_size)\n",
    "                \n",
    "                logits = logits.view(B*N, vocab_size)\n",
    "                targets = targets.view(B*N)\n",
    "            else:\n",
    "                # logits = logits.view(B, vocab_size)\n",
    "                \n",
    "                logits = logits.view(B, vocab_size)\n",
    "                targets = targets.view(B)                \n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "model = MTAAttentionRetrievalModel(variant=\"all\")\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate,weight_decay=0.01)\n",
    "# optimizer = torch.optim.Lion(model.parameters(), lr=learning_rate)#,weight_decay=0.01)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, \n",
    "    mode='min', \n",
    "    factor=0.1, \n",
    "    patience=1000,  # Number of eval intervals to wait before reducing LR\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "start_scheduler_at_iter = 10000  \n",
    "resume_from_checkpoint = False\n",
    "\n",
    "dataloader = train_dataloader\n",
    "dataloader_iter = iter(dataloader)\n",
    "if resume_from_checkpoint:\n",
    "    start_iter = load_checkpoint(\"checkpoints/best_model.pt\", model, optimizer, scheduler)\n",
    "else:\n",
    "    start_iter = 0\n",
    "for i in tqdm(range(start_iter, max_iters)):\n",
    "\n",
    "    if i % eval_interval == 0 or i == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {i}: train loss {losses['train']:.4f}, test loss {losses['test']:.4f}, lr {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "        # Save best model\n",
    "        if losses['test'] < best_test_loss:\n",
    "            best_test_loss = losses['test']\n",
    "            save_checkpoint(i, model, optimizer, scheduler, best=True)\n",
    "    \n",
    "    # Regular checkpoint saving\n",
    "    if i % checkpoint_interval == 0 and i > 0:\n",
    "        save_checkpoint(i, model, optimizer, scheduler)\n",
    "        \n",
    "\n",
    "    try:\n",
    "        batch = next(dataloader_iter)\n",
    "        # process the batch\n",
    "    except StopIteration:\n",
    "        # Reinitialize the iterator if you reach the end\n",
    "        dataloader_iter = iter(dataloader)\n",
    "        batch = next(dataloader_iter)\n",
    "    blocks = batch['blocks'].to(device)\n",
    "    queries = batch['query'].to(device)\n",
    "    targets = batch['target'].to(device)\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(blocks, queries, targets)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # Only step the scheduler after start_scheduler_at_iter\n",
    "    if i >= start_scheduler_at_iter:\n",
    "        scheduler.step(loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a4c35e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-12T16:34:28.355615Z",
     "iopub.status.busy": "2025-08-12T16:34:28.354464Z",
     "iopub.status.idle": "2025-08-12T16:34:28.504998Z",
     "shell.execute_reply": "2025-08-12T16:34:28.504269Z"
    },
    "papermill": {
     "duration": 0.167981,
     "end_time": "2025-08-12T16:34:28.506160",
     "exception": false,
     "start_time": "2025-08-12T16:34:28.338179",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MTAAttentionRetrievalModel(\n",
       "  (token_embedding_table): Embedding(27, 256)\n",
       "  (position_embedding_table): Embedding(299, 256)\n",
       "  (blocks): ModuleList(\n",
       "    (0-3): 4 x Block(\n",
       "      (sa): MultiTokenAttention(\n",
       "        (block_proj): Linear(in_features=256, out_features=512, bias=True)\n",
       "        (query_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (conv2d): Conv2d(2, 2, kernel_size=(2, 9), stride=(1, 1), padding=same)\n",
       "        (head_conv): Conv2d(2, 2, kernel_size=(2, 2), stride=(1, 1), padding=same)\n",
       "        (group_norm): GroupNorm(2, 256, eps=1e-05, affine=True)\n",
       "        (gate): Linear(in_features=256, out_features=2, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "      (ffwd_blocks): FeedFoward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ffwd_queries): FeedFoward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (blocks_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (ln_blocks): RMSNorm((256,), eps=None, elementwise_affine=True)\n",
       "      (ln_queries): RMSNorm((256,), eps=None, elementwise_affine=True)\n",
       "      (ln2): RMSNorm((256,), eps=None, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (ln_f): RMSNorm((256,), eps=None, elementwise_affine=True)\n",
       "  (lm_head): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=135, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(\"checkpoints/best_model.pt\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "edc932d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-12T16:34:28.538193Z",
     "iopub.status.busy": "2025-08-12T16:34:28.537965Z",
     "iopub.status.idle": "2025-08-12T16:34:29.187316Z",
     "shell.execute_reply": "2025-08-12T16:34:29.186339Z"
    },
    "papermill": {
     "duration": 0.666472,
     "end_time": "2025-08-12T16:34:29.188465",
     "exception": false,
     "start_time": "2025-08-12T16:34:28.521993",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75b9832ddb6f47d7be86896bbee67d4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "test_dataset = MTADataset(f\"{path_folder}mta_test.txt\", variant=\"all\")#, tokenizer)\n",
    "# Modified DataLoader with deterministic shuffling\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    worker_init_fn=seed_worker,\n",
    "    generator=g,\n",
    "    num_workers=4,\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "full_output = np.array([])\n",
    "full_targets = np.array([])\n",
    "for batch in tqdm(test_dataloader):\n",
    "    blocks = batch['blocks'].to(device)\n",
    "    queries = batch['query'].to(device)\n",
    "    targets = batch['target'].to(device)\n",
    "    \n",
    "    # evaluate the loss\n",
    "    logits, loss = model(blocks, queries, targets=None)\n",
    "    output = torch.argmax(logits.view(-1, N, vocab_size), dim=-1)\n",
    "    \n",
    "    output = np.array([decode(i.tolist()) for i in output])\n",
    "    full_output = np.append(full_output, output)\n",
    "    targets = np.array([decode(i.tolist()) for i in targets])\n",
    "    full_targets = np.append(full_targets, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b8df79",
   "metadata": {
    "papermill": {
     "duration": 0.016037,
     "end_time": "2025-08-12T16:34:29.221145",
     "exception": false,
     "start_time": "2025-08-12T16:34:29.205108",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b3ec444",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-12T16:34:29.254846Z",
     "iopub.status.busy": "2025-08-12T16:34:29.254479Z",
     "iopub.status.idle": "2025-08-12T16:34:29.260884Z",
     "shell.execute_reply": "2025-08-12T16:34:29.260111Z"
    },
    "papermill": {
     "duration": 0.024665,
     "end_time": "2025-08-12T16:34:29.262060",
     "exception": false,
     "start_time": "2025-08-12T16:34:29.237395",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Accuracy of the MTA Model on the test set is 17.3250%\n"
     ]
    }
   ],
   "source": [
    "acc = sum((full_output == full_targets).astype(int))/ len(targets)\n",
    "print(f\"The Accuracy of the MTA Model on the test set is {acc:.4f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "15ef596d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-12T16:34:29.294158Z",
     "iopub.status.busy": "2025-08-12T16:34:29.293940Z",
     "iopub.status.idle": "2025-08-12T16:34:29.297298Z",
     "shell.execute_reply": "2025-08-12T16:34:29.296743Z"
    },
    "papermill": {
     "duration": 0.02029,
     "end_time": "2025-08-12T16:34:29.298338",
     "exception": false,
     "start_time": "2025-08-12T16:34:29.278048",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#[int(a == b) for a, b in zip(output, targets)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7790aea",
   "metadata": {
    "papermill": {
     "duration": 0.014887,
     "end_time": "2025-08-12T16:34:29.328250",
     "exception": false,
     "start_time": "2025-08-12T16:34:29.313363",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf15433",
   "metadata": {
    "papermill": {
     "duration": 0.015028,
     "end_time": "2025-08-12T16:34:29.358401",
     "exception": false,
     "start_time": "2025-08-12T16:34:29.343373",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7256032c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-12T15:10:58.916111Z",
     "iopub.status.busy": "2025-08-12T15:10:58.915814Z",
     "iopub.status.idle": "2025-08-12T15:10:58.921099Z",
     "shell.execute_reply": "2025-08-12T15:10:58.920499Z",
     "shell.execute_reply.started": "2025-08-12T15:10:58.916087Z"
    },
    "papermill": {
     "duration": 0.014934,
     "end_time": "2025-08-12T16:34:29.388561",
     "exception": false,
     "start_time": "2025-08-12T16:34:29.373627",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86a285be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-12T16:34:29.420163Z",
     "iopub.status.busy": "2025-08-12T16:34:29.419916Z",
     "iopub.status.idle": "2025-08-12T16:34:29.428012Z",
     "shell.execute_reply": "2025-08-12T16:34:29.427485Z"
    },
    "papermill": {
     "duration": 0.025265,
     "end_time": "2025-08-12T16:34:29.429102",
     "exception": false,
     "start_time": "2025-08-12T16:34:29.403837",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class StandardMultiAttention(nn.Module):\n",
    "    def __init__(self,n_heads, head_size):\n",
    "        self.n_heads = n_heads\n",
    "        self.head_size = head_size\n",
    "        super().__init__()\n",
    "        # Projections for blocks (keys/values) and queries (letters)\n",
    "        self.block_proj = nn.Linear(n_embd, n_embd * 2)  # K, V\n",
    "        self.query_proj = nn.Linear(n_embd, n_embd)      # Q\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(L, block_size)))\n",
    "\n",
    "\n",
    "    def forward(self, blocks, queries):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            blocks:  (batch, block_len, n_embd)  # All blocks concatenated\n",
    "            queries: (batch, query_len, n_embd)  # Query letters\n",
    "        \"\"\"\n",
    "        # batch_size = blocks.size(0)\n",
    "        B,T,C = blocks.shape\n",
    "        _,Q_len,_ = queries.shape\n",
    "        \n",
    "        # 1. Project blocks to K/V and queries to Q\n",
    "        K, V = self.block_proj(blocks).chunk(2, dim=-1)  # Each (batch, block_len, n_embd)\n",
    "        Q = self.query_proj(queries)                     # (batch, query_len, n_embd)\n",
    "\n",
    "        \n",
    "        # 2. Split into heads\n",
    "        Q = Q.view(B, -1, self.n_heads, self.head_size).transpose(1, 2)  # [B, h, q_len, d]\n",
    "        K = K.view(B, -1, self.n_heads, self.head_size).transpose(1, 2)  # [B, h, b_len, d]\n",
    "        V = V.view(B, -1, self.n_heads, self.head_size).transpose(1, 2)  # [B, h, b_len, d]\n",
    "\n",
    "        \n",
    "        # 3. Attention scores between queries and blocks\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) *(self.head_size **-0.5)  # [B, h, q_len, b_len]\n",
    "\n",
    "        attn_scores = attn_scores.masked_fill(self.tril[:Q_len, :T] == 0, float('-inf'))    \n",
    "        # attn_scores = attn_scores.masked_fill(mask == 0, float('-inf'))        \n",
    "\n",
    "\n",
    "        \n",
    "        # 5. Softmax + Head Mixing\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "        \n",
    "        # 6. Weighted sum of block values\n",
    "        output = torch.matmul(attn_weights, V)  # [B, h, q_len, d]\n",
    "        output = output.transpose(1, 2).reshape(B, -1, self.n_heads * self.head_size)\n",
    "        output = self.dropout(self.proj(output))\n",
    "        \n",
    "        return output  # (batch, query_len, n_embd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8eb38ab1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-12T16:34:29.460764Z",
     "iopub.status.busy": "2025-08-12T16:34:29.460529Z",
     "iopub.status.idle": "2025-08-12T17:26:30.622367Z",
     "shell.execute_reply": "2025-08-12T17:26:30.621214Z"
    },
    "papermill": {
     "duration": 3121.179366,
     "end_time": "2025-08-12T17:26:30.623673",
     "exception": false,
     "start_time": "2025-08-12T16:34:29.444307",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.942407 M parameters\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a809d93343694603b87d087445b9c8eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 3.3188, test loss 3.3167, lr 3.00e-04\n",
      "Saved checkpoint at iteration 0\n",
      "step 100: train loss 2.8819, test loss 2.8825, lr 3.00e-04\n",
      "Saved checkpoint at iteration 100\n",
      "step 200: train loss 2.8322, test loss 2.8283, lr 3.00e-04\n",
      "Saved checkpoint at iteration 200\n",
      "step 300: train loss 2.8184, test loss 2.8155, lr 3.00e-04\n",
      "Saved checkpoint at iteration 300\n",
      "step 400: train loss 2.8121, test loss 2.8116, lr 3.00e-04\n",
      "Saved checkpoint at iteration 400\n",
      "step 500: train loss 2.8157, test loss 2.8120, lr 3.00e-04\n",
      "step 600: train loss 2.8075, test loss 2.8027, lr 3.00e-04\n",
      "Saved checkpoint at iteration 600\n",
      "step 700: train loss 2.8020, test loss 2.7945, lr 3.00e-04\n",
      "Saved checkpoint at iteration 700\n",
      "step 800: train loss 2.7998, test loss 2.7955, lr 3.00e-04\n",
      "step 900: train loss 2.8009, test loss 2.7962, lr 3.00e-04\n",
      "step 1000: train loss 2.8000, test loss 2.7950, lr 3.00e-04\n",
      "Saved checkpoint at iteration 1000\n",
      "step 1100: train loss 2.7928, test loss 2.7904, lr 3.00e-04\n",
      "Saved checkpoint at iteration 1100\n",
      "step 1200: train loss 2.7956, test loss 2.7901, lr 3.00e-04\n",
      "Saved checkpoint at iteration 1200\n",
      "step 1300: train loss 2.7948, test loss 2.7926, lr 3.00e-04\n",
      "step 1400: train loss 2.7938, test loss 2.7894, lr 3.00e-04\n",
      "Saved checkpoint at iteration 1400\n",
      "step 1500: train loss 2.7919, test loss 2.7865, lr 3.00e-04\n",
      "Saved checkpoint at iteration 1500\n",
      "step 1600: train loss 2.7890, test loss 2.7856, lr 3.00e-04\n",
      "Saved checkpoint at iteration 1600\n",
      "step 1700: train loss 2.7902, test loss 2.7870, lr 3.00e-04\n",
      "step 1800: train loss 2.7940, test loss 2.7837, lr 3.00e-04\n",
      "Saved checkpoint at iteration 1800\n",
      "step 1900: train loss 2.7914, test loss 2.7811, lr 3.00e-04\n",
      "Saved checkpoint at iteration 1900\n",
      "step 2000: train loss 2.7924, test loss 2.7834, lr 3.00e-04\n",
      "Saved checkpoint at iteration 2000\n",
      "step 2100: train loss 2.7891, test loss 2.7850, lr 3.00e-04\n",
      "step 2200: train loss 2.7879, test loss 2.7799, lr 3.00e-04\n",
      "Saved checkpoint at iteration 2200\n",
      "step 2300: train loss 2.7903, test loss 2.7892, lr 3.00e-04\n",
      "step 2400: train loss 2.7848, test loss 2.7915, lr 3.00e-04\n",
      "step 2500: train loss 2.7871, test loss 2.7855, lr 3.00e-04\n",
      "step 2600: train loss 2.7871, test loss 2.7875, lr 3.00e-04\n",
      "step 2700: train loss 2.7870, test loss 2.7833, lr 3.00e-04\n",
      "step 2800: train loss 2.7900, test loss 2.7842, lr 3.00e-04\n",
      "step 2900: train loss 2.7842, test loss 2.7833, lr 3.00e-04\n",
      "step 3000: train loss 2.7859, test loss 2.7793, lr 3.00e-04\n",
      "Saved checkpoint at iteration 3000\n",
      "Saved checkpoint at iteration 3000\n",
      "step 3100: train loss 2.7852, test loss 2.7816, lr 3.00e-04\n",
      "step 3200: train loss 2.7831, test loss 2.7814, lr 3.00e-04\n",
      "step 3300: train loss 2.7851, test loss 2.7793, lr 3.00e-04\n",
      "Saved checkpoint at iteration 3300\n",
      "step 3400: train loss 2.7842, test loss 2.7803, lr 3.00e-04\n",
      "step 3500: train loss 2.7849, test loss 2.7766, lr 3.00e-04\n",
      "Saved checkpoint at iteration 3500\n",
      "step 3600: train loss 2.7850, test loss 2.7801, lr 3.00e-04\n",
      "step 3700: train loss 2.7812, test loss 2.7763, lr 3.00e-04\n",
      "Saved checkpoint at iteration 3700\n",
      "step 3800: train loss 2.7843, test loss 2.7757, lr 3.00e-04\n",
      "Saved checkpoint at iteration 3800\n",
      "step 3900: train loss 2.7848, test loss 2.7803, lr 3.00e-04\n",
      "step 4000: train loss 2.7872, test loss 2.7820, lr 3.00e-04\n",
      "Saved checkpoint at iteration 4000\n",
      "step 4100: train loss 2.7794, test loss 2.7759, lr 3.00e-04\n",
      "step 4200: train loss 2.7819, test loss 2.7794, lr 3.00e-04\n",
      "step 4300: train loss 2.7817, test loss 2.7766, lr 3.00e-04\n",
      "step 4400: train loss 2.7830, test loss 2.7782, lr 3.00e-04\n",
      "step 4500: train loss 2.7845, test loss 2.7790, lr 3.00e-04\n",
      "step 4600: train loss 2.7801, test loss 2.7739, lr 3.00e-04\n",
      "Saved checkpoint at iteration 4600\n",
      "step 4700: train loss 2.7835, test loss 2.7746, lr 3.00e-04\n",
      "step 4800: train loss 2.7836, test loss 2.7740, lr 3.00e-04\n",
      "step 4900: train loss 2.7804, test loss 2.7813, lr 3.00e-04\n",
      "step 5000: train loss 2.7804, test loss 2.7759, lr 3.00e-04\n",
      "Saved checkpoint at iteration 5000\n",
      "step 5100: train loss 2.7811, test loss 2.7765, lr 3.00e-04\n",
      "step 5200: train loss 2.7786, test loss 2.7773, lr 3.00e-04\n",
      "step 5300: train loss 2.7810, test loss 2.7748, lr 3.00e-04\n",
      "step 5400: train loss 2.7861, test loss 2.7747, lr 3.00e-04\n",
      "step 5500: train loss 2.7844, test loss 2.7745, lr 3.00e-04\n",
      "step 5600: train loss 2.7814, test loss 2.7754, lr 3.00e-04\n",
      "step 5700: train loss 2.7796, test loss 2.7751, lr 3.00e-04\n",
      "step 5800: train loss 2.7779, test loss 2.7782, lr 3.00e-04\n",
      "step 5900: train loss 2.7806, test loss 2.7779, lr 3.00e-04\n",
      "step 6000: train loss 2.7826, test loss 2.7746, lr 3.00e-04\n",
      "Saved checkpoint at iteration 6000\n",
      "step 6100: train loss 2.7817, test loss 2.7742, lr 3.00e-04\n",
      "step 6200: train loss 2.7793, test loss 2.7741, lr 3.00e-04\n",
      "step 6300: train loss 2.7808, test loss 2.7742, lr 3.00e-04\n",
      "step 6400: train loss 2.7779, test loss 2.7740, lr 3.00e-04\n",
      "step 6500: train loss 2.7821, test loss 2.7726, lr 3.00e-04\n",
      "Saved checkpoint at iteration 6500\n",
      "step 6600: train loss 2.7809, test loss 2.7714, lr 3.00e-04\n",
      "Saved checkpoint at iteration 6600\n",
      "step 6700: train loss 2.7785, test loss 2.7733, lr 3.00e-04\n",
      "step 6800: train loss 2.7780, test loss 2.7775, lr 3.00e-04\n",
      "step 6900: train loss 2.7778, test loss 2.7767, lr 3.00e-04\n",
      "step 7000: train loss 2.7840, test loss 2.7788, lr 3.00e-04\n",
      "Saved checkpoint at iteration 7000\n",
      "step 7100: train loss 2.7801, test loss 2.7755, lr 3.00e-04\n",
      "step 7200: train loss 2.7805, test loss 2.7731, lr 3.00e-04\n",
      "step 7300: train loss 2.7792, test loss 2.7745, lr 3.00e-04\n",
      "step 7400: train loss 2.7818, test loss 2.7769, lr 3.00e-04\n",
      "step 7500: train loss 2.7809, test loss 2.7780, lr 3.00e-04\n",
      "step 7600: train loss 2.7815, test loss 2.7772, lr 3.00e-04\n",
      "step 7700: train loss 2.7814, test loss 2.7746, lr 3.00e-04\n",
      "step 7800: train loss 2.7804, test loss 2.7721, lr 3.00e-04\n",
      "step 7900: train loss 2.7817, test loss 2.7741, lr 3.00e-04\n",
      "step 8000: train loss 2.7770, test loss 2.7738, lr 3.00e-04\n",
      "Saved checkpoint at iteration 8000\n",
      "step 8100: train loss 2.7798, test loss 2.7741, lr 3.00e-04\n",
      "step 8200: train loss 2.7761, test loss 2.7730, lr 3.00e-04\n",
      "step 8300: train loss 2.7794, test loss 2.7704, lr 3.00e-04\n",
      "Saved checkpoint at iteration 8300\n",
      "step 8400: train loss 2.7799, test loss 2.7748, lr 3.00e-04\n",
      "step 8500: train loss 2.7790, test loss 2.7675, lr 3.00e-04\n",
      "Saved checkpoint at iteration 8500\n",
      "step 8600: train loss 2.7791, test loss 2.7688, lr 3.00e-04\n",
      "step 8700: train loss 2.7807, test loss 2.7672, lr 3.00e-04\n",
      "Saved checkpoint at iteration 8700\n",
      "step 8800: train loss 2.7802, test loss 2.7679, lr 3.00e-04\n",
      "step 8900: train loss 2.7771, test loss 2.7699, lr 3.00e-04\n",
      "step 9000: train loss 2.7804, test loss 2.7698, lr 3.00e-04\n",
      "Saved checkpoint at iteration 9000\n",
      "step 9100: train loss 2.7816, test loss 2.7712, lr 3.00e-04\n",
      "step 9200: train loss 2.7769, test loss 2.7714, lr 3.00e-04\n",
      "step 9300: train loss 2.7766, test loss 2.7716, lr 3.00e-04\n",
      "step 9400: train loss 2.7794, test loss 2.7678, lr 3.00e-04\n",
      "step 9500: train loss 2.7788, test loss 2.7698, lr 3.00e-04\n",
      "step 9600: train loss 2.7770, test loss 2.7723, lr 3.00e-04\n",
      "step 9700: train loss 2.7791, test loss 2.7720, lr 3.00e-04\n",
      "step 9800: train loss 2.7784, test loss 2.7700, lr 3.00e-04\n",
      "step 9900: train loss 2.7811, test loss 2.7695, lr 3.00e-04\n",
      "step 10000: train loss 2.7772, test loss 2.7708, lr 3.00e-04\n",
      "Saved checkpoint at iteration 10000\n",
      "step 10100: train loss 2.7775, test loss 2.7706, lr 3.00e-04\n",
      "step 10200: train loss 2.7776, test loss 2.7721, lr 3.00e-04\n",
      "step 10300: train loss 2.7785, test loss 2.7679, lr 3.00e-04\n",
      "step 10400: train loss 2.7771, test loss 2.7676, lr 3.00e-04\n",
      "step 10500: train loss 2.7778, test loss 2.7694, lr 3.00e-04\n",
      "step 10600: train loss 2.7787, test loss 2.7676, lr 3.00e-04\n",
      "step 10700: train loss 2.7763, test loss 2.7730, lr 3.00e-04\n",
      "step 10800: train loss 2.7746, test loss 2.7692, lr 3.00e-04\n",
      "step 10900: train loss 2.7787, test loss 2.7696, lr 3.00e-04\n",
      "step 11000: train loss 2.7764, test loss 2.7694, lr 3.00e-04\n",
      "Saved checkpoint at iteration 11000\n",
      "step 11100: train loss 2.7745, test loss 2.7718, lr 3.00e-04\n",
      "step 11200: train loss 2.7728, test loss 2.7717, lr 3.00e-04\n",
      "step 11300: train loss 2.7768, test loss 2.7711, lr 3.00e-04\n",
      "step 11400: train loss 2.7770, test loss 2.7731, lr 3.00e-04\n",
      "step 11500: train loss 2.7792, test loss 2.7711, lr 3.00e-04\n",
      "step 11600: train loss 2.7743, test loss 2.7663, lr 3.00e-04\n",
      "Saved checkpoint at iteration 11600\n",
      "step 11700: train loss 2.7795, test loss 2.7711, lr 3.00e-04\n",
      "step 11800: train loss 2.7741, test loss 2.7708, lr 3.00e-05\n",
      "step 11900: train loss 2.7754, test loss 2.7693, lr 3.00e-05\n",
      "step 12000: train loss 2.7739, test loss 2.7688, lr 3.00e-05\n",
      "Saved checkpoint at iteration 12000\n",
      "step 12100: train loss 2.7728, test loss 2.7685, lr 3.00e-05\n",
      "step 12200: train loss 2.7723, test loss 2.7679, lr 3.00e-05\n",
      "step 12300: train loss 2.7722, test loss 2.7681, lr 3.00e-05\n",
      "step 12400: train loss 2.7734, test loss 2.7675, lr 3.00e-05\n",
      "step 12500: train loss 2.7724, test loss 2.7663, lr 3.00e-05\n",
      "step 12600: train loss 2.7731, test loss 2.7662, lr 3.00e-05\n",
      "Saved checkpoint at iteration 12600\n",
      "step 12700: train loss 2.7705, test loss 2.7656, lr 3.00e-05\n",
      "Saved checkpoint at iteration 12700\n",
      "step 12800: train loss 2.7699, test loss 2.7659, lr 3.00e-05\n",
      "step 12900: train loss 2.7712, test loss 2.7648, lr 3.00e-05\n",
      "Saved checkpoint at iteration 12900\n",
      "step 13000: train loss 2.7665, test loss 2.7652, lr 3.00e-05\n",
      "Saved checkpoint at iteration 13000\n",
      "step 13100: train loss 2.7715, test loss 2.7652, lr 3.00e-05\n",
      "step 13200: train loss 2.7694, test loss 2.7654, lr 3.00e-05\n",
      "step 13300: train loss 2.7752, test loss 2.7653, lr 3.00e-05\n",
      "step 13400: train loss 2.7701, test loss 2.7641, lr 3.00e-05\n",
      "Saved checkpoint at iteration 13400\n",
      "step 13500: train loss 2.7679, test loss 2.7637, lr 3.00e-05\n",
      "Saved checkpoint at iteration 13500\n",
      "step 13600: train loss 2.7687, test loss 2.7636, lr 3.00e-05\n",
      "Saved checkpoint at iteration 13600\n",
      "step 13700: train loss 2.7691, test loss 2.7638, lr 3.00e-05\n",
      "step 13800: train loss 2.7690, test loss 2.7637, lr 3.00e-05\n",
      "step 13900: train loss 2.7678, test loss 2.7633, lr 3.00e-05\n",
      "Saved checkpoint at iteration 13900\n",
      "step 14000: train loss 2.7711, test loss 2.7631, lr 3.00e-06\n",
      "Saved checkpoint at iteration 14000\n",
      "Saved checkpoint at iteration 14000\n",
      "step 14100: train loss 2.7675, test loss 2.7639, lr 3.00e-06\n",
      "step 14200: train loss 2.7697, test loss 2.7632, lr 3.00e-06\n",
      "step 14300: train loss 2.7686, test loss 2.7633, lr 3.00e-06\n",
      "step 14400: train loss 2.7707, test loss 2.7631, lr 3.00e-06\n",
      "step 14500: train loss 2.7690, test loss 2.7634, lr 3.00e-06\n",
      "step 14600: train loss 2.7701, test loss 2.7629, lr 3.00e-06\n",
      "Saved checkpoint at iteration 14600\n",
      "step 14700: train loss 2.7646, test loss 2.7631, lr 3.00e-06\n",
      "step 14800: train loss 2.7716, test loss 2.7629, lr 3.00e-06\n",
      "step 14900: train loss 2.7709, test loss 2.7630, lr 3.00e-06\n",
      "step 15000: train loss 2.7745, test loss 2.7634, lr 3.00e-07\n",
      "Saved checkpoint at iteration 15000\n",
      "step 15100: train loss 2.7693, test loss 2.7628, lr 3.00e-07\n",
      "Saved checkpoint at iteration 15100\n",
      "step 15200: train loss 2.7673, test loss 2.7632, lr 3.00e-07\n",
      "step 15300: train loss 2.7716, test loss 2.7629, lr 3.00e-07\n",
      "step 15400: train loss 2.7714, test loss 2.7627, lr 3.00e-07\n",
      "Saved checkpoint at iteration 15400\n",
      "step 15500: train loss 2.7688, test loss 2.7628, lr 3.00e-07\n",
      "step 15600: train loss 2.7695, test loss 2.7628, lr 3.00e-07\n",
      "step 15700: train loss 2.7710, test loss 2.7630, lr 3.00e-07\n",
      "step 15800: train loss 2.7694, test loss 2.7636, lr 3.00e-07\n",
      "step 15900: train loss 2.7663, test loss 2.7630, lr 3.00e-07\n",
      "step 16000: train loss 2.7716, test loss 2.7626, lr 3.00e-08\n",
      "Saved checkpoint at iteration 16000\n",
      "Saved checkpoint at iteration 16000\n",
      "step 16100: train loss 2.7674, test loss 2.7632, lr 3.00e-08\n",
      "step 16200: train loss 2.7713, test loss 2.7635, lr 3.00e-08\n",
      "step 16300: train loss 2.7674, test loss 2.7630, lr 3.00e-08\n",
      "step 16400: train loss 2.7664, test loss 2.7631, lr 3.00e-08\n",
      "step 16500: train loss 2.7685, test loss 2.7633, lr 3.00e-08\n",
      "step 16600: train loss 2.7702, test loss 2.7628, lr 3.00e-08\n",
      "step 16700: train loss 2.7725, test loss 2.7626, lr 3.00e-08\n",
      "step 16800: train loss 2.7695, test loss 2.7624, lr 3.00e-08\n",
      "Saved checkpoint at iteration 16800\n",
      "step 16900: train loss 2.7697, test loss 2.7628, lr 3.00e-08\n",
      "step 17000: train loss 2.7688, test loss 2.7630, lr 3.00e-09\n",
      "Saved checkpoint at iteration 17000\n",
      "step 17100: train loss 2.7670, test loss 2.7629, lr 3.00e-09\n",
      "step 17200: train loss 2.7721, test loss 2.7629, lr 3.00e-09\n",
      "step 17300: train loss 2.7695, test loss 2.7628, lr 3.00e-09\n",
      "step 17400: train loss 2.7705, test loss 2.7635, lr 3.00e-09\n",
      "step 17500: train loss 2.7664, test loss 2.7628, lr 3.00e-09\n",
      "step 17600: train loss 2.7717, test loss 2.7625, lr 3.00e-09\n",
      "step 17700: train loss 2.7664, test loss 2.7630, lr 3.00e-09\n",
      "step 17800: train loss 2.7691, test loss 2.7627, lr 3.00e-09\n",
      "step 17900: train loss 2.7712, test loss 2.7625, lr 3.00e-09\n",
      "step 18000: train loss 2.7681, test loss 2.7632, lr 3.00e-09\n",
      "Saved checkpoint at iteration 18000\n",
      "step 18100: train loss 2.7678, test loss 2.7624, lr 3.00e-09\n",
      "Saved checkpoint at iteration 18100\n",
      "step 18200: train loss 2.7682, test loss 2.7631, lr 3.00e-09\n",
      "step 18300: train loss 2.7675, test loss 2.7628, lr 3.00e-09\n",
      "step 18400: train loss 2.7690, test loss 2.7627, lr 3.00e-09\n",
      "step 18500: train loss 2.7699, test loss 2.7632, lr 3.00e-09\n",
      "step 18600: train loss 2.7702, test loss 2.7629, lr 3.00e-09\n",
      "step 18700: train loss 2.7701, test loss 2.7630, lr 3.00e-09\n",
      "step 18800: train loss 2.7690, test loss 2.7628, lr 3.00e-09\n",
      "step 18900: train loss 2.7721, test loss 2.7622, lr 3.00e-09\n",
      "Saved checkpoint at iteration 18900\n",
      "step 19000: train loss 2.7694, test loss 2.7628, lr 3.00e-09\n",
      "Saved checkpoint at iteration 19000\n",
      "step 19100: train loss 2.7691, test loss 2.7626, lr 3.00e-09\n",
      "step 19200: train loss 2.7683, test loss 2.7627, lr 3.00e-09\n",
      "step 19300: train loss 2.7709, test loss 2.7628, lr 3.00e-09\n",
      "step 19400: train loss 2.7688, test loss 2.7628, lr 3.00e-09\n",
      "step 19500: train loss 2.7698, test loss 2.7627, lr 3.00e-09\n",
      "step 19600: train loss 2.7712, test loss 2.7630, lr 3.00e-09\n",
      "step 19700: train loss 2.7690, test loss 2.7630, lr 3.00e-09\n",
      "step 19800: train loss 2.7682, test loss 2.7628, lr 3.00e-09\n",
      "step 19900: train loss 2.7694, test loss 2.7627, lr 3.00e-09\n",
      "step 19999: train loss 2.7712, test loss 2.7629, lr 3.00e-09\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# hyperparameters\n",
    "batch_size = 64 # how many independent sequences will we process in parallel?\n",
    "block_size = sequence_len#32 # what is the maximum context length for predictions?\n",
    "max_iters = 20000\n",
    "eval_interval = 100\n",
    "learning_rate = 3e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 256#//4\n",
    "n_head = 2\n",
    "n_layer = 4\n",
    "dropout = 0.1\n",
    "N = 5\n",
    "c_q=2\n",
    "c_k=2*N -1\n",
    "c_h=2\n",
    "\n",
    "# ------------\n",
    "SEED = 1337\n",
    "torch.manual_seed(SEED)\n",
    "# For maximum reproducibility\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.use_deterministic_algorithms(False) \n",
    "# 1. Set environment variables for CuBLAS determinism\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'  # or ':16:8'\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "\n",
    "# 2. Set Python, NumPy and PyTorch seeds\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# 3. Configure PyTorch for deterministic operations\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "# Add these near your hyperparameters\n",
    "checkpoint_dir = \"./checkpoints_stan\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "checkpoint_interval = 1000  # Save every 1000 iterations\n",
    "best_test_loss = float('inf')\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "g = torch.Generator()\n",
    "g.manual_seed(SEED)\n",
    "\n",
    "# Add this function to save checkpoints\n",
    "def save_checkpoint(iteration, model, optimizer, scheduler, best=False):\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    checkpoint = {\n",
    "        'iteration': iteration,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n",
    "        'loss': best_test_loss,\n",
    "        'timestamp': timestamp\n",
    "    }\n",
    "    \n",
    "    filename = f\"checkpoint_{iteration}.pt\" if not best else \"best_model.pt\"\n",
    "    torch.save(checkpoint, os.path.join(checkpoint_dir, filename))\n",
    "    print(f\"Saved checkpoint at iteration {iteration}\")\n",
    "\n",
    "# Add this function to load checkpoints\n",
    "def load_checkpoint(path, model, optimizer, scheduler):\n",
    "    checkpoint = torch.load(path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    if scheduler and checkpoint['scheduler_state_dict']:\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    return checkpoint['iteration']\n",
    "train_dataset = MTADataset(f\"{path_folder}mta_train.txt\", variant=\"all\")#, tokenizer)\n",
    "# Modified DataLoader with deterministic shuffling\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    worker_init_fn=seed_worker,\n",
    "    generator=g,\n",
    "    num_workers=4,\n",
    "    persistent_workers=True\n",
    ")\n",
    "test_dataset = MTADataset(f\"{path_folder}mta_test.txt\", variant=\"all\")#, tokenizer)\n",
    "# Modified DataLoader with deterministic shuffling\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    worker_init_fn=seed_worker,\n",
    "    generator=g,\n",
    "    num_workers=4,\n",
    "    persistent_workers=True\n",
    ")\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'test']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        if split == 'test':\n",
    "            dataloader = test_dataloader\n",
    "        else:\n",
    "            dataloader = train_dataloader\n",
    "        dataloader_iter = iter(dataloader)\n",
    "        for k in range(eval_iters):\n",
    "            try:\n",
    "                batch = next(dataloader_iter)\n",
    "                # process the batch\n",
    "            except StopIteration:\n",
    "                # Reinitialize the iterator if you reach the end\n",
    "                dataloader_iter = iter(dataloader)\n",
    "                batch = next(dataloader_iter)\n",
    "            blocks = batch['blocks'].to(device)\n",
    "            queries = batch['query'].to(device)\n",
    "            targets = batch['target'].to(device)\n",
    "            logits, loss = model(blocks, queries, targets)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = StandardMultiAttention(n_head, head_size)\n",
    "        self.ffwd_blocks = FeedFoward(n_embd)\n",
    "        self.ffwd_queries = FeedFoward(n_embd)\n",
    "\n",
    "        self.blocks_proj  = nn.Linear(n_embd, n_embd) \n",
    "        self.ln_blocks = nn.LayerNorm(n_embd)\n",
    "        self.ln_queries = nn.LayerNorm(n_embd)\n",
    "\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, blocks_pos_emb, queries_emb):\n",
    "        blocks_pos_emb = self.ln_blocks(blocks_pos_emb)\n",
    "        queries_emb = self.ln_queries(queries_emb)\n",
    "        attn_out = self.sa(blocks_pos_emb, queries_emb)\n",
    "        blocks_pos_emb_attn = self.blocks_proj(attn_out.mean(dim=1))\n",
    "        blocks_pos_emb = blocks_pos_emb + blocks_pos_emb_attn.unsqueeze(1)\n",
    "        queries_emb = queries_emb + attn_out\n",
    "        queries_emb = queries_emb + self.ffwd_queries(self.ln2(queries_emb))\n",
    "        blocks_pos_emb = blocks_pos_emb + self.ffwd_blocks(self.ln2(blocks_pos_emb))\n",
    "\n",
    "        return blocks_pos_emb,queries_emb\n",
    "\n",
    "# super simple bigram model\n",
    "class StandardAttentionRetrievalModel(nn.Module):\n",
    "\n",
    "    def __init__(self,variant=\"all\"):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.ModuleList([Block(n_embd, n_head) for _ in range(n_layer)]) #nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.variant = variant\n",
    "\n",
    "        if self.variant == \"position\":\n",
    "            self.lm_head = nn.Linear(L*n_embd, num_blocks)\n",
    "        elif self.variant == \"all\":\n",
    "            self.lm_head = nn.Sequential(\n",
    "                nn.Linear(n_embd*L, n_embd * 2),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_embd * 2, vocab_size * N)\n",
    "            )\n",
    "        elif self.variant in ['first', 'last']:\n",
    "            self.lm_head = nn.Linear(n_embd*L, vocab_size)\n",
    "        else:\n",
    "            raise f\"{self.variant} is not recognized as a supported variant type\"\n",
    "\n",
    "    def forward(self, blocks, queries, targets=None):\n",
    "        B, T = blocks.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        blocks_emb = self.token_embedding_table(blocks) # (B,T,C)\n",
    "        queries_emb = self.token_embedding_table(queries) # (B,T,C)\n",
    "\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        blocks_pos_emb = blocks_emb + pos_emb # (B,T,C)\n",
    "        for block in self.blocks:\n",
    "            blocks_pos_emb, queries_emb = block(blocks_pos_emb, queries_emb)\n",
    "        # x = self.blocks(blocks_pos_emb,queries_emb) # (B,T,C)\n",
    "        x = self.ln_f(queries_emb) # (B,T,C)\n",
    "        # print(x.shape)\n",
    "        logits = self.lm_head(x.view(B,-1)) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, C = logits.shape\n",
    "            if self.variant == \"all\":\n",
    "                # logits = logits.view(-1, vocab_size)\n",
    "                \n",
    "                logits = logits.view(B*N, vocab_size)\n",
    "                targets = targets.view(B*N)\n",
    "            else:\n",
    "                # logits = logits.view(B, vocab_size)\n",
    "                \n",
    "                logits = logits.view(B, vocab_size)\n",
    "                targets = targets.view(B)                \n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "model = StandardAttentionRetrievalModel(variant=\"all\")\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, \n",
    "    mode='min', \n",
    "    factor=0.1, \n",
    "    patience=1000,  # Number of eval intervals to wait before reducing LR\n",
    "    verbose=True\n",
    ")\n",
    "# scheduler = lr_scheduler.LambdaLR\n",
    "start_scheduler_at_iter = 10000  \n",
    "resume_from_checkpoint = False\n",
    "\n",
    "dataloader = train_dataloader\n",
    "dataloader_iter = iter(dataloader)\n",
    "if resume_from_checkpoint:\n",
    "    start_iter = load_checkpoint(\"checkpoints_stan/best_model.pt\", model, optimizer, scheduler)\n",
    "else:\n",
    "    start_iter = 0\n",
    "for i in tqdm(range(start_iter, max_iters)):\n",
    "\n",
    "    if i % eval_interval == 0 or i == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {i}: train loss {losses['train']:.4f}, test loss {losses['test']:.4f}, lr {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "        # Save best model\n",
    "        if losses['test'] < best_test_loss:\n",
    "            best_test_loss = losses['test']\n",
    "            save_checkpoint(i, model, optimizer, scheduler, best=True)\n",
    "    \n",
    "    # Regular checkpoint saving\n",
    "    if i % checkpoint_interval == 0 and i > 0:\n",
    "        save_checkpoint(i, model, optimizer, scheduler)\n",
    "        \n",
    "\n",
    "    try:\n",
    "        batch = next(dataloader_iter)\n",
    "        # process the batch\n",
    "    except StopIteration:\n",
    "        # Reinitialize the iterator if you reach the end\n",
    "        dataloader_iter = iter(dataloader)\n",
    "        batch = next(dataloader_iter)\n",
    "    blocks = batch['blocks'].to(device)\n",
    "    queries = batch['query'].to(device)\n",
    "    targets = batch['target'].to(device)\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(blocks, queries, targets)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # Only step the scheduler after start_scheduler_at_iter\n",
    "    if i >= start_scheduler_at_iter:\n",
    "        scheduler.step(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43f7d67",
   "metadata": {
    "papermill": {
     "duration": 0.022955,
     "end_time": "2025-08-12T17:26:30.670467",
     "exception": false,
     "start_time": "2025-08-12T17:26:30.647512",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "562dab51",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-12T17:26:30.717281Z",
     "iopub.status.busy": "2025-08-12T17:26:30.716943Z",
     "iopub.status.idle": "2025-08-12T17:26:30.845050Z",
     "shell.execute_reply": "2025-08-12T17:26:30.844323Z"
    },
    "papermill": {
     "duration": 0.153246,
     "end_time": "2025-08-12T17:26:30.846187",
     "exception": false,
     "start_time": "2025-08-12T17:26:30.692941",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardAttentionRetrievalModel(\n",
       "  (token_embedding_table): Embedding(27, 256)\n",
       "  (position_embedding_table): Embedding(299, 256)\n",
       "  (blocks): ModuleList(\n",
       "    (0-3): 4 x Block(\n",
       "      (sa): StandardMultiAttention(\n",
       "        (block_proj): Linear(in_features=256, out_features=512, bias=True)\n",
       "        (query_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "      (ffwd_blocks): FeedFoward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ffwd_queries): FeedFoward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (blocks_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (ln_blocks): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_queries): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "  (lm_head): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=135, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(\"checkpoints_stan/best_model.pt\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "74979c86",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-12T17:26:30.893100Z",
     "iopub.status.busy": "2025-08-12T17:26:30.892874Z",
     "iopub.status.idle": "2025-08-12T17:26:31.511043Z",
     "shell.execute_reply": "2025-08-12T17:26:31.510134Z"
    },
    "papermill": {
     "duration": 0.642882,
     "end_time": "2025-08-12T17:26:31.512402",
     "exception": false,
     "start_time": "2025-08-12T17:26:30.869520",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "873b90310cf444df88337864c738574d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "test_dataset = MTADataset(f\"{path_folder}mta_test.txt\", variant=\"all\")#, tokenizer)\n",
    "# Modified DataLoader with deterministic shuffling\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    worker_init_fn=seed_worker,\n",
    "    generator=g,\n",
    "    num_workers=4,\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "full_output = np.array([])\n",
    "full_targets = np.array([])\n",
    "for batch in tqdm(test_dataloader):\n",
    "    blocks = batch['blocks'].to(device)\n",
    "    queries = batch['query'].to(device)\n",
    "    targets = batch['target'].to(device)\n",
    "    \n",
    "    # evaluate the loss\n",
    "    logits, loss = model(blocks, queries, targets=None)\n",
    "    output = torch.argmax(logits.view(-1, N, vocab_size), dim=-1)\n",
    "    \n",
    "    output = np.array([decode(i.tolist()) for i in output])\n",
    "    full_output = np.append(full_output, output)\n",
    "    targets = np.array([decode(i.tolist()) for i in targets])\n",
    "    full_targets = np.append(full_targets, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcac1e96",
   "metadata": {
    "papermill": {
     "duration": 0.030876,
     "end_time": "2025-08-12T17:26:31.567173",
     "exception": false,
     "start_time": "2025-08-12T17:26:31.536297",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Accuracy of the Standard Attention Model on the test set is 0.0000%\n"
     ]
    }
   ],
   "source": [
    "acc = sum((full_output == full_targets).astype(int))/ len(targets)\n",
    "print(f\"The Accuracy of the Standard Attention Model on the test set is {acc:.4f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cfcf04",
   "metadata": {
    "papermill": {
     "duration": 0.022668,
     "end_time": "2025-08-12T17:26:31.612745",
     "exception": false,
     "start_time": "2025-08-12T17:26:31.590077",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30af665",
   "metadata": {
    "papermill": {
     "duration": 0.022988,
     "end_time": "2025-08-12T17:26:31.658891",
     "exception": false,
     "start_time": "2025-08-12T17:26:31.635903",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392eb6ad",
   "metadata": {
    "papermill": {
     "duration": 0.023391,
     "end_time": "2025-08-12T17:26:31.705538",
     "exception": false,
     "start_time": "2025-08-12T17:26:31.682147",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50d2409",
   "metadata": {
    "papermill": {
     "duration": 0.022774,
     "end_time": "2025-08-12T17:26:31.751756",
     "exception": false,
     "start_time": "2025-08-12T17:26:31.728982",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8e6eae",
   "metadata": {
    "papermill": {
     "duration": 0.023426,
     "end_time": "2025-08-12T17:26:31.798053",
     "exception": false,
     "start_time": "2025-08-12T17:26:31.774627",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5813d1",
   "metadata": {
    "papermill": {
     "duration": 0.022822,
     "end_time": "2025-08-12T17:26:31.844042",
     "exception": false,
     "start_time": "2025-08-12T17:26:31.821220",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6714c9a1",
   "metadata": {
    "papermill": {
     "duration": 0.02275,
     "end_time": "2025-08-12T17:26:31.889646",
     "exception": false,
     "start_time": "2025-08-12T17:26:31.866896",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38de4b1d",
   "metadata": {
    "papermill": {
     "duration": 0.022725,
     "end_time": "2025-08-12T17:26:31.935112",
     "exception": false,
     "start_time": "2025-08-12T17:26:31.912387",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc824c3",
   "metadata": {
    "papermill": {
     "duration": 0.022792,
     "end_time": "2025-08-12T17:26:31.980669",
     "exception": false,
     "start_time": "2025-08-12T17:26:31.957877",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8054090,
     "sourceId": 12741303,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 6658.726313,
   "end_time": "2025-08-12T17:26:34.730292",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-08-12T15:35:36.003979",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
