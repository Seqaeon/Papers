{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb179a06-481b-4c13-93a2-ea10e3665844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n",
    "# !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a00beca-4701-413c-9206-9505252b08c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read it in to inspect it\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52b7c16-a573-4f38-ac27-3601ac16f85d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce421f29-273b-48cf-8f24-1e14d3248bc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a120cce8-aa67-4db1-946f-03b9a0aacf0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.208705 M parameters\n",
      "Dense FFN parameters: 33,088\n",
      "step 0: train loss 4.3402, val loss 4.3471\n",
      "step 1: train loss 4.1937, val loss 4.1881\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "# read it in to inspect it\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "# hyperparameters\n",
    "batch_size = 8 # how many independent sequences will we process in parallel?\n",
    "block_size = 16 # what is the maximum context length for predictions?\n",
    "max_iters = 2#000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 50\n",
    "n_embd = 64\n",
    "n_head = 8\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "training = True\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "dense_ffn_params = sum(p.numel() for p in model.blocks[0].ffwd.parameters()) \n",
    "print(f\"Dense FFN parameters: {dense_ffn_params:,}\")\n",
    "\n",
    "\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# # generate from the model\n",
    "# context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "# print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e4cff5-573b-4e01-8994-f8c9bccce06f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755b3813-45da-4ccd-97b8-58f094c0e9f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16a35ae-67a5-40b1-9275-5c19d9e9f888",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a03f5ea-8b7e-4b57-ab01-4b11bf966712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.216705 M parameters\n",
      "Total parameters: 0.22M parameters\n",
      "Parameters per expert: 2,128\n",
      "Total expert parameters: 34,048\n",
      "Gate parameters: 1,040\n",
      "Total MoE parameters: 35,088\n",
      "Dense FFN parameters: 33,088\n",
      "Equivalent experts: 1.1\n",
      "Total MoE parameters from model: 35,088\n",
      "Parameters For all experts from model (layer 0): 34,048\n",
      "Parameters per expert (layer 0) from model: 2,128\n",
      "At Sparsity 100.0%, Loss at 2 is: 4.2583160400390625\n",
      "Parameters of Top 0 out of 16 Experts with Drop Rate of 0.037109375: 0\n",
      "At Sparsity 93.75%, Loss at 2 is: 4.174821376800537\n",
      "Parameters of Top 1 out of 16 Experts with Drop Rate of 0.06640625: 2,128\n",
      "At Sparsity 87.5%, Loss at 2 is: 3.9646475315093994\n",
      "Parameters of Top 2 out of 16 Experts with Drop Rate of 0.076171875: 4,256\n",
      "At Sparsity 81.25%, Loss at 2 is: 3.8845980167388916\n",
      "Parameters of Top 3 out of 16 Experts with Drop Rate of 0.115234375: 6,384\n",
      "At Sparsity 75.0%, Loss at 2 is: 3.7866714000701904\n",
      "Parameters of Top 4 out of 16 Experts with Drop Rate of 0.130859375: 8,512\n",
      "At Sparsity 68.75%, Loss at 2 is: 3.6313719749450684\n",
      "Parameters of Top 5 out of 16 Experts with Drop Rate of 0.197265625: 10,640\n",
      "At Sparsity 62.5%, Loss at 2 is: 3.6017394065856934\n",
      "Parameters of Top 6 out of 16 Experts with Drop Rate of 0.240234375: 12,768\n",
      "At Sparsity 56.25%, Loss at 2 is: 3.6200218200683594\n",
      "Parameters of Top 7 out of 16 Experts with Drop Rate of 0.275390625: 14,896\n",
      "At Sparsity 50.0%, Loss at 2 is: 3.3763959407806396\n",
      "Parameters of Top 8 out of 16 Experts with Drop Rate of 0.361328125: 17,024\n",
      "At Sparsity 43.75%, Loss at 2 is: 3.6553196907043457\n",
      "Parameters of Top 9 out of 16 Experts with Drop Rate of 0.345703125: 19,152\n",
      "At Sparsity 37.5%, Loss at 2 is: 3.3516085147857666\n",
      "Parameters of Top 10 out of 16 Experts with Drop Rate of 0.37109375: 21,280\n",
      "At Sparsity 31.25%, Loss at 2 is: 3.4555647373199463\n",
      "Parameters of Top 11 out of 16 Experts with Drop Rate of 0.416015625: 23,408\n",
      "At Sparsity 25.0%, Loss at 2 is: 3.3187742233276367\n",
      "Parameters of Top 12 out of 16 Experts with Drop Rate of 0.431640625: 25,536\n",
      "At Sparsity 18.75%, Loss at 2 is: 3.320791244506836\n",
      "Parameters of Top 13 out of 16 Experts with Drop Rate of 0.44921875: 27,664\n",
      "At Sparsity 12.5%, Loss at 2 is: 3.1812570095062256\n",
      "Parameters of Top 14 out of 16 Experts with Drop Rate of 0.482421875: 29,792\n",
      "At Sparsity 6.25%, Loss at 2 is: 3.15624737739563\n",
      "Parameters of Top 15 out of 16 Experts with Drop Rate of 0.48046875: 31,920\n",
      "At Sparsity 0.0%, Loss at 2 is: 3.17948842048645\n",
      "Parameters of Top 16 out of 16 Experts with Drop Rate of 0.521484375: 34,048\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from torch.nn import Parameter\n",
    "import math\n",
    "\n",
    "\n",
    "# read it in to inspect it\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "# hyperparameters\n",
    "batch_size = 8 # how many independent sequences will we process in parallel?\n",
    "block_size = 16 # what is the maximum context length for predictions?\n",
    "max_iters = 2\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 50\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "num_experts = 16\n",
    "top_k = 2\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss, total_loss,_ = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head,num_experts, top_k, dropout=0.1):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        ff_hidden_dim = max(8, (4 * n_embd) // num_experts)  #max(32, (4 * n_embd) // (num_experts // 4)) \n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.moe = SparseMoE(n_embd, ff_hidden_dim, n_embd, num_experts, top_k, dropout=dropout)\n",
    "        # self._aux_loss = {}\n",
    "\n",
    "        # self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        # MoE feedforward with residual connection\n",
    "        moe_out, aux_loss = self.moe(self.ln2(x))\n",
    "        x = x + moe_out\n",
    "        # self._aux_loss = aux_loss \n",
    "        \n",
    "        return x, aux_loss\n",
    "\n",
    "\n",
    "    def get_balance_loss(self):\n",
    "        loss = self._balance_loss\n",
    "        self._balance_loss = 0.0 \n",
    "        return loss\n",
    "\n",
    "\n",
    "\n",
    "class Expert(nn.Module):\n",
    "    \"\"\"Individual expert network - a simple feedforward network\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, dropout=0.1):\n",
    "        super(Expert, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TopKGate(nn.Module):\n",
    "    \"\"\"\n",
    "    Gating network with capacity constraints, next-best assignment, and auxiliary loss\n",
    "    to minimize token drops during training.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, num_experts, top_k=2, capacity_factor=1.5):\n",
    "        super().__init__()\n",
    "        self.num_experts = num_experts\n",
    "        self.top_k = top_k\n",
    "        self.capacity_factor = capacity_factor  # 1.5 is reasonable - not too wasteful\n",
    "        self.gate = nn.Linear(input_dim, num_experts, bias=True)\n",
    "        nn.init.zeros_(self.gate.bias)  # Start with equal expert probability\n",
    "        self.noise_epsilon = 1e-2\n",
    "        \n",
    "        # Auxiliary loss weights\n",
    "        self.load_balance_weight = 0.01\n",
    "        self.drop_penalty_weight = 1.0  # Strong penalty for drops\n",
    "        self.router_balance_weight = 1e-4\n",
    "        \n",
    "    def calculate_expert_capacity(self, num_tokens):\n",
    "        \"\"\"Calculate per-expert capacity based on total tokens\"\"\"\n",
    "        base_capacity = num_tokens // self.num_experts\n",
    "        return int(base_capacity * self.capacity_factor)\n",
    "    \n",
    "    def assign_tokens_with_capacity(self, gate_probs, expert_capacity):\n",
    "        \"\"\"\n",
    "        Assign tokens to experts with capacity constraints using next-best strategy\n",
    "        \n",
    "        Returns:\n",
    "            assignments: tensor of shape (num_tokens,) with expert indices (-1 for dropped)\n",
    "            expert_masks: tensor of shape (num_tokens, num_experts) for routing\n",
    "            assignment_probs: tensor of shape (num_tokens,) with routing probabilities\n",
    "        \"\"\"\n",
    "        num_tokens, num_experts = gate_probs.shape\n",
    "        expert_counts = torch.zeros(num_experts, device=gate_probs.device, dtype=torch.long)\n",
    "        assignments = torch.full((num_tokens,), -1, device=gate_probs.device, dtype=torch.long)\n",
    "        assignment_probs = torch.zeros(num_tokens, device=gate_probs.device)\n",
    "        \n",
    "        # Sort all tokens by their maximum gate probability (process confident assignments first)\n",
    "        max_probs, _ = gate_probs.max(dim=1)\n",
    "        sorted_token_indices = torch.argsort(max_probs, descending=True)\n",
    "        dropped_tokens_count = 0\n",
    "        \n",
    "        for token_idx in sorted_token_indices:\n",
    "            token_probs = gate_probs[token_idx]\n",
    "            # Get all experts sorted by preference (not just top-k)\n",
    "            expert_preferences = torch.argsort(token_probs, descending=True)\n",
    "            \n",
    "            assigned = False\n",
    "            \n",
    "            # First, try the top-k experts\n",
    "            for rank, expert_idx in enumerate(expert_preferences[:self.top_k]):\n",
    "                if expert_counts[expert_idx] < expert_capacity:\n",
    "                    assignments[token_idx] = expert_idx\n",
    "                    assignment_probs[token_idx] = token_probs[expert_idx]\n",
    "                    expert_counts[expert_idx] += 1\n",
    "                    assigned = True\n",
    "                    break\n",
    "            \n",
    "            # If top-k experts are all full, try next-best available experts\n",
    "            if not assigned:\n",
    "                dropped_tokens_count += 1\n",
    "                for expert_idx in expert_preferences[self.top_k:]:\n",
    "                    if expert_counts[expert_idx] < expert_capacity:\n",
    "                        assignments[token_idx] = expert_idx\n",
    "                        assignment_probs[token_idx] = token_probs[expert_idx]\n",
    "                        expert_counts[expert_idx] += 1\n",
    "                        assigned = True\n",
    "                        break\n",
    "            \n",
    "            # If still not assigned, token will be dropped (assignments[token_idx] remains -1)\n",
    "        \n",
    "        # Create expert masks for routing\n",
    "        expert_masks = torch.zeros(num_tokens, num_experts, device=gate_probs.device)\n",
    "        valid_assignments = assignments >= 0\n",
    "        expert_masks[valid_assignments, assignments[valid_assignments]] = 1.0\n",
    "        \n",
    "        return assignments, expert_masks, assignment_probs, expert_counts,dropped_tokens_count\n",
    "    \n",
    "    def calculate_auxiliary_losses(self, gate_probs, assignments, expert_counts,dropped_tokens_count):\n",
    "        \"\"\"Calculate all auxiliary losses\"\"\"\n",
    "        num_tokens = gate_probs.shape[0]\n",
    "        \n",
    "        # 1. Load Balance Loss (encourage uniform expert usage)\n",
    "        mean_gate = gate_probs.mean(dim=0)\n",
    "        cv_squared = gate_probs.var(dim=0) / (mean_gate + 1e-8)\n",
    "        load_balance_loss = cv_squared.mean()\n",
    "        \n",
    "        # 2. Drop Penalty Loss (heavily penalize dropped tokens)\n",
    "        num_dropped = (assignments == -1).sum().float()\n",
    "        drop_rate = dropped_tokens_count / num_tokens\n",
    "        drop_penalty_loss = drop_rate  # Linear penalty on drop rate\n",
    "        \n",
    "        # 3. Capacity Utilization Loss (encourage using available capacity)\n",
    "        target_capacity = num_tokens // self.num_experts\n",
    "        capacity_diff = expert_counts.float() - target_capacity\n",
    "        capacity_imbalance = (capacity_diff ** 2).mean()\n",
    "        \n",
    "        # 4. Router Z-Loss (from PaLM paper - prevents logit scale explosion)\n",
    "        router_z_loss = torch.logsumexp(gate_probs, dim=-1).pow(2).mean()\n",
    "        \n",
    "        return {\n",
    "            'load_balance_loss': load_balance_loss,\n",
    "            'drop_penalty_loss': drop_penalty_loss,\n",
    "            'capacity_imbalance': capacity_imbalance,\n",
    "            'router_z_loss': router_z_loss,  # Small weight\n",
    "            'drop_rate': drop_rate,\n",
    "            'num_dropped': num_dropped.item(),\n",
    "            'num_dropped_pre_next_best': dropped_tokens_count\n",
    "        }\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass with capacity constraints\n",
    "        \n",
    "        Returns:\n",
    "            expert_masks: (batch_size * seq_len, num_experts) - binary masks for routing\n",
    "            assignment_probs: (batch_size * seq_len,) - probabilities for assigned experts\n",
    "            aux_losses: dict of auxiliary losses and metrics\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, input_dim = x.shape\n",
    "        num_tokens = batch_size * seq_len\n",
    "        x_flat = x.view(-1, input_dim)\n",
    "        \n",
    "        # Calculate expert capacity\n",
    "        expert_capacity = self.calculate_expert_capacity(num_tokens)\n",
    "        \n",
    "        # Gate computation\n",
    "        gate_logits = self.gate(x_flat)\n",
    "        \n",
    "        # Add noise during training for exploration\n",
    "        if self.training:\n",
    "            gate_logits += torch.randn_like(gate_logits) * self.noise_epsilon\n",
    "        \n",
    "        gate_probs = F.softmax(gate_logits, dim=-1)\n",
    "        \n",
    "        # Assign tokens to experts with capacity constraints\n",
    "        assignments, expert_masks, assignment_probs, expert_counts, dropped_tokens_count = self.assign_tokens_with_capacity(gate_probs, expert_capacity)\n",
    "        \n",
    "        # Calculate auxiliary losses\n",
    "        aux_losses = self.calculate_auxiliary_losses(gate_probs, assignments, expert_counts,dropped_tokens_count)\n",
    "        effective_lb_weight = self.load_balance_weight * (self.top_k / 2) # Scale based on top_k\n",
    "\n",
    "        # Combine losses with weights\n",
    "        total_aux_loss = (\n",
    "            effective_lb_weight * aux_losses['load_balance_loss'] +\n",
    "            self.drop_penalty_weight * aux_losses['drop_penalty_loss'] +\n",
    "             self.router_balance_weight * aux_losses['router_z_loss']\n",
    "        )\n",
    "        aux_losses['total_aux_loss'] = total_aux_loss\n",
    "        # Monitor drop rates\n",
    "        # if aux_losses['drop_rate'] > 0.05:  # More than 5% drops\n",
    "        #     print(f\"Warning: High drop rate {aux_losses['drop_rate']:.2%}\")\n",
    "        \n",
    "        return expert_masks, assignment_probs, aux_losses\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SparseMoE(nn.Module):\n",
    "    \"\"\"Sparse Mixture of Experts Layer\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_experts=8, top_k=2, \n",
    "                 capacity_factor=1.25, dropout=0.1):\n",
    "        super(SparseMoE, self).__init__()\n",
    "        self.num_experts = num_experts\n",
    "        self.top_k = top_k\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        # Create expert networks\n",
    "        self.experts = nn.ModuleList([\n",
    "            Expert(input_dim, hidden_dim, output_dim, dropout) \n",
    "            for _ in range(num_experts)\n",
    "        ])\n",
    "        \n",
    "        # Gating network\n",
    "        self.gate = TopKGate(input_dim, num_experts, top_k, capacity_factor)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, input_dim = x.shape\n",
    "        x_flat = x.view(-1, input_dim)  # (batch_size * seq_len, input_dim)\n",
    "        \n",
    "        # Get routing decisions from gate\n",
    "        # expert_indices, expert_weights, balance_loss = self.gate(x)\n",
    "        expert_masks, assignment_probs, aux_losses = self.gate(x)\n",
    "        \n",
    "        # Initialize output\n",
    "        output = torch.zeros(x_flat.shape[0], self.output_dim, \n",
    "                           device=x.device, dtype=x.dtype)\n",
    "\n",
    "        \n",
    "        # Process tokens through assigned experts\n",
    "        for expert_idx, expert in enumerate(self.experts):\n",
    "            # Find tokens assigned to this expert\n",
    "            expert_tokens = expert_masks[:, expert_idx] > 0\n",
    "            if expert_tokens.any():\n",
    "                # Get tokens for this expert\n",
    "                tokens = x_flat[expert_tokens]\n",
    "                # Process through expert\n",
    "                expert_output = expert(tokens)\n",
    "                # Weight by assignment probability\n",
    "                weights = assignment_probs[expert_tokens].unsqueeze(-1)\n",
    "                # Add to final output\n",
    "                output[expert_tokens] += expert_output * weights\n",
    "        \n",
    "        return output.view(batch_size, seq_len, -1), aux_losses\n",
    "\n",
    "\n",
    "\n",
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.ModuleList([Block(n_embd, n_head,num_experts, top_k) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "    #     # Initialize weights\n",
    "    #     self._init_weights()\n",
    "        \n",
    "    # def _init_weights(self):\n",
    "    #     for module in self.modules():\n",
    "    #         if isinstance(module, nn.Linear):\n",
    "    #             nn.init.xavier_uniform_(module.weight)\n",
    "    #             if module.bias is not None:\n",
    "    #                 nn.init.zeros_(module.bias)\n",
    "    #         elif isinstance(module, nn.Embedding):\n",
    "    #             nn.init.normal_(module.weight, std=0.02)\n",
    "    #         elif isinstance(module, nn.LayerNorm):\n",
    "    #             nn.init.ones_(module.weight)\n",
    "    #             nn.init.zeros_(module.bias)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        \n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        # Collect balance losses after forward pass\n",
    "        total_aux_loss = 0.0\n",
    "        aux_metrics = {'drop_rate': 0.0, 'load_balance': 0.0, 'total_aux_loss': 0.0}\n",
    "\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            x, block_metrics = block(x)\n",
    "            layer_weight = (i + 1) / len(self.blocks)\n",
    "            total_aux_loss += (block_metrics['total_aux_loss'] * layer_weight)\n",
    "            \n",
    "            # Optionally track per-block metrics\n",
    "            aux_metrics['drop_rate'] += block_metrics['drop_rate']\n",
    "            aux_metrics['load_balance'] += block_metrics['load_balance_loss']\n",
    "        \n",
    "        # Average metrics across blocks\n",
    "        aux_metrics = {k: v / len(self.blocks) for k, v in aux_metrics.items()}\n",
    "        aux_metrics['total_aux_loss'] = total_aux_loss\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            total_loss = loss + total_aux_loss\n",
    "\n",
    "        return logits, loss, total_loss,aux_metrics\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss, total_loss,_ = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "    # Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters()) / 1e6\n",
    "\n",
    "expert_params_model = sum(p.numel() for expert in model.blocks[0].moe.experts for p in expert.parameters()) \n",
    "total_moe_params_model = sum(p.numel() for p in model.blocks[0].moe.parameters()) \n",
    "expert_hidden_dim = (4*n_embd)// num_experts\n",
    "\n",
    "params_per_expert = (n_embd * expert_hidden_dim) + expert_hidden_dim + (expert_hidden_dim * n_embd) + n_embd\n",
    "expert_params = num_experts * params_per_expert\n",
    "\n",
    "# Gate network parameters\n",
    "gate_params = n_embd * num_experts + num_experts  # weights + bias\n",
    "\n",
    "# Total MoE parameters\n",
    "total_moe_params = expert_params + gate_params\n",
    "\n",
    "# Dense FFN equivalent\n",
    "dense_ffn_params = (n_embd * (4 * n_embd)) + (4 * n_embd) + ((4 * n_embd) * n_embd) + n_embd\n",
    "# print the number of parameters in the model\n",
    "print(f\"Total parameters: {total_params:.2f}M parameters\")\n",
    "print(f\"Parameters per expert: {params_per_expert:,}\")\n",
    "print(f\"Total expert parameters: {expert_params:,}\")\n",
    "print(f\"Gate parameters: {gate_params:,}\")\n",
    "print(f\"Total MoE parameters: {total_moe_params:,}\")\n",
    "print(f\"Dense FFN parameters: {dense_ffn_params:,}\")\n",
    "print(f\"Equivalent experts: {total_moe_params / dense_ffn_params:.1f}\")\n",
    "\n",
    "print(f\"Total MoE parameters from model: {total_moe_params_model:,}\")\n",
    "print(f\"Parameters For all experts from model (layer 0): {expert_params_model:,}\")\n",
    "print(f\"Parameters per expert (layer 0) from model: {expert_params_model // num_experts:,}\")\n",
    "\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "loss_dict = {}\n",
    "for i in range(num_experts+1):\n",
    "    top_k = i\n",
    "    \n",
    "    for iter in range(max_iters):\n",
    "    \n",
    "        # every once in a while evaluate the loss on train and val sets\n",
    "        if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "            losses = estimate_loss()\n",
    "            # print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "    \n",
    "        # sample a batch of data\n",
    "        xb, yb = get_batch('train')\n",
    "    \n",
    "        # evaluate the loss\n",
    "        logits, loss, total_loss,aux_metrics = model(xb, yb)\n",
    "        \n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    sparsity = round(((num_experts-top_k)/num_experts) * 100, 2)\n",
    "    drop_rate = aux_metrics['drop_rate']\n",
    "    print(f\"At Sparsity {sparsity}%, Loss at {max_iters} is: {loss}\")\n",
    "    expert_params = sum(p.numel() for expert in model.blocks[0].moe.experts for p in expert.parameters()) \n",
    "    print(f\"Parameters of Top {top_k} out of {num_experts} Experts with Drop Rate of {drop_rate}: {(expert_params // num_experts)*top_k:,}\")\n",
    "\n",
    "    loss_dict[f\"{sparsity}%\"] = loss.item()\n",
    "# # generate from the model\n",
    "# context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "# print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e04ae3-edf6-47ba-adc8-71e7399acf43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "706ab6f6-300e-485b-af39-145e0b5d7f4c",
   "metadata": {},
   "source": [
    "loss_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4acac1b5-c7ef-4ec9-a7e7-975070a5a6da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'100.0%': 4.2583160400390625,\n",
       " '93.75%': 4.174821376800537,\n",
       " '87.5%': 3.9646475315093994,\n",
       " '81.25%': 3.8845980167388916,\n",
       " '75.0%': 3.7866714000701904,\n",
       " '68.75%': 3.6313719749450684,\n",
       " '62.5%': 3.6017394065856934,\n",
       " '56.25%': 3.6200218200683594,\n",
       " '50.0%': 3.3763959407806396,\n",
       " '43.75%': 3.6553196907043457,\n",
       " '37.5%': 3.3516085147857666,\n",
       " '31.25%': 3.4555647373199463,\n",
       " '25.0%': 3.3187742233276367,\n",
       " '18.75%': 3.320791244506836,\n",
       " '12.5%': 3.1812570095062256,\n",
       " '6.25%': 3.15624737739563,\n",
       " '0.0%': 3.17948842048645}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4563bcdc-9ee2-4ffc-a484-6c99c7ee9049",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4.2583160400390625,\n",
       " 4.174821376800537,\n",
       " 3.9646475315093994,\n",
       " 3.8845980167388916,\n",
       " 3.7866714000701904,\n",
       " 3.6313719749450684,\n",
       " 3.6017394065856934,\n",
       " 3.6200218200683594,\n",
       " 3.3763959407806396,\n",
       " 3.6553196907043457,\n",
       " 3.3516085147857666,\n",
       " 3.4555647373199463,\n",
       " 3.3187742233276367,\n",
       " 3.320791244506836,\n",
       " 3.1812570095062256,\n",
       " 3.15624737739563,\n",
       " 3.17948842048645]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_labels = list(loss_dict.keys())\n",
    "x_values = [float(k.strip('%')) for k in x_labels]\n",
    "y_values = [v for v in loss_dict.values()]\n",
    "\n",
    "y_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a1f2680-4e6f-467d-8761-ac42253f08cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA90AAAHqCAYAAAAZLi26AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjUsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvWftoOwAAAAlwSFlzAAAPYQAAD2EBqD+naQAAwCFJREFUeJzs3XlYVOX7BvB7ZhiGdWDYFBVzLTU31MoVd03NssTcUrOyUvu2/yorEy1LzRZLM9PKJUkF0zJXXJBMzQ0Vd9AUFVT2AQZmfX9/IJPIIihwZob7c11eNWfODM/DYWDuOe95X5kQQoCIiIiIiIiIKp1c6gKIiIiIiIiIHBVDNxEREREREVEVYegmIiIiIiIiqiIM3URERERERERVhKGbiIiIiIiIqIowdBMRERERERFVEYZuIiIiIiIioirC0E1ERERERERURRi6iYiIiIiIiKoIQzcRERFVqwYNGuDZZ5+VugxJRUdHQyaTITo6WupSiIioijF0ExGRXVm6dClkMhkOHTokdSmS27BhA7p3746AgAC4ubmhUaNGePrpp7FlyxapS6uQU6dOISwsDBcvXqzU53322Wfh4eFRqc9JRERUUQzdREREdmju3Ll4/PHHIZPJMGXKFHz11VcYOnQo4uPjsWrVKqnLK9PZs2exePFi6+1Tp05h+vTplR66iYiIbIGT1AUQERFRxZhMJnz88cfo27cvtm3bVuz+GzduVHtNubm5cHd3L9e+KpWqiqshIiKyHTzTTUREDik2NhYDBgyAWq2Gh4cHevfujf379xfZx2g0Yvr06WjatClcXFzg6+uLrl27IioqyrrPtWvXMH78eNSrVw8qlQqBgYF44oknyjwrO3fuXMhkMly6dKnYfVOmTIGzszMyMjIAAPHx8Rg6dChq164NFxcX1KtXDyNGjEBWVlapz5+amgqtVosuXbqUeH9AQID1/wuvHV69ejXef/991K5dG+7u7nj88cdx+fLlIo/766+/MGzYMNSvXx8qlQpBQUF44403kJeXV2S/wmHb58+fx8CBA+Hp6YnRo0eXu59br+leunQphg0bBgDo2bMnZDKZ9VrncePGwc/PD0ajsViP/fr1wwMPPFDq96gi/vnnHzz66KPw8vKCm5sbunfvjr///tt6f2RkJGQyGXbv3l3ssYsWLYJMJsOJEyes286cOYPQ0FD4+PjAxcUFHTp0wB9//FEptRIRkf1h6CYiIodz8uRJdOvWDceOHcM777yDqVOn4t9//0WPHj3wzz//WPcLCwvD9OnT0bNnT8yfPx8ffPAB6tevjyNHjlj3GTp0KNatW4fx48fju+++w6uvvors7GwkJiaW+vWffvppyGQyrFmzpth9a9asQb9+/aDRaGAwGNC/f3/s378f//vf/7BgwQK8+OKLuHDhAjIzM0t9/oCAALi6umLDhg1IT08v1/dk5syZ2LhxI9599128+uqriIqKQp8+fYoE6oiICOh0OkycOBHffvst+vfvj2+//RZjx44t9nwmkwn9+/dHQEAA5s6di6FDh95VPyEhIXj11VcBAO+//z5WrFiBFStWoHnz5hgzZgzS0tKwdevWIo+5du0adu7ciWeeeaZcvZdl586dCAkJgVarxbRp0/Dpp58iMzMTvXr1woEDBwAAgwYNgoeHR4nHc/Xq1XjwwQfRsmVLAAU/ex07dsTp06fx3nvv4YsvvoC7uzuGDBmCdevW3XO9RERkhwQREZEd+fnnnwUAcfDgwVL3GTJkiHB2dhbnz5+3bktKShKenp4iJCTEuq1NmzZi0KBBpT5PRkaGACA+//zzCtfZqVMn0b59+yLbDhw4IACI5cuXCyGEiI2NFQBEREREhZ//o48+EgCEu7u7GDBggJg5c6Y4fPhwsf127dolAIi6desKrVZr3b5mzRoBQMybN8+6TafTFXv8Z599JmQymbh06ZJ127hx4wQA8d577xXZt7z93HfffWLcuHHW2xEREQKA2LVrV5H9zGazqFevnhg+fHiR7V9++aWQyWTiwoULZX6dcePGCXd391Lvt1gsomnTpqJ///7CYrFYt+t0OtGwYUPRt29f67aRI0eKgIAAYTKZrNuSk5OFXC4XM2bMsG7r3bu3aNWqlcjPzy/ydTp37iyaNm1q3VZ4XG7vmYiIHA/PdBMRkUMxm83Ytm0bhgwZgkaNGlm3BwYGYtSoUdizZw+0Wi0AwNvbGydPnkR8fHyJz+Xq6gpnZ2dER0dbh4OX1/Dhw3H48GGcP3/eum316tVQqVR44oknAABeXl4AgK1bt0Kn01Xo+adPn47w8HAEBwdj69at+OCDD9C+fXu0a9cOp0+fLrb/2LFj4enpab0dGhqKwMBAbNq0qUi/hXJzc5GamorOnTtDCIHY2Nhizzlx4sQit++ln5LI5XKMHj0af/zxB7Kzs63bV65cic6dO6Nhw4b39PxHjx5FfHw8Ro0ahbS0NKSmpiI1NRW5ubno3bs3YmJiYLFYABQczxs3bhRZ4isyMhIWiwXDhw8HAKSnp2Pnzp14+umnkZ2dbX2+tLQ09O/fH/Hx8bh69eo91UxERPaHoZuIiBxKSkoKdDpdidf7Nm/eHBaLxXot84wZM5CZmYn7778frVq1wv/93//h+PHj1v1VKhVmz56NzZs3o1atWggJCcGcOXNw7dq1O9YxbNgwyOVyrF69GgAghEBERIT1OnMAaNiwId58800sWbIEfn5+6N+/PxYsWFDm9dy3GjlyJP766y9kZGRg27ZtGDVqFGJjYzF48GDk5+cX2bdp06ZFbstkMjRp0qTItemJiYl49tln4ePjAw8PD/j7+6N79+4AUKwmJycn1KtXr8i2e+2nJGPHjkVeXp51aPbZs2dx+PBhjBkz5q6fs1Dhhy3jxo2Dv79/kX9LliyBXq+31l54zXfh8QQKPkRp27Yt7r//fgBAQkIChBCYOnVqseebNm0aAGkmuSMiImkxdBMRUY0VEhKC8+fP46effkLLli2xZMkStGvXDkuWLLHu8/rrr+PcuXP47LPP4OLigqlTp6J58+Ylnvm9VZ06ddCtWzfrdcD79+9HYmKi9axooS+++ALHjx/H+++/j7y8PLz66qt48MEHceXKlXL3oVar0bdvX6xcuRLjxo3D+fPni1y7Xh5msxl9+/a1Xve9fv16REVFYenSpQBgPeNbSKVSQS4v/jaiMvq5VYsWLdC+fXv88ssvAIBffvkFzs7OePrpp+/q+W5V2NPnn3+OqKioEv8VrvOtUqms12WbTCZcvXoVf//9d5HjWfh8b7/9dqnP16RJk3uum4iI7AuXDCMiIofi7+8PNzc3nD17tth9Z86cgVwuR1BQkHWbj48Pxo8fj/HjxyMnJwchISEICwvDCy+8YN2ncePGeOutt/DWW28hPj4ebdu2xRdffGENgqUZPnw4Jk2ahLNnz2L16tVwc3PD4MGDi+3XqlUrtGrVCh9++CH27t2LLl264Pvvv8cnn3xS4f47dOiAZcuWITk5ucj224fQCyGQkJCA1q1bAwDi4uJw7tw5LFu2rMjEabfO5F5eFe1HJpOV+Xxjx47Fm2++ieTkZISHh2PQoEHQaDQVrut2jRs3BlDwoUWfPn3uuP/w4cOxbNky7NixA6dPn4YQokjoLrycQalUluv5iIioZuCZbiIicigKhQL9+vXD77//XmTo9PXr1xEeHo6uXbtah3enpaUVeayHhweaNGkCvV4PANDpdMWGaTdu3Bienp7WfcoydOhQKBQK/Prrr4iIiMBjjz1WZC1rrVYLk8lU5DGtWrWCXC4v8/l1Oh327dtX4n2bN28GgGLD65cvX17kuujIyEgkJydjwIABAAq+b0BBGC8khMC8efPu2Oe99lP4PSlthvORI0dCJpPhtddew4ULFypl1nIAaN++PRo3boy5c+ciJyen2P0pKSlFbvfp0wc+Pj5YvXo1Vq9ejYcffrjIdeUBAQHo0aMHFi1aVOxDj5Kej4iIagae6SYiIrv0008/YcuWLcW2v/baa/jkk08QFRWFrl27YtKkSXBycsKiRYug1+sxZ84c674tWrRAjx490L59e/j4+ODQoUOIjIzEK6+8AgA4d+4cevfujaeffhotWrSAk5MT1q1bh+vXr2PEiBF3rDEgIAA9e/bEl19+iezs7GJDy3fu3IlXXnkFw4YNw/333w+TyYQVK1ZAoVBg6NChpT6vTqdD586d0bFjRzz66KMICgpCZmYm1q9fj7/++gtDhgxBcHBwkcf4+Piga9euGD9+PK5fv46vv/4aTZo0wYQJEwAAzZo1Q+PGjfH222/j6tWrUKvVWLt2bYUmkLvbftq2bQuFQoHZs2cjKysLKpUKvXr1sq437u/vj0cffRQRERHw9vbGoEGDyl2T0Wgs8Qy7j48PJk2ahCVLlmDAgAF48MEHMX78eNStWxdXr17Frl27oFarsWHDButjlEolnnrqKaxatQq5ubmYO3duseddsGABunbtilatWmHChAlo1KgRrl+/jn379uHKlSs4duxYuWsnIiIHIeHM6URERBVWuGRYaf8uX74shBDiyJEjon///sLDw0O4ubmJnj17ir179xZ5rk8++UQ8/PDDwtvbW7i6uopmzZqJmTNnCoPBIIQQIjU1VUyePFk0a9ZMuLu7Cy8vL/HII4+INWvWlLvexYsXCwDC09NT5OXlFbnvwoUL4rnnnhONGzcWLi4uwsfHR/Ts2VNs3769zOc0Go1i8eLFYsiQIeK+++4TKpVKuLm5ieDgYPH5558LvV5v3bdwaapff/1VTJkyRQQEBAhXV1cxaNCgIsuACSHEqVOnRJ8+fYSHh4fw8/MTEyZMEMeOHRMAxM8//2zdr7SluMrbz+1LhhV+nxo1aiQUCkWJS2kVLnH24osvlvm9uVXh0mYl/WvcuLF1v9jYWPHUU08JX19foVKpxH333SeefvppsWPHjmLPGRUVJQAImUxm/Vm73fnz58XYsWNF7dq1hVKpFHXr1hWPPfaYiIyMtO7DJcOIiGoOmRC3jCMjIiIihxIdHY2ePXsiIiICoaGhUpdz137//XcMGTIEMTEx6Natm9TlEBERlRuv6SYiIiKbt3jxYjRq1Ahdu3aVuhQiIqIK4TXdREREZLNWrVqF48ePY+PGjZg3b94dZzonIiKyNQzdREREZLNGjhwJDw8PPP/885g0aZLU5RAREVUYr+kmIiIiIiIiqiK8ppuIiIiIiIioijB0ExEREREREVWRGndNt8ViQVJSEjw9PTkZCxEREREREd0VIQSys7NRp04dyOWln8+ucaE7KSkJQUFBUpdBREREREREDuDy5cuoV69eqffXuNDt6ekJoOAbo1arJa6mdEajEdu2bUO/fv2gVCqlLueesR/b52g9sR/bxn5sn6P1xH5sG/uxfY7WE/uxbfbSj1arRVBQkDVjlqbGhe7CIeVqtdrmQ7ebmxvUarVN/6CVF/uxfY7WE/uxbezH9jlaT+zHtrEf2+doPbEf22Zv/dzpsmVOpEZERERERERURRi6iYiIiIiIiKoIQzcRERERERFRFWHoJiIiIiIiIqoiDN1EREREREREVYShm4iIiIiIiKiKMHQTERERERERVRGGbiIiIiIiIqIqwtBNREREREREVEUYuomIiIiIiMgmmC0Cx+L1OJuswbF4PcwWIXVJ98xJ6gKIiIiIiIiIYmJ1WBCRgZRMM4AG2HI8Hf7eWZg8TIOQYDepy7trPNNNREREREREkoqJ1SFscerNwP2flEwzwhanIiZWJ1Fl946hm4iIiIiIiCRjtggsiMgoc58FkRl2O9ScoZuIiIiIiIgkE5egL3aG+3YpGWbEJeirqaLKxdBNREREREREkknLKjtwV3Q/W8PQTURERERERJJIvGbE5n055drX10tRxdVUDc5eTkRERERERNXq3yQDftmsRfQRHUQ5LtX21yjQqomq6gurAgzdREREREREVC3iLxvwy+Ys/HU0z7qtS2tXtGyswqJ1maU+bnKoBgq5rBoqrHwM3URERERERFSlTl/UY8WmLOw/kQ8AkMmAkGA3PPOoGo3rOQMAAv2cblmnu4C/RoHJofa9TjdDNxEREREREVWJE+f1WLE5CwdPFYRtuQzo2cENox/1QoNAZZF9Q4Ld0KWNK2LP5GJnzBH0CmmH4GbudnuGuxBDNxEREREREVWqY+fysXxzFmLPFizzJZcDfR92x6j+agTVUpb6OIVchjZNVbgan4E2TVV2H7gBhm4iIiIiIiKqBEIIHD6TjxWbtdY1tZ0UQL+O7hjV3wt1/Gpm/KyZXRMREREREVGlEELgn5P5WLEpC6cvGgAASidgYGcPjOinRi2fmh07a3b3REREREREdFeEENh7PA8rNmtxLrEgbDsrZXisqweG9/WEvzfjJsDQTURERERERBVgsQjEHM3DL5uzcOGqEQDg4izD4yEeeLq3Gj5eCokrtC0M3URERERERHRHZotA9GEdftmixaXkgrDt5iLDk909EdrbE14eDNslYegmIiIiIiKiUpnMAjsO5mLlFi2u3DABADxcZXiqpyee6ukJtTvDdlkYuomIiIiIiKgYo0lg2z+5CN+SheQ0MwBA7S5HaC9PDOnhCQ9XucQV2geb+S7NmjULMpkMr7/+eqn7LF68GN26dYNGo4FGo0GfPn1w4MCB6iuSiIiIiIjIwRmMAr/vzsaYsCR8sTIdyWlmeHvI8eIQb4R/XAfPDPBi4K4AmzjTffDgQSxatAitW7cuc7/o6GiMHDkSnTt3houLC2bPno1+/frh5MmTqFu3bjVVS0RERERE5HjyDRZs3JODVVHZSMsqOLPt66XA8L6eeKyrB1ycGbTvhuShOycnB6NHj8bixYvxySeflLnvypUri9xesmQJ1q5dix07dmDs2LFVWSYREREREZFDysu34I+/crBmuxYZ2RYAgL+3AiP7qzGwsweclTKJK7RvkofuyZMnY9CgQejTp88dQ/ftdDodjEYjfHx8St1Hr9dDr9dbb2u1WgCA0WiE0Wi8u6KrQWFttlxjRbAf2+doPbEf28Z+bJ+j9cR+bBv7sX2O1hP7KZCbb8GGv3RYuysH2lwBAKjlo8CIvh7o87ArnJ1kAEyo7m+TvRyf8tYnE0KIKq6lVKtWrcLMmTNx8OBBuLi4oEePHmjbti2+/vrrcj1+0qRJ2Lp1K06ePAkXF5cS9wkLC8P06dOLbQ8PD4ebm9u9lE9ERERERGR38o0KHL3kj6OX/KE3FZyH9XLLx0ONrqNZYDoUHEVeLjqdDqNGjUJWVhbUanWp+0l2pvvy5ct47bXXEBUVVWpgLsusWbOwatUqREdHl/n4KVOm4M0337Te1mq1CAoKQr9+/cr8xkjNaDQiKioKffv2hVKplLqce8Z+bJ+j9cR+bBv7sX2O1hP7sW3sx/Y5Wk81tR9trgW/Refi97250OUXnHsNqqXAqH6e6B7sAoWiYXWVXCZ7OT6Fo6jvRLLQffjwYdy4cQPt2rWzbjObzYiJicH8+fOh1+uhUJS83tvcuXMxa9YsbN++/Y6Tr6lUKqhUqmLblUqlTR/AQvZSZ3mxH9vnaD2xH9vGfmyfo/XEfmwb+7F9jtZTTeknI9uMiO1arI/JQb6+IGw3rKPEmAFqdAt2g0Jum9ds2/rxKW9tkoXu3r17Iy4ursi28ePHo1mzZnj33XdLDdxz5szBzJkzsXXrVnTo0KE6SiUiIiIiIrJJZovAsXg9ziZrUDdej+BmTtYQnZZlxuooLTb8lQO9sSBsNwlSYswAL3Rp7Qq5jYZtRyNZ6Pb09ETLli2LbHN3d4evr691+9ixY1G3bl189tlnAIDZs2fjo48+Qnh4OBo0aIBr164BADw8PODh4VG9DRAREREREUkoJlaHBREZSMk0A2iALcfT4e+dhTED1DifZMSmv3NgNBXs26yBM8YM8ELHli6QyRi2q5Pks5eXJTExEXL5f1fxL1y4EAaDAaGhoUX2mzZtGsLCwqq5OiIiIiIiImnExOoQtji12PaUTDO+/DXDertlYxXGDFCjQ3OGbanYVOiOjo4u8/bFixerrRYiIiIiIiJbZLYILIjIKHMfpRPw6UR/tGvGsC01TgZPRERERERkR+IS9DeHlJfOaAIUChkDtw1g6CYiIiIiIrIjaVllB+6K7kdVi6GbiIiIiIjIjvh6lbzS093uR1WLoZuIiIiIiMiOtGqigpuq7GHj/hoFWjVRVVNFVBaGbiIiIiIiIjvyV6wOOr0oc5/JoRrret0kLYZuIiIiIiIiO5Fw2YA5K9IBAJ1bu8Dfu+gQcn+NAmET/BAS7CZFeVQCm1oyjIiIiIiIiEqWmW3G1EUpyDcIdGjugukv+gMAYs/kYmfMEfQKaYfgZu48w21jGLqJiIiIiIhsnMksMH1JKq6nm1HX3wlTn/ezhus2TVW4Gp+BNk1VDNw2iMPLiYiIiIiIbNx3kRk4Fq+Hq0qGj1/2h6cbo5y94JEiIiIiIiKyYRv/zsH63TkAgPfH+6JBoFLiiqgiGLqJiIiIiIhs1InzesxbVTBx2vjHvNClNSdIszcM3URERERERDYoJcOEaYtTYDIDIcGuGP2oWuqS6C4wdBMREREREdkYvcGCjxalIkNrQaM6Srw7xhdyTpJmlxi6iYiIiIiIbIgQAl+Gp+NsogFqdzk+ftkfri6MbvaKR46IiIiIiMiGRO7MRtQBHeRy4KPn/RDox5We7RlDNxERERERkY04dDoPi37LBABMGqpBu2Yu0hZE94yhm4iIiIiIyAZcTTHi4x/TYBFA/47ueLKHh9QlUSVg6CYiIiIiIpKYLt+Cqd+nIltnQfMGznhjpA9kMk6c5ggYuomIiIiIiCRksQh8tiwNF5ON8PVSYPqLfnBWMnA7CoZuIiIiIiIiCa3YrMXfx/KgdAKmv+gHP29OnOZIGLqJiIiIiIgk8tdRHZZtzAIAvD7SBy0aqiSuiCobQzcREREREZEE/k0y4LNlaQCAp3p4YEAnTpzmiBi6iYiIiIiIqpk214wPv09Fvl4g+AEVXh6qkbokqiIM3URERERERNXIbBb4+Mc0JKeaEOirwEfP+8FJwYnTHBVDNxERERERUTVatC4Th8/kw8VZhhkv+cPLQyF1SVSFGLqJiIiIiIiqybb9OYjcmQ0AeG+cLxrXc5a4IqpqDN1ERERERETV4PRFPb4ITwcAjBmgRkiwm8QVUXVg6CYiIiIiIqpiaVlmfLQoFUYT0Lm1K8YN8pK6JKomDN1ERERERERVyGAUmPZDCtKyzLivthOmjPOFXM6J02oKhm4iIiIiIqIqIoTAN6vTcepfAzxcZfj4ZX+4uzKG1SQ82kRERERERFVk/e4cbNqbC7kMmPq8H+oFKKUuiaoZQzcREREREVEVOHouHwsiMwAAE4Z446EWrhJXRFJg6CYiIiIiIqpk19JMCFucCosF6POQG57u4yl1SSQRhm4iIiIiIqJKlKe34MPvU6DNteD++s54a7QPZDJOnFZTMXQTERERERFVEiEE5qxIx4WrRmg85Zjxoh9UzoxdNRmPPhERERERUSUJ36rF7iM6OCmAsAl+CPBxkrokkhhDNxERERERUSXYF5eHnzZkAQBeHe6DVk1cJK6IbAFDNxERERER0T26lGzEzJ9TIQTweDcPPNbVQ+qSyEYwdBMREREREd2DHJ0FUxelQJcv0KqJCpOHaaQuiWwIQzcREREREdFdMlsEPvkpFVdumBCgUSBsgh+UTpypnP7D0E1ERERERHSXfvwjCwdO5UOllGHGS/7QeCqkLolsDEM3ERERERHRXdhxMBertmkBAG8/44P76ztLXBHZIoZuIiIiIiKiCjqXaMDnv6QDAEb0U6P3Q+4SV0S2iqGbiIiIiIioAjKyzfhoUQoMRoGHH3TB8497SV0S2TCGbiIiIiIionIymgTCFqfiRoYZ9QKc8OF4PyjknDiNSsfQTUREREREVE7zIzIQl6CHu4sMn7zsDw83RioqG39CiIiIiIiIymHDX9nY8FcOZDLg/fF+qF9bKXVJZAcYuomIiIiIiO7geEI+vlmdAQB4brAXOrVylbgishcM3URERERERGW4kW7C9MWpMFuA7u3cMKq/WuqSyI4wdBMREREREZUi32DB1EUpyMi2oHE9Jd4Z4wOZjBOnUfnZTOieNWsWZDIZXn/99TL3i4iIQLNmzeDi4oJWrVph06ZN1VMgERERERHVKEIIfLEyHfGXjfDykOPjl/zhqrKZCEV2wiZ+Yg4ePIhFixahdevWZe63d+9ejBw5Es8//zxiY2MxZMgQDBkyBCdOnKimSomIiIiIqKZYvT0bOw7qIJcD017wQ21fJ6lLIjskeejOycnB6NGjsXjxYmg0mjL3nTdvHh599FH83//9H5o3b46PP/4Y7dq1w/z586upWiIiIiIiqgkOnMzDkvWZAIDJoRq0vd9F2oLIbkn+Uc3kyZMxaNAg9OnTB5988kmZ++7btw9vvvlmkW39+/fH+vXrS32MXq+HXq+33tZqtQAAo9EIo9F494VXscLabLnGimA/ts/RemI/to392D5H64n92Db2Y/scrac79XPlhgkf/5QKiwAe7eiKQZ1VNt17TTs+tqK89cmEEKKKaynVqlWrMHPmTBw8eBAuLi7o0aMH2rZti6+//rrE/Z2dnbFs2TKMHDnSuu27777D9OnTcf369RIfExYWhunTpxfbHh4eDjc3t0rpg4iIiIiIHIPeJMea/fcjPdcVgd45eOqhBDjJJYtMZMN0Oh1GjRqFrKwsqNWlz2gv2Znuy5cv47XXXkNUVBRcXKpuqMaUKVOKnB3XarUICgpCv379yvzGSM1oNCIqKgp9+/aFUqmUupx7xn5sn6P1xH5sG/uxfY7WE/uxbezH9jlaT6X1Y7EITP8xA+m5evh6yfHFm43g69VUwkrLp6YcH1tTOIr6TiQL3YcPH8aNGzfQrl076zaz2YyYmBjMnz8fer0eCoWiyGNq165d7Iz29evXUbt27VK/jkqlgkqlKrZdqVTa9AEsZC91lhf7sX2O1hP7sW3sx/Y5Wk/sx7axH9vnaD3d3s9PGzKx/4QeSidgxkv+qO1XPEfYMkc/PramvLVJNpFa7969ERcXh6NHj1r/dejQAaNHj8bRo0eLBW4A6NSpE3bs2FFkW1RUFDp16lRdZRMRERERkQPafUSHXzYXnLl8a5QPmjewr8BNtkuyM92enp5o2bJlkW3u7u7w9fW1bh87dizq1q2Lzz77DADw2muvoXv37vjiiy8waNAgrFq1CocOHcIPP/xQ7fUTEREREZFjOH/FgNnL0wAAob080a+jh8QVkSORfMmwsiQmJiI5Odl6u3PnzggPD8cPP/yANm3aIDIyEuvXry8W3omIiIiIiMojK8eMqYtSkG8QaN/MBS896S11SeRgJF8y7FbR0dFl3gaAYcOGYdiwYdVTEBERERERORSzReBYvB5nkzWofTYfq6JycS3NjEA/J0x93hcKhUzqEsnB2FToJiIiIiIiqioxsTosiMhASqYZQANsOZ4BAFA6AZ+87Ae1e/F5pYjulU0PLyciIiIiIqoMMbE6hC1OvRm4izKagMvXTRJURTUBQzcRERERETk0s0VgQURGmfssiMyA2SKqqSKqSRi6iYiIiIjIocUl6Es8w32rlAwz4hL01VQR1SQM3URERERE5NCSUso3dDwtq+xgTnQ3OJEaERERERE5pHStGb/tysZvu7Tl2t/XixOpUeVj6CYiIiIiIoeSlGrCmigtNu/LgfHmSW6FHDBbSn+Mv0aBVk1U1VMg1SgM3URERERE5BDOXzFgVZQWuw7pUDgnWvMGzhjVXw2TWWD6krRSHzs5VAOFnGt0U+Vj6CYiIiIiIrt2PCEfv27V4p+T+dZtD7Vwwch+arRpqoJMVhCmwybIblmnu4C/RoHJoRqEBLtVe91UMzB0ExERERGR3bFYBP45mY9ft2lx4nzBrONyGdC9nRtG9FOjaZBzsceEBLuhSxtXxJ7Jxc6YI+gV0g7Bzdx5hpuqFEM3ERERERHZDZNZYNchHVZFafFvkhEAoHQC+nf0wPA+nqgboCzz8Qq5DG2aqnA1PgNtmqoYuKnKMXQTEREREZHNyzdYsGVfLtZs1+JaWsHwcDcXGR7v5oGhvdSceZxsFkM3ERERERHZrGydBb/vzsZvu7KRmVMw/bjGU46hPT3xeIgnPNzkEldIVDaGbiIiIiIisjmpmSZE7szGhr9ykKcvmIq8tq8Cw/uo8Wgnd6icGbbJPjB0ExERERGRzbhyw4jVUVps+yfXusZ2ozpKjOyvRo92blAoeA022ReGbiIiIiIikty5RAN+3aZFTKwO4uYa260aqzCynxqPtHSxLvtFZG8YuomIiIiISBJCCBw9p8ev27Q4dPq/NbY7tixYY7tVExcJqyOqHAzdRERERERUrSwWgb+P5+HXbVqcuWgAAMjlQK/2BWtsN6pbfI1tInvF0E1ERERERNXCaBLYcTAXq7ZpkXi94IJtZ6UMAzq74+neagT6MZ6Q4+FPNRERERERVam8fAs27s1BxPZspGQWrLHt7irDkBBPPNXLExpPrrFNjouhm4iIiIiIqkRWjhnrd+dgXXQ2tLkFa2z7qOUI7a3G4K4ecHflsl/k+Bi6iYiIiIioUt1INyFiZzY27slBvqFgKvK6/k4Y3leNfo+4w1nJmcip5mDoJiIiIiKiSpF4zYhVUVpsP5ALU8EocjQJUmJUPzW6BbtBIWfYppqHoZuIiIiIiO7J6Yt6/LpVi7+P51nX2G57f8Ea2x2ac41tqtkYuomIiIiIqFRmi8CxeD3OJmtQN16P4GZOUMhlEELg8Jl8/LpNi9izeuv+Xdq4YmQ/NVo0VElYNZHtYOgmIiIiIqISxcTqsCAi4+aM4w2w5Xg6/Lyz0LO9G46ey0f8ZSMAQCEH+jzsjhF91bgvUClt0UQ2hqGbiIiIiIiKiYnVIWxxarHtqZlmROzIBgC4OMswqKsHQnt5opYPowVRSfjKICIiIiKiIswWgQURGWXu4+Yiw/KwQPioGSmIysKF8YiIiIiIqIi4BP3NIeWl0+ULJF4zVVNFRPaLoZuIiIiIiIo4cT6/XPulZZUdzImIw8uJiIiIiOim9CwzlvyRiS37csu1v6+XooorIrJ/DN1ERERERDWcwSgQuVOLlVu0yNMXLLStcpZBbxClPsZfo0CrJlwWjOhOGLqJiIiIiGooIQT+OpqHRb9lIDmtYKh48wbOmDxMg9RMc4mzlxeaHKqBQi6rrlKJ7BZDNxERERFRDXT+igELIjJwNF4PoGCo+ItDvNH7ITfIb4bpsAl+t6zTXcBfo8DkUA1Cgt0kqZvI3jB0ExERERHVIBnZZvy8IQub/s6BRQDOShmG9/HEiH5quKqKzrMcEuyGLm1cEXsmFztjjqBXSDsEN3PnGW6iCmDoJiIiIiKqAYwmgXXR2VixKQu5+QXXavds74YJQ7xR27f0WKCQy9CmqQpX4zPQpqmKgZuoghi6iYiIiIgcmBAC++Ly8P1vmbhyo2Bd7aZBSkwepkHrJi4SV0fk+Bi6iYiIiIgc1L9JBnwXmYnDZwrW3dao5XjhcW/07+huvW6biKoWQzcRERERkYPJyjFj6cYsbPgrBxYLoHQChvZSY3R/Ndxd5Xd+AiKqNAzdREREREQOwmQW+CMmB8s2ZiFbZwEAdG3jipee8kZdf6XE1RHVTAzdREREREQO4MDJPCxcm4FL1wqu225UV4nJoRoEP8DrtomkxNBNRERERGTHEq8bsTAyA/+cLLhu28tDjucGe2FgFw/ONE5kAxi6iYiIiIjsUI7OguWbsrAuOhtmC6CQA0/19MSYAV7wcON120S2gqGbiIiIiMiOmM0CG//Owc9/ZiErp+C67Y4tXTBxqAZBtXjdNpGtYegmIiIiIrITR87m47uIDFxIMgIA7qvthEmhGjzUwlXiyoioNAzdREREREQ27mqKEd//lom/j+UBADzd5Hj2MS8M7uYBJwWv2yayZQzdREREREQ2KjfPgpVbsrB2VzaMJkAuBx7v5oFxg7zg5aGQujwiKgeGbiIiIiIiG2O2CGzdl4sfN2QiQ1tw3XaH5i6YONQbDes4S1wdEVUEQzcRERERkQ05npCP+REZSLhccN12vQAnTByqQceWLpDJOJScyN4wdBMRERER2YBraSYsWpeJ3Ud0AAB3VxnGDvTCkO6eUDoxbBPZK0kX8Fu4cCFat24NtVoNtVqNTp06YfPmzWU+5uuvv8YDDzwAV1dXBAUF4Y033kB+fn41VUxEREREVLny8i348Y9MjJuehN1HdJDLgMFdPbAirA6G9VYzcBPZOUnPdNerVw+zZs1C06ZNIYTAsmXL8MQTTyA2NhYPPvhgsf3Dw8Px3nvv4aeffkLnzp1x7tw5PPvss5DJZPjyyy8l6ICIiIiI6O5YLALbD+Ri8e9ZSMsyAwDa3q/C5FANGtfjddtEjkLS0D148OAit2fOnImFCxdi//79JYbuvXv3okuXLhg1ahQAoEGDBhg5ciT++eefaqmXiIiIiKgynLygx4LIDJy5aAAABPo54eWnvNG1jSuv2yZyMDZzTbfZbEZERARyc3PRqVOnEvfp3LkzfvnlFxw4cAAPP/wwLly4gE2bNmHMmDHVXC0RERERUcWlZJjww/pM7DhYcN22q0qGZwZ4YWhPTzgrGbaJHFGFQ/eWLVvg4eGBrl27AgAWLFiAxYsXo0WLFliwYAE0Gk2Fni8uLg6dOnVCfn4+PDw8sG7dOrRo0aLEfUeNGoXU1FR07doVQgiYTCa8/PLLeP/990t9fr1eD71eb72t1WoBAEajEUajsUK1VqfC2my5xopgP7bP0XpiP7aN/dg+R+uJ/dg2R+rHbBE4elaHs8ka1Dqdi7YPuEEhlyHfIBC5IwdrduRAbwRkMqDfw6549jFP+KgVAEyw5fYd6RgB7MfW2Us/5a1PJoQQFXniVq1aYfbs2Rg4cCDi4uLw0EMP4c0338SuXbvQrFkz/PzzzxUq1GAwIDExEVlZWYiMjMSSJUuwe/fuEoN3dHQ0RowYgU8++QSPPPIIEhIS8Nprr2HChAmYOnVqic8fFhaG6dOnF9seHh4ONze3CtVKRERERFSahOte2H26HnL0/12P7aEyoEntDCRc1yAnv2B7He8chDS7glpeeVKVSkSVQKfTYdSoUcjKyoJarS51vwqHbg8PD5w4cQINGjRAWFgYTpw4gcjISBw5cgQDBw7EtWvX7qnwPn36oHHjxli0aFGx+7p164aOHTvi888/t2775Zdf8OKLLyInJwdyefHJ2Es60x0UFITU1NQyvzFSMxqNiIqKQt++faFUKqUu556xH9vnaD2xH9vGfmyfo/XEfmybI/Sz51gePv4ps8x9AjQKPP+4J7oH2996245wjG7FfmybvfSj1Wrh5+d3x9Bd4eHlzs7O0OkKrkHZvn07xo4dCwDw8fGxDt2+FxaLpUhIvpVOpysWrBUKBQCgtM8OVCoVVCpVse1KpdKmD2Ahe6mzvNiP7XO0ntiPbWM/ts/RemI/ts1e+zFbBL7/LbvMfdxdZPhpam24uSiqqaqqYa/HqDTsx7bZej/lra3Cobtr165488030aVLFxw4cACrV68GAJw7dw716tWr0HNNmTIFAwYMQP369ZGdnY3w8HBER0dj69atAICxY8eibt26+OyzzwAUzHb+5ZdfIjg42Dq8fOrUqRg8eLA1fBMRERERVae4BD1SMs1l7pObL3Au0Yi29/M9K1FNU+HQPX/+fEyaNAmRkZFYuHAh6tatCwDYvHkzHn300Qo9140bNzB27FgkJyfDy8sLrVu3xtatW9G3b18AQGJiYpEz2x9++CFkMhk+/PBDXL16Ff7+/hg8eDBmzpxZ0TaIiIiIiCpF4RrblbUfETmWCofu+vXr488//yy2/auvvqrwF//xxx/LvD86OrrIbScnJ0ybNg3Tpk2r8NciIiIiIqoKvl7lO3td3v2IyLEUn3nsDo4cOYK4uDjr7d9//x1DhgzB+++/D4PBUKnFERERERHZuuzcO5/B9tco0KpJ8XmGiMjxVTh0v/TSSzh37hwA4MKFCxgxYgTc3NwQERGBd955p9ILJCIiIiKyVZv+zsH0JWl33G9yqAYKuX3NWE5ElaPCofvcuXNo27YtACAiIgIhISEIDw/H0qVLsXbt2squj4iIiIjI5gghEL4lC3NXpsMigAGd3PHR877w9y46hNxfo0DYBD+EBLtJVCkRSa3C13QLIWCxWAAULBn22GOPAYB17WsiIiIiIkdmsQh8tzYTv+0qWCZsZD81XnjCCzKZDN2C3RB7Jhc7Y46gV0g7BDdz5xluohquwqG7Q4cO+OSTT9CnTx/s3r0bCxcuBAD8+++/qFWrVqUXSERERERkK4wmgdnL07DzkA4AMCnUG6G91Nb7FXIZ2jRV4Wp8Bto0VTFwE1HFQ/fXX3+N0aNHY/369fjggw/QpEkTAEBkZCQ6d+5c6QUSEREREdmCvHwLpi1OxaHT+VDIgXfH+qLPw+5Sl0VENq7Cobt169ZFZi8v9Pnnn0Oh4DIIREREROR4snLMmPJdCs5cNMDFWYawCX54+EFXqcsiIjtQ4dBd6PDhwzh9+jQAoEWLFmjXrl2lFUVEREREZCuupZnw7vwbuHzdBLW7HJ9N8kfzhlz+i4jKp8Kh+8aNGxg+fDh2794Nb29vAEBmZiZ69uyJVatWwd/fv7JrJCIiIiKSxL9JBrw7PwWpmWYEaBSY/UoA7gtUSl0WEdmRCi8Z9r///Q85OTk4efIk0tPTkZ6ejhMnTkCr1eLVV1+tihqJiIiIiKrdifN6vP7lDaRmmnFfbSd881YtBm4iqrAKn+nesmULtm/fjubNm1u3tWjRAgsWLEC/fv0qtTgiIiIiIinsj8vD9CWp0BsFWjR0xsyJ/vDy4PxFRFRxFQ7dFosFSmXxT/iUSqV1/W4iIiIiInu1dX8OPv8lHRYL8PCDLpj2gh9cVRUeIEpEBOAuhpf36tULr732GpKSkqzbrl69ijfeeAO9e/eu1OKIiIiIiKrTqigtZi8vCNx9H3bDJy/7M3AT0T2p8G+Q+fPnQ6vVokGDBmjcuDEaN26Mhg0bQqvV4ptvvqmKGomIiIiIqpTFIvD9bxn4YV0mAGBYb0+8O9YXTgqZtIURkd2r8PDyoKAgHDlyBNu3b8eZM2cAAM2bN0efPn0qvTgiIiIioqpmMgvM/SUd2/7JBQC8+KQ3RvRVS1wVETmKu1qnWyaToW/fvujbt69125kzZ/D444/j3LlzlVYcEREREVFVyjdYMGNJKvafyIdcDrw92gePdvKQuiwiciB3FbpLotfrcf78+cp6OiIiIiKiKqXNNeP971Jw6l8DnJUyfPS8Lzq3dpO6LCJyMJUWuomIiIiI7EVKhgnvzE/BpWQjPFxl+HRSAFo2VkldFhE5IIZuIiIiIqpREq8Z8c63N3AjwwxfLwXm/M8fDes4S10WETkohm4iIiIiqjFO/6vHlO9SoM21oF6AE+b8LwC1ffmWmIiqTrl/w2g0GshkpS+ZYDKZKqUgIiIiIqKqcPBUHqb9kIp8g8AD9znjs0n+8PZUSF0WETm4cofur7/+ugrLICIiIiKqOjsO5mLWsjSYLUD7Zi6Y8aIfXF3kUpdFRDVAuUP3uHHjqrIOIiIiIqIqsXanFgsiMwEAPTu44b2xvlA6lT6Ck4ioMvECFiIiIiJySEII/PhHFsK3agEAT/bwwORQDeRyBm4iqj4M3URERETkcMxmga9+TcemvbkAgOcGe2H0o+oy5ygiIqoKDN1ERERE5FD0Bgs++SkNfx/Pg1wGvD7SB4919ZC6LCKqoRi6iYiIiMhh5Ogs+OD7FMQl6KF0AqY+54eubd2kLouIarBKm7LxwoUL6NevX2U9HRERERFRhaRlmfH6V9cRl6CHu4sMc14JYOAmIslV2pnu7Oxs7Nixo7KejoiIiIio3K7cMOKdb2/gWpoZPmo5Zr8SgMb1nKUui4iIw8uJiIiIyL6dSzTgvfk3kJljQV1/J8z+XwDq+PFtLhHZBv42IiIiIiK7deRMPqYuSkGeXqBJkBKzJgfAR62QuiwiIiuGbiIiIiKyS9FHdPhsaSqMJiD4ARVmvOgPd9dKm7KIiKhSlDt0BwcHl7muoU6nq5SCiIiIiIju5Pfd2fhmTQaEAEKCXfH+s35wVnINbiKyPeUO3UOGDKnCMoiIiOyT2SJwLF6Ps8ka1I3XI7iZExRyvvEnqipCCCzbmIXlm7QAgMe7eeB/wzV83RGRzSp36J42bVpV1kFERGR3YmJ1WBCRgZRMM4AG2HI8Hf7eWZg8TIOQYC5TRFTZzBaBb1ZnYMNfOQCAcYO8MHaguszRmEREUqu0i16OHz8OZ2cuy0BERDVDTKwOYYtTbwbu/6RkmhG2OBUxsbzsiqgyGYwCH/+Yig1/5UAmA14bocG4QV4M3ERk8yotdAshYDKZKuvpiIiIbJbZIrAgIqPMfRZEZsBsEdVUEZFjy82z4L0FNxATmwelEzD1eT88EeIpdVlEROVSqdM78pNGIiKqCeIS9MXOcN8uJcOMuAR9NVVE5LjStWa88fV1HD2nh6tKhs8mB6BHO16+QUT2g0uGERERVVBaVtmBu6L7EVHJklJNeOfbG0hKMcHbQ45ZrwTg/vq8nJGI7Eu5Q7dWqy3z/uzs7HsuhoiIyB74eikqdT8iKi7hsgHvLbiBdK0FtX0VmPO/ANQLUEpdFhFRhZU7dHt7e5c5fFwIweHlRERUI7RqooK/t6LMIeb+GgVaNVFVY1VE9qmkZffiEvSY+n0KcvMFGtVVYvYrAfwQi4jsVrlD965du6qyDiIiIruhkMswMdQbM5aklbrPS0O8uW4w0R2UtOye2j0TuXkWmC1A6yYqfPKyPzzcKnUaIiKialXu0N29e/eqrIOIiMiu5OsLZiaXAbh1jnK5DLAI4NI1oyR1EdmLwmX3bqfNtQAAmt3njNmv+EPlzMBNRPaNE6kRERFVUJ7egh//yAIAvDDEC03rKbAz5gh6hbRDTp4MM35Mw6ooLfo87I6gWrwGleh25Vl2Ly3LDCcnjhYhIvtX7o8O5XI5FApFmf+cnJjhiYjI8a3Zno20LDMCfRUY2lONNk1VeCAwA22aqtC9nRsebuECown4ZnUGhOBa3VK79ZrhY/F6rp9uA/45kXfnZfcyueweETmGcqfkdevWlXrfvn378M0338BisVRKUURERLYqJdOE1VEFK3pMeFIDZ6UMxltGkstkMvxvuAbPfZyMw2fyEX1Yh54d3CWqlkq6ZtjfOwuTh2kQEsy1nqua3mDBpWsm/JtkwIWrRlxMNuLCVSOX3SOiGqXcofuJJ54otu3s2bN47733sGHDBowePRozZsyo1OKIiIhszU9/ZCHfIPBgI2d0D3YtcZ+6/kqMftQLS//MwndrM/Hwg65wd+V1qdWttGuGUzLNCFucirAJfgzelcRsFriaYsK/SUb8m2S4+V8jklJMuJeBBZyxnIgcwV2NB09KSsK0adOwbNky9O/fH0ePHkXLli0ruzYiIiKbEn/ZgG3/5AIAJg3VlLlU5oi+amw/kIsrN0z4+c8svDJMU11lEsp3zfCCyAx0aePKWeYrQAiBlAwz/k024t+r/wXsS9eMMJpKfozaXY5GdZRoUEeJRnWd0SBQiaDaTnhx5jUuu0dENUKFQndWVhY+/fRTfPvtt2jbti127NiBbt26VVVtRERENkMIgYVrMyAE0KuDG5o3LDsMOCtleHW4Bu98m4L10dno39EdTYOcq6laikvQ3/ma4Qwzxs9IRv3aSvh7K+B385+/t9PN/yrg6lJzRyhoc83WM9b//TMgN6/kU9cuzjLcF6hEwzr//WtUxxkatbzED6gmD9OUOBLBen+ohh+IEJFDKHfonjNnDmbPno3atWvj119/LXG4ORERkaPaG5eHo+f0UDoBLzzhXa7HdGjuip7t3bDrsA5f/5qOb9+uBTlDRLUo77XAV26YcOVGKadoAbi7yOCncYKflwL+GoU1jPvdEsy9PEoOlVXl1onh6sbrEdzM6Z7Cab7BgsRrJly4aigSsEv7HsrlQFAtJRoGKtGw7n//DfR1qtDPd0iwG8Im+N1yzX0Bf40Ck0N5zT0ROY5yh+733nsPrq6uaNKkCZYtW4Zly5aVuN9vv/1W7i++cOFCLFy4EBcvXgQAPPjgg/joo48wYMCAUh+TmZmJDz74AL/99hvS09Nx33334euvv8bAgQPL/XWJiIgqwmQWWPRbJgBgWG81avuWf6DYxKHe+OdkHk5fNGDT3lw81tWjiqqkW5X3WuDnBquhdlcgJdOMVOs/E1IzzcjNFwX/ko24lFz6uutKJ8DP62YQ1yhuCej/hXVfLwWcFPcezO9lYrjC664vJBlx8ZaJza6mmFDaJPu1fBQ3z1o7W89eB9VSwllZOR8yhAS7oUsbV8SeybUuuxfczJ1nuInIoZT7XcPYsWMr/VPcevXqYdasWWjatCmEEFi2bBmeeOIJxMbG4sEHHyy2v8FgQN++fREQEIDIyEjUrVsXly5dgre3d6XWRUREdKs/YnJw5YYJGk85RvZTV+ixft5OGP+YFxZEZmLx+kx0beMKb09ODlXVWjVRwd1VVupQaKDgjOrI/l6lBjxdvgWpmeabgdyElAxzkdupmWZkZFtgNAHJaWYkp5V+dl0mAzSecvh5O1mHsluHtGtunjX3Kns4e3knhiu87rogXBtxIcmAi3e47trLQ37LsPCCgN0gUFktEwAq5DK0aarC1fiCZfcYuInI0ZQ7dC9durTSv/jgwYOL3J45cyYWLlyI/fv3lxi6f/rpJ6Snp2Pv3r1QKpUAgAYNGlR6XURERIWydRYs35QFAHj2Ma+7CiFDunti6/5cJFwxYtG6TLw71reyy6TbJF4zIl9f9rTZd7pm2M1Fjvq15ahfW1nqPkaTQFrWf2fIC8+Yp2SYkZplRkqGCWlZZpjMQLrWgnStAecSS6/J3VVWYjD3VSswb1V6mf3MWpaGiB1aXEw2lnnddYM6twwNvxmwNZ7VO0SeiKgmuavZy6uC2WxGREQEcnNz0alTpxL3+eOPP9CpUydMnjwZv//+O/z9/TFq1Ci8++67UCh41oCIiCrfL5uzoM21oEGgEgM7393QcIVChtdH+uCVz69j6/5cDOjsjtZNXCq5UipkNAl8tjQNZgvQJEiJrGxLlV0zrHSSobav081LDkqeXM9iEcjMKTxrXnCGPPWWUF549jxPL5CbJ5CbV/Zw9tLkGwROXjAAABSF113fMqlZw7rOqO2j4LwCRETVTPLQHRcXh06dOiE/Px8eHh5Yt24dWrRoUeK+Fy5cwM6dOzF69Ghs2rQJCQkJmDRpEoxGI6ZNm1biY/R6PfR6vfW2VqsFABiNRhiNFf+DVl0Ka7PlGiuC/dg+R+uJ/dg2e+knKdWEddHZAIAXnvCAxWKCxVJ8v/L007SeHAM6uWLzvjx8/Ws6FvyfX6Vc41tV7OUYleTHP7RIuGKEl7scH7+ogZeHHEfP6hCz9zhCOrdG2wfcoJDLqrU3T1fA01WGhoFKACWfOc/NtyAt04yUTMt/Z8+zCsL6xWQjrqeX8MN3m8Fd3TCoixvqBjjB2en2ny8Bs9kEc/nmmKtW9vzzVhJH6wdwvJ7Yj22zl37KW59MiNKmzqgeBoMBiYmJyMrKQmRkJJYsWYLdu3eXGLzvv/9+5Ofn499//7We2f7yyy/x+eefIzk5ucTnDwsLw/Tp04ttDw8Ph5sbZ8UkIqLSbTzaAAnXNbjPV4shHc7f8/PlGRRYsac58oxKdL3/Kto3vFEJVdKtrqa7I/JgUwAyPNb2AhrXypK6pEpxJd0Daw82veN+Qx+KRz2fnGqoiIiIdDodRo0ahaysLKjVpc/5Innovl2fPn3QuHFjLFq0qNh93bt3h1KpxPbt263bNm/ejIEDB0Kv18PZufj6pyWd6Q4KCkJqamqZ3xipGY1GREVFoW/fvtbr1+0Z+7F9jtYT+7Ft9tBP3HkD3v4mDXIZsPAdPzSoU8Z1vRXoZ+t+Hb78NQsuzjIsft8fARrbvDzKHo7R7XLzLHh5dipuZJjR/xFXvDnK23qfPfZzK7NFYOz0G0jNLP1st7+3HMumBdjlRGT2fnxu52j9AI7XE/uxbfbSj1arhZ+f3x1Dt+TDy29nsViKhORbdenSBeHh4bBYLJDLCyayOXfuHAIDA0sM3ACgUqmgUhW/xkqpVNr0ASxkL3WWF/uxfY7WE/uxbbbaj8UisPj3NADAoC4eaHpf+UZGlaefgV3U2HYgHyfO6/HD+mxMf9H/nuutSrZ6jEqycGVB4A70c8L/hvtCqSw+6Z099XMrJYBXhvmUOHt5ocnDfOCiKvn9kL2w1+NTGkfrB3C8ntiPbbP1fspbW9WvA1GGKVOmICYmBhcvXkRcXBymTJmC6OhojB49GkDBMmVTpkyx7j9x4kSkp6fjtddew7lz57Bx40Z8+umnmDx5slQtEBGRA9p5SIezlwxwVckw7jGvSn1uuVyG10doIJcDfx3Nw/4TeZX6/DVV9BEdog7oIJcBU8b5wq2MpbfsVUiwG8Im+MHfu+joCH+NwrpcGBER2R5Jz3TfuHEDY8eORXJyMry8vNC6dWts3boVffv2BQAkJiZaz2gDQFBQELZu3Yo33ngDrVu3Rt26dfHaa6/h3XfflaoFIiJyMHqDBUt+zwQAjO6vho+68od/N6rrjKE9PRGxIxvfrk5H8P2BUDk7XkisLimZJnwVXrCc1sj+arRsXPIs4o4gJNgNXdq4IvZMLnbGHEGvkHYIbuZul0PKiYhqCklD948//ljm/dHR0cW2derUCfv376+iioiIqKaL3JmNGxlmBPgoMLSXZ5V9nWcHeWHXYR2S08xYuVWL5wZ7V9nXcmQWi8Cc5enI1lnwQH1njBtUuSMTbJFCLkObpipcjc9Am6YqBm4iIhvHj9WJiIhuSs8yI3xrwdKSE57wrtKzz64ucrwyTAMAWB2lReJ1214WxVat352Dw2fyoVLKMGW8r00vw0ZERDUTQzcREdFNP/+ZiTy9QLMGzujZvuqvj+3W1hUPP+gCown4ZlU6bGxBEZv3b5IBP6zPBAC8/JQ36tey3cl2iIio5mLoJiIiAnDhqgGb9+YCACYN1UBeDUN2ZTIZ/ve0Bs5KGY6c1WPXYV2Vf01HYTQJfLY0DQajwMMPuuDxEA+pSyIiIioRQzcREdV4QggsXJsJiwBCgl2rdSKuuv5KjO5fsLbnd5EZyMkrfR1m+s/Pf2Yh4YoRanc5/u8ZX8hkHFZORES2iaGbiIhqvAOn8nH4TD6UTsCEId7V/vWH91WjXoAT0rUWLN2QWe1f394ci8/H6qiCa+/fGu0DX6/Kn2GeiIiosjB0ExFRjWY2C3y/NgMA8GQPT9T1r/7rgp2VMrw6vGBStfW7cxB/2VDtNdiLnDwLZi1LgxDAo53c0a0t16YmIiLbxtBNREQ12sa/c3DpmglqdzmeeVS65aY6NHdFz/ZusAjg61/TYbFwUrWSzF+TgevpZgT6KqyzvxMREdkyhm4iIqqxcvIsWPpnFgBg3CAveLhJ+2dx4lBvuLnIcPqiAZtuTupG/9l9RIdt/+RCLgOmPOsHNxe+jSEiItvHv1ZERFRjhW/VIjPHgqBaThjcTfrZr/28nTD+sYKz7YvXZyIj2yxxRbYjNdOEr35NBwCM7K+u1snuiIiI7gVDNxER1UjX0kxYu7NgMq6Xn9LASWEbs18P6e6JJvWUyNZZ8MO6TKnLsQkWi8Ds5enQ5lpwf31njBsk3WUAREREFcXQTURENdLi3zNhNAHBD6jQsaWL1OVYKRQyvD7SBzIZsHV/Lo4n5EtdkuTW787B4TP5UClleP9ZX5v5gISIiKg8GLqJiKjGOfWvHrsO6SCTAROf0tjcGs8tGqowqEvBcPevf82AyVxzJ1W7mGzED+szAQAvPeWN+rWrf3Z5IiKie8HQTURENYoQAt9FFiwR9mhHdzQJcpa4opI9/7gXvDzkuJhsROTObKnLkYTRJPDpz6kwGAUebuGCJ0Kkv+6eiIioohi6iYioRtl9RIdT/xrg4izD+MG2e22wl4cCLz7pDQBYvjELN9JN0hYkgaV/ZiHhihFqdzn+b4yvzY1IICIiKg+GbiIiqjEMRmEdqjyinxp+3k7SFnQH/R9xR6vGKuQbBOZHZEhdTrU6npCPVVEFE929OcoHvl4KiSsiIiK6OwzdRERUY/y2KxvX0szw81ZgWG9Pqcu5I7lchtdGaCCXA3uO5WF/XJ7UJVWL3DwLPluaBiGARzu5IyTYTeqSiIiI7hpDNxER1QiZ2Was3JIFoOB6aVeVffwJbFTXGaG9Cj4g+HZNOvINFokrqnrzIzJwPd2MQF8FXhmmkbocIiKie2If7ziIiIju0bKNWcjNF2gapETfh92lLqdCxg30gr+3AslpZoRv1UpdTpXafUSHrftzIZcBU571g5sL36oQEZF9418yIiJyeJeSjdiwJwcAMHGoBnK5fU3I5eoix+SbZ3xXbdMi8bpR4oqqRmqmCV/9mg4AGNlPjZaNVRJXREREdO8YuomIyOEtWpcBiwXo0toVbe93kbqcu9KtrSseftAFJjPwzap0COFYa3dbLAJzVqRDm2tB0yAlxg6y3ZnliYiIKoKhm4iIHNqh03nYfyIfCjmsS3DZI5lMhleH+8BZKcORs3rsPKSTuqRKtX53Dg6dzoezUob3n/WD0sm+RiMQERGVhqGbiIgcltki8P3aTADAE909EVRLKW1B96iOnxNG91cDABauzUBOnmNMqnYx2Whdyu3lp7xxX6B9HyciIqJbMXQTEZHD2rovFxeSjPB0k2PsQLXU5VSK4X3VqBfghHStBT9vyJS6nHtmNAl8ujQVBqPAwy1c8ESIh9QlERERVSqGbiIicki6fAt+uhlKnxmghtpdIW1BlcRZKcNrI3wAAL/vzsG5RIPEFd2bZRuzkHDZCLW7HP83xhcyGYeVExGRY2HoJiIih7QqSot0rQV1/Z0wpLun1OVUqvbNXNCzgxssAvh6VTrMFvucVO14Qj5+3VawBNqbo3zg6+UYH4wQERHdiqGbiIgczo10E9ZszwZQMHmaI07KNfEpb7i5yHDmogGb/s6RupwKy82z4LOlaRAC6N/RHSHBblKXREREVCUYuomIyOH8+EcmDEaBVk1U6NrGVepyqoSftxOeG+wNAFjyexYyss3SFlRB8yMycD3djNq+Crxycw1yIiIiR8TQTUREDuXsJT2iDhQspzVpqLdDXyP8RIgHmtRTIltnwQ/rMqUup9xiYnXYuj8XchkwZZwv3F35doSIiBwX/8oREZHDEELgu5tLhPV92A0P3KeStqAqplDI8MZIH8hkwNb9uTgWny91SXeUmmnCl+HpAICR/dRo1cRF4oqIiIiqFkM3ERE5jD3H8hCXoIezUobnH/eWupxq0byhCoO6FCyzNW9VBkxm251UTQiBz39JhzbXgqZBSowd5CV1SURERFWOoZuIiByC0SSw6OYQ66f7eCLAx0nagqrRC094wctDjovJRkTuzJa6nFKt352Dg6fy4ayU4f1n/RxygjsiIqLbMXQTEZFDWL87G0kpJvio5RjZVy11OdVK7a7AS096AwCWb8zC9XSTtAWV4GKy0fqhyEtPeuO+QKW0BREREVUThm4iIrJ72lwzftlcsN7z+MHecHWpeX/e+j3ijlaNVcg3CCyIyJC6nCKMJoFPl6bCYBR4qIULhnT3kLokIiKialPz3pUQEdkZs0XgWLweZ5M1OBavh9liu9fsSmX5Ji2ydRY0qqPEo53cpS5HEnK5DK+P1EAuL7i2fX9cntQlWS3bmIWEy0ao3eX4v2d8HHpGeSIiotvVnAveiIjsUEysDgsiMpCSaQbQAFuOp8PfOwuTh2kQEuwmdXk24fJ1I37fXXAd88RQDRTymhvoGtZxRmgvT6zZno1v1qSj7QOBcHGW9vP1uIR8rNpWMArhzVE+8PPmWw8iIqpZeKabiMhGxcTqELY49Wbg/k9Kphlhi1MRE6uTqDLb8sP6TJgtQMeWLmjfjMtPjRvoBX9vBa6lmbFyi1bSWnLzLPhsWRosAujf0Z0fFBERUY3E0E1EZIPMljtfl7sgMqPGDzU/ei4ffx/Lg1wOvPSkRupybIKrixyThxV8L1ZHaZF43ShZLfMjMnAtzYzavgq8MozHh4iIaiaGbiIiGxSXoC92hvt2KRlmxCXoq6ki22OxCHy3tuCDicFdPTgb9i26tXXFIw+6wGQG5q1KhxDV/+FMTKwOW/fnQi4Dpozzhbsr33IQEVHNxL+AREQ2KC2r7MBd0f0cUdSBXCRcNsLdRYZxg7ykLsemyGQy/G+4D5yVMsSe1WPnoeq9FCE104Qvw9MBACP6qdGqCYf9ExFRzcXQTURkgzSe5fv1rHavmb/G8/QW/PhHFgBg9KNe8PZUSFyR7anj54TRjxasV75wbQZy8izV8nWFEPj8l3Rocy1oGqTkByJERFTj1cx3a0RENiw3z4K1u7LLte/Xq9Jx4KTtLA1VXSJ2ZCM1s+Ba4ad6ekpdjs0a3keNegFOSNda8POGzGr5mut35+DgqXw4K2V4/1k/KJ1q7mzyREREAEM3EZFNSbxuxOQ517AvLh+KO/yG9nCTIznVjPcWpCBscQpSMkzVU6TEUjNN1iWoXhziDWclQ11pnJUyvDbCBwDw++4cnEs0VOnXu5RsxKJ1mQAKjg2vsyciImLoJiKyGXuP6zB59jUkXjfBz1uBb96uhbAJfvD3Ljp02l+jQNgEP/z6cR2E9vKEXA7ExOZh3IxkrNmuhcns2DOa/7whC/kGgRYNndG9HZegupP2zVzQs4MbLAL46tf0Kpvx3mgS+HRpKgxGgQ7NXTCku0eVfB0iIiJ74yR1AURENZ3FIrByixY//1lwjXKrxipMe8EPPl4KNG8AdGnjitgzudgZcwS9QtohuJk7FPKCs7uTQjXo39EdX69Kx8kLBnz/Wya27s/F6yM0Djl5VcJlA7bszwVQ0LtMxrPc5TFpqAb/nMjD2UsGbNyTg8dDKn9I/vKNWYi/bITaXY53xvhALuexISIiAnimm4hIUrp8C6YtTrUG7idCPDD3tQD4eP13dlshl6FNUxUeCMxAm6Yqa+Au1LieM+a9WQv/94wP1O5y/JtkxGtf3sDs5WnIzHac2c2FEFj4WwaEAHp2cEOLhiqpS7Ibvl4KPDfYGwCw5PdMZFTyz0VcQj5+vTnk/81RPvDz5mf6REREhRi6iYgkcuWGEZM/v46/j+VB6QS8PdoHr43wuauJp+RyGQZ09sCyaYEY1MUdALB1fy7GTU/Gn3tyYKmiIcXVaf+JfMSe1UPpBEx4wlvqcuzOEyEeaBKkRE6ewKLfMivteXPzLPhsWRosAujf0R0hwRzyT0REdCuGbiIiCew/kYeJs6/hUrIRvl4KfPVGLQzscu/XwHp5KPDWaF98+3YtNK6nRLbOgi/D0/HK3OtVPolWVTKZBb7/LQMAMLSXGrV9eSa1ohQKGd4Y4QOZDNj2Ty6OncuvlOedH5GBa2kFM8m/MkxTKc9JRETkSBi6iYiqkRACK7dk4YOFKcjNE3iwkTO+f692pQ+VfrCRCt+/WxuvDNPAzUWGMxcNmDT7Gr5dk15t6zVXpg1/5eDydRO8PeQY1V8tdTl2q3lDFR67+eHO16szYDTd2wiImFgdtu7PhUwGvDfOF+6ufFtBRER0O/51JCKqJnn5Fkxfkoof/8iCEMDgrh748vVa8PVS3PnBd0GhkOGpnp5Y+lGgdfbqddE5GDc9CTsO5kII+xhynqOzYNnGgmven33MCx4Mdvfk+Se84OUhx6VkI9buLN968CVJyzLjy/B0AMCIvmq0dsCJ+4iIiCoD37kQEVWDqylGvDL3OmJi8+CkKJhs6o1Rd3f9dkX5eTth6nN++PzVANQLcEKG1oKZP6fhrXk3kHjNWOVf/179siUL2lwL7gtUYlAlDMGv6dTuCrz0pDcAYPmmLFxLq/j67kIIfL4iDdpcC5oEKfHsY16VXCUREZHjYOgmIqpiB0/lYeKsa/g3yQgftRxfvl4Lj3Wt/vDYvpkLlnwQiOcGe8FZKcPRc3q8MDMZP/6eiXyDbQ45v5pixLrogrOxLz/lDYWCy1BVhv4d3dGqiQr5BoEFkRkVfvzvMTk4cCofzkoZ3n/Wr1o+PCIiIrJXkobuhQsXonXr1lCr1VCr1ejUqRM2b95crseuWrUKMpkMQ4YMqdoiiYjukhACv27TYsqCFOTkCTRvUHD9dsvG0i115ayU4ZkBXvh5aiA6tnSByQys3KrF+BnJ2HtcJ1ldpVm8PhNGE9ChuQsebsHhy5VFJpPh9REaKOTA38fysC8ur9yPvZRsxPc3Zz9/cYg3GgQqq6hKIiIixyBp6K5Xrx5mzZqFw4cP49ChQ+jVqxeeeOIJnDx5sszHXbx4EW+//Ta6detWTZUSEVVMnt6Cj39Mw+L1mbAIYGBnd3z1Ri2bWb840M8JMyf64+OX/BDgo8D1dDM+/D4VH36fclfDjavCifN6xMTmQS4rOMstk/FsamVqWMcZob08AQDfrkkv12gHo0ng06WpMBgFOjR3wZDuHO5PRER0J5KG7sGDB2PgwIFo2rQp7r//fsycORMeHh7Yv39/qY8xm80YPXo0pk+fjkaNGlVjtURE5ZOUasL/5l5H9BEdFHLgtREavDXaB85K2wqNMpkMXdq44eepgRjZTw2FHNh7PA/jZyQjfEvWPc9sfS8sFoHv1hYMex7Q2R2N6jpLVosjGzvQCwEaBa6lmbFyi/aO+y/fmIX4y0ao3eV4Z4wP5HLb+pkmIiKyRbZxygUFYToiIgK5ubno1KlTqfvNmDEDAQEBeP755/HXX3/d8Xn1ej30er31tlZb8KbCaDTCaLTdCYQKa7PlGiuC/dg+R+tJqn4On9Hjs2UZyNYJeHvI8eFzGrRq7AyT6d7OHldlP05y4NlB7ujZ3hnzI7Q4nmDAkj+ysPWfHLwS6oW291f+cPg79bPzUB7OXDTAVSXDM4+62/zPpb2+fpwUwMtPqjHjpwysjtKiRzsV6tcqeGtwe08nLxjw67aCv6GvPq2Gl7uwq37t9RiVhv3YNkfrB3C8ntiPbbOXfspbn0xIvGZMXFwcOnXqhPz8fHh4eCA8PBwDBw4scd89e/ZgxIgROHr0KPz8/PDss88iMzMT69evL/X5w8LCMH369GLbw8PD4ebmVlltEFENJwRw5GIA/j5XBwIy1FLnYlDwv/B0se0/FrcTAjibrEHM2brIMxRcq/tAYDq6PXAV7qrqGXZuMsuwfE8LZOc7o1OTJDzc+Hq1fN2aSgjgjyONcDHVC/V8svFUhwTcPpJfb5IjfG8zaPNUaF4nDf1aJUpTLBERkQ3R6XQYNWoUsrKyoFarS91P8tBtMBiQmJiIrKwsREZGYsmSJdi9ezdatGhRZL/s7Gy0bt0a3333HQYMGAAA5QrdJZ3pDgoKQmpqapnfGKkZjUZERUWhb9++UCrtf5Ia9mP7HK2n6uwn3yDw1a+ZiD6SDwDo+7ArXn3aq1KHk1f38cnRWbB0Uzb+3KODEICbiwzPDvLEY13doKiEIcVl9bMqKgc//5kNf285fvwgACpn2x/CbO+vn+RUE16clQKDEXh3jDd6dXAt0tM3EbnY9k8eavkosPAdP7jb4Vrp9n6Mbsd+bJuj9QM4Xk/sx7bZSz9arRZ+fn53DN2SDy93dnZGkyZNAADt27fHwYMHMW/ePCxatKjIfufPn8fFixcxePBg6zaLpWDSFycnJ5w9exaNGzcu9vwqlQoqVfGhkUql0qYPYCF7qbO82I/tc7Seqrqfa2kmfLQoFQlXjJDLgcmhGgzp7lFlk35V1/HReAFvjFRhYGc9vv41A2cTDfhurRbbDuThjRE+aN6wcoac395PutaMVVE5AIAJQzTwcLeva7nt9fVTP1CJ0Y964ecNWfjhdy06tnZH/CULziZrkL4jD9v+yYNMBkx51hfeaulm368M9nqMSsN+bJuj9QM4Xk/sx7bZej/lrU3y0H07i8VS5Mx0oWbNmiEuLq7Itg8//BDZ2dmYN28egoKCqqtEIiIAwJEz+ZjxYyq0uRZ4e8gx7QU/tLnfsZa1euA+Fea/Uwsb9+Rgye+ZSLhsxCtzr+OxLh54/gkvqN0Vlfr1lv6ZhTy9wAP1ndGrAy8Bqk7D+6gR9U8urtwwYeQHScg3CAANgOO5AIAurV3Ruolj/XwTERFVB0lD95QpUzBgwADUr18f2dnZCA8PR3R0NLZu3QoAGDt2LOrWrYvPPvsMLi4uaNmyZZHHe3t7A0Cx7UREVUkIgcid2Vj0W8FyYPfXd8b0F/1Qy8fmPsesFAq5DI+HeKJbsBt+WJeJrftzsWFPDmKO6vDSk97o39G9Us7s/5tkwKa/C85yTwz15szY1cxZKUPvDm5Ytkl7M3AXtedYHmJidQgJ5ochREREFSHpO8QbN25g7NixSE5OhpeXF1q3bo2tW7eib9++AIDExETI5fZ33RgROS69wYIvVqZj+0EdAKDvw254c5QPVM6O/7tK46nAu2N98Wgnd8xblYGLyUbMWZGOzXtz8doIzT0v6/X9zQ8xQoJ5RlUKZovApr25Ze6zIDIDXdq4Vsp1/URERDWFpKH7xx9/LPP+6OjoMu9funRp5RVDRHQH19NN+GhRCuIvF1y/PfEpbzzV07PKrt+2VW2auuCH92sjcmc2lm/MQtx5PV787BpCe3li3EAvuLpU/AOIAyfzcPBUPpwUwIQh3pVfNN1RXIIeKZnmMvdJyTAjLkGPtg52GQUREVFVcvxTM0REleDouXy8POsa4i8b4eUhx+f/C8DQXuoaF7gLOSlkGNFXjaUfBaJbW1dYLMCa7dkYNyMZMbE6VGRhDLNZ4PvfMgEAT/bwRF1/250wxZGlZZUduCu6HxERERVwzAsQiYgqiRAC66Jz8N3aDFgsQJMgJWa86I/avvz1CQABPk6Y/qI/9p/Iw7drMpCcakLY4lQ83MIF/3tag7oBdw7Qm/bm4GKyEWp3OZ4Z4FUNVVNJfL3KNyleefcjIiKiAjzTTURUCoNRYM6KdMyPKAjcfR5ywzdv1WLgLkHHlq746cPaGDtQDaUTcOBUPp77JBnLNmbBYCz9rHduvgU/b8gCAIwd6AVPN/5ZkkqrJir4e5cdqP01CrRqYt9LhhEREVU3vrshIirBjXQTXvviOrbuz4VcBkwc6o0pz/rCpQZMmHa3VM5yPPuYN378MBDtm7nAaAKWbczCc58k4+CpPOt+ZovAsXg9ziZr8O3qLGTmWFAvwAmPh3hIWD0p5DJMHqYpc5/JoRpOokZERFRBPF1DRHSbY/H5mLEkFRnZFqjd5Zj6vB/aN+PEUeVVL0CJOf/zx+7YPCyIyEBSignvzk9BSLArOjR3wYpN2psTdjUAkA8A6NrWFU4KhjmphQS7IWyCHxZEZBSZVM1fo8DkUA2XCyMiIroLDN1ERDcJIfB7TA4WRGTAbAEa1yu4fjvQj78qK0omk6FHOzc81NwFyzZm4bfobMTE5iEmNq/E/Vdty0az+1QMdTYgJNgNXdq4IvZMLnbGHEGvkHYIbubOM9xERER3ieMkiYhQcP323F/S8c3qgsDds33B9dsM3PfG3VWOSaEaLHynFpzuMP/WgsgMmC3ln/Wcqo5CLkObpio8EJiBNk1VDNxERET3gO8miajGS8k0YdoPqThz0QC5DHhhiDeG96l5629Xpdx8AdMdVpriGtBERETkiBi6iahGO3Fej2mLU5ChtcDTTY4Pn/PFQy1cpS7L4XANaCIiIqqpGLqJqMba8Fc2vl2TAZMZaFRHiekv+aGu/53XlaaK4xrQREREVFMxdBNRjWMwCny7Jh0b/84FAIQEu+LdMb5wdeE0F1WlcA3oW2fEvh3XgCYiIiJHxHeYRFSjpGaa8ObX17Hx71zIZMALT3hh2gt+DNxVjGtAExERUU3Fd5lEBLNF4Fi8HmeTNTgWr3fYGaRPXtBj4uzrOPWvAR6uMnw6yR+j+ntxwrRqUrgGtL930SHk/hoFwib4cbkwIiIickgcXk5Uw8XE6rAgIuPmsN8G2HI8Hf7eWZg8TGO3IejWDxHqxusR3MwJW/blYt6qdJjMQINAJT5+yQ91A3j9dnXjGtBERERU0zB0E9VgMbE6hC1OLbY9JdOMsMWpdnn2saQPEVycM5BvKDh7362tK94d6ws3DieXTOEa0FfjuQY0EREROT6+6ySqocwWgQURGWXusyAyw66Gmhd+iHD7ZF2FgbtXBzdMe8GPgZuIiIiIqg3PdBPVUHEJ+jJnkgaAlAwzXp17Dd6eTpDLAYUckMtkkMtx87YMchlu3i7YrpD99/9yuezm7dLuL3g+xa3PV/g4OW4+9y1f75avfftzAwJfr0ovu+fzetjPRwhERERE5AgYuolqqLSssgN3odMXjQCMVVtMNUnJMCMuQY+297tIXQoRERER1RAM3UQ1lMazfEOsh/fxRN0AJSwWAYsALJaCoekWC27eFv9tu3m/xSJgvu1+iwUwi//+32IRMN+yf0nPUfCYsp/DIgCzBcjNM0Obe+fz2OX9sIGIiIiIqDIwdBPVQLp8C9buyr7jfv4aBV4Y4m0XE10dPZePN7++ccf9fL0Ud9yHiIiIiKiyMHQT1TBXU4yY+n0qLiYboZAXnCUuzeRQjV0EbgBo1UQFf29Fmdep+2sUaNVEVY1VEREREVFNxyl8iWqQI2fyMXnOdVxMNsLXS4Fv3qqFsAl+8PcuevbXX6Owu+XCFHIZJg/TlLmPPX2IQERERESOgWe6iWoAIQTWRefgu7UZsFiAB+5zxoyX/ODv7YTmALq0cUXsmVzsjDmCXiHtENzM3S7DaUiwG8Im+N2yTncBf40Ck0M1dvUhAhERERE5BoZuIgdnNAnMW52OTX/nAgD6POyGt0b5QOX830AXhVyGNk1VuBqfgTZNVXYZuAuFBLs5zIcIRERERGT/GLqJHFhGthlhP6Qi7rweMhkwYYg3hvfxhEzm2AHUkT5EICIiIiL7xtBN5KDiLxsw9fsU3Mgww91Fhg+e80PHlq5Sl0VEREREVKMwdBM5oOjDuZi9PB16o0C9ACd88rI/6tdWSl0WEREREVGNw9BN5EAsFoGf/8zCyi1aAMBDLVzw4XN+8HTjQgVERERERFJg6CZyELp8Cz5bmoa/j+cBAIb19sSLQ7yhUPB6ZiIiIiIiqTB0EzmApFQTPlyYgovJRiidgLdG+aBfRw+pyyIiIiIiqvEYuons3JGz+ZixJBXaXAt81HLMeMkfLRqqpC6LiIiIiIjA0E1kt4QQWL87BwsiM2CxAA/Ud8aMl/3g782XNRERERGRreC7cyI7ZDQJfLM6HRv/zgUA9HnIDW+N9oHKmROmERERERHZEoZuIjuTkW1G2A+piDuvh0wGTHjCG8P7ekIm44RpRERERES2hqGbyI4kXDbgw0UpuJFuhruLDB8854eOLV2lLouIiIiIiErB0E1kJ6KP6DB7WRr0RoF6AU745GV/1K+tlLosIiIiIiIqA0M3kY2zWASWbczCis1aAECH5i6Y+rwfPN14/TYRERERka1j6CayYbp8Cz5bloa/j+UBAIb19sSLQ7yhUPD6bSIiIiIie8DQTWSjklJNmPp9Cv5NMkLpBLw5ygf9O3pIXRYREREREVUAQzeRDYo9m4/pS1KhzbXARy3HjJf80aKhSuqyiIiIiIioghi6iWyIEAJ/xOTg24gMWCzAA/WdMeNlP/h786VKRERERGSP+E6eyEYYTQLfrsnAn3tyAAC9H3LD26N9oHLmhGlERERERPaKoZvIBmRkmxG2OBVxCXrIZMCEJ7wxvK8nZDJOmEZEREREZM8YuokklnDZgA8XpeBGuhnuLjJ8MN4PHVu5Sl0WERERERFVAoZuIglFH9FhzvI05BsE6vo74ZOX/XFfoFLqsoiIiIiIqJIwdBNJwGIRWL4pC8s3aQEA7Zu54KMX/ODpxuu3iYiIiIgcCUM3UTXLy7fgs2Vp2HMsDwAQ2ssTLz3pDYWC128TERERETkahm6iapScasLU71NwIckIpRPwxkgfPNrJQ+qyiIiIiIioijB0E1WT2LP5mL4kFdpcC3zUckx/0R8PNlJJXRYREREREVUhhm6iavD77mx8G5EBiwW4v74zPn7JD/4avvyIiIiIiBydpLM2LVy4EK1bt4ZarYZarUanTp2wefPmUvdfvHgxunXrBo1GA41Ggz59+uDAgQPVWDFRxRhNAl+Fp2Pe6oLA3auDG+a9GcDATURERERUQ0gauuvVq4dZs2bh8OHDOHToEHr16oUnnngCJ0+eLHH/6OhojBw5Ert27cK+ffsQFBSEfv364erVq9VcOdGdZWab8fY3N7BhTw5kMmDCEG98MN4XKmfOUE5EREREVFNIerpt8ODBRW7PnDkTCxcuxP79+/Hggw8W23/lypVFbi9ZsgRr167Fjh07MHbs2Cqtlagizl8x4MPvU3A93Qw3Fxk+GO+HTq1cpS6LiIiIiIiqmc2McTWbzYiIiEBubi46depUrsfodDoYjUb4+PiUuo9er4der7fe1moL1kU2Go0wGo33VnQVKqzNlmusiJrUz19H8/D5yizoDQJ1/BSYPkGD+rWdbL73mnSM7BH7sW2O1g/geD2xH9vGfmyfo/XEfmybvfRT3vpkQghRxbWUKS4uDp06dUJ+fj48PDwQHh6OgQMHluuxkyZNwtatW3Hy5Em4uLiUuE9YWBimT59ebHt4eDjc3NzuqXaiWwkB/HO+Nv45HwgACPLVYmDri3BxNktcGRERERERVTadTodRo0YhKysLarW61P0kD90GgwGJiYnIyspCZGQklixZgt27d6NFixZlPm7WrFmYM2cOoqOj0bp161L3K+lMd1BQEFJTU8v8xkjNaDQiKioKffv2hVKplLqce+ZI/ZgtAkfP6hCz9zhCOrdG2wfcYDAKfP5LJv4+XvCz9mR3d0x4whMKhUziasvPkY4RwH5sHfuxfY7WE/uxbezH9jlaT+zHttlLP1qtFn5+fncM3ZIPL3d2dkaTJk0AAO3bt8fBgwcxb948LFq0qNTHzJ07F7NmzcL27dvLDNwAoFKpoFIVXwtZqVTa7AE0WwROxetxNlmDuhctCG7mBIXcfsJbWWz5+14eMbE6LIjIQEqmGUADbDmuhY86B0qFDNczzFA6Aa+P9MGATh5Sl3rX7P0Y3Y792Db2Y/scrSf2Y9vYj+1ztJ7Yj22z9X7KW5vkoft2FoulyJnp282ZMwczZ87E1q1b0aFDh2qsrHoUD3Xp8PfOwuRhGoQEczi8lGJidQhbnFpse7rWAgBwd5Xhs0kBaNm4+Ic8RERERERUM0m6dtGUKVMQExODixcvIi4uDlOmTEF0dDRGjx4NABg7diymTJli3X/27NmYOnUqfvrpJzRo0ADXrl3DtWvXkJOTI1ULlaow1BUE7v+kZJoRtjgVMbE6iSojs0VgQURGmfu4OMvQvKFzNVVERERERET2QNLQfePGDYwdOxYPPPAAevfujYMHD2Lr1q3o27cvACAxMRHJycnW/RcuXAiDwYDQ0FAEBgZa/82dO1eqFipNeULdgsgMmC2SXoJf4+TlWxB/2YDlG7OKfRhyu7QsC+ISSh+lQURERERENY+kw8t//PHHMu+Pjo4ucvvixYtVV4zE4hL0dwx1KRlmvPBJMprWd0YdP6eCf/5K1PFzgkYth0zmGNd9VzeDUSAp1YQr1424kmLClRtGXLluwtUUE9KyKjbzeEX3JyIiIiIix2Zz13TXVOUNa5eumXDpmqnYdheVDHV8nRDoXxjGnazBvJavE5zsaBbtqmA2C1xLN+HKjVvC9XUTrqYYcT3djLLm8Pf2kMPbU46LycW/77fz9VJUYtVERERERGTvGLptRHnD2tiBajgrZUhONSEp1YSkFBNSMszI1wtcSDLiQlLxBdrlcqCWRmE9K14YzOv6OyHQzwluLpJeZVBpLBaB1EzzzUBtxNUUEy7f/G9yqgmmMj7XcHeRoW6AEvUCnG7+U1r/6+Emh9kiMOrDpDJHI/hrFGjVhJOoERERERHRfxi6bUSrJir4eyvuGOrGDPQqtnyY0SRwLe2/EH7rf5NTTTAYBZLTzEhOM+NwCc/r7SH/78y4f+UPWzdbBI4VLoEWr7+nJdCEEMjMseDqDRMu3zDi6o2bw8FvmHD1hgl6Y+mnrJ2VMtTzd0LdIqHaCXUDlNB4lt2nQi7D5GGaEmcvLzQ5VOMwS7sREREREVHlYOi2EfcS6pROMgTVUiKoVvF14iwWgXStucRAnpRigjbXgswcCzJzDDj1r6HY463D1osE8vIPW7/bJdBy8iy4ejNMX7nx33XWV1KMyM0rPVgr5ECg339nq+sGOCHo5n/9vRWQ30MoDgl2Q9gEv1v6KeCvUWByKJd0IyIiIiKi4hi6bUhVhDq5XAY/byf4eTuhdZPi9+fkWZB0c/h1QRA33tWw9UC/28+UO+HQ6fwSP0QoXALtg/G+aBCoxNWUWycxM+HqDSMysi2l9iSTAQEahfVsdeGZ66CAqr9+PSTYDV3auCL2TC52xhxBr5B2CG7mzjPcRERERERUIoZuG1Pdoc7DVY776zvj/vrF15cubdh68s1h6/pbhq2X5E6j0mf+nFbm/T5qeZFrq+sWDgf3V8JZKV3IVchlaNNUhavxGWjTVMXATUREREREpWLotkG2Euruddh6WTOCF3JVydAgsOgw8HoBStT1d4K7q2NM8EZERERERDUXQzfdlTsNW9/4dw6+WJl+x+d5c5QPej/kXgUVEhERERERSY+nEqlK1PUv3+c5XNeaiIiIiIgcGUM3VYnCJdDKwnWtiYiIiIjI0TF0U5UoXAKtLFzXmoiIiIiIHB1DN1WZwiXQbj/j7a9RIGyCH9e1JiIiIiIih8eJ1KhKcV1rIiIiIiKqyXimm6pc4RJoDwRyXWsiIiIiIqpZGLqJiIiIiIiIqghDNxEREREREVEVYegmIiIiIiIiqiIM3URERERERERVhKGbiIiIiIiIqIowdBMRERERERFVEYZuIiIiIiIioirC0E1ERERERERURRi6iYiIiIiIiKqIk9QFVDchBABAq9VKXEnZjEYjdDodtFotlEql1OXcM/Zj+xytJ/Zj29iP7XO0ntiPbWM/ts/RemI/ts1e+inMlIUZszQ1LnRnZ2cDAIKCgiSuhIiIiIiIiOxddnY2vLy8Sr1fJu4Uyx2MxWJBUlISPD09IZPJpC6nVFqtFkFBQbh8+TLUarXU5dwz9mP7HK0n9mPb2I/tc7Se2I9tYz+2z9F6Yj+2zV76EUIgOzsbderUgVxe+pXbNe5Mt1wuR7169aQuo9zUarVN/6BVFPuxfY7WE/uxbezH9jlaT+zHtrEf2+doPbEf22YP/ZR1hrsQJ1IjIiIiIiIiqiIM3URERERERERVhKHbRqlUKkybNg0qlUrqUioF+7F9jtYT+7Ft7Mf2OVpP7Me2sR/b52g9sR/b5mj91LiJ1IiIiIiIiIiqC890ExEREREREVURhm4iIiIiIiKiKsLQTURERERERFRFGLqJiIiIiIiIqghDNxERERHZJYvFInUJdAc8RraNc2pXD4ZusilGo1HqEiod/9jYLrPZLHUJVA6O+IYgLy9P6hIqjSMeH0fhiH9TC6WkpAAA5HL7fyvrqK8hRzhGjvwaysrKAgDIZDKJK6kZ7PdVUAMlJCQgIiJC6jKqTFpaGp577jnodDqYTCapy7lnN27cAGDff2xKc/XqVezbt0/qMu5JSkoKpk2bBpPJ5BA/b7e6fv06zp49K3UZ9ywjIwOA470huH79OqZPnw6DwWB9s22Pb7od9fgAjvEacrS/qbfKy8tDWFgYzp49y9eQjXKEY+TIr6HMzEyEhYUhLS3Nbo/P7Wz997bjpQEHdurUKWvQuf2FYe8vFKDgF0B6ejrc3Nzg5ORU7H57OmNsNBrxxhtv4PDhww53NtVsNuOXX37BmTNnpC7lnpw8eRKxsbFwcnKy+5+3WxkMBsyaNQvx8fFSl3JPcnNz8f777yMxMdHhXkNyuRw7d+7E2bNnrW+2bz+bYuu/0x35+DjKa8iR/qbeTgiBM2fO4OTJk9bXUGZmZpERJHwNScsRjpEjv4YAYN++fThx4oT1+Oh0uiL32/rxuZU9/N5m6LYjzs7OOHDgAPR6PWQyGbZt24bff/8d+/fvd4hPSRs3bgyDwYC9e/cCABYtWoQPP/wQL7/8MlJTUyGXy+3mF4DZbEZmZiZSUlKgUCgAFAzjKRzKY88UCgW8vLzw559/Wv/gHDhwAKdPn7b+srOH49SjRw8YjUZcunQJALBs2TJ88cUXmDVrFnJzcyGXy+3yjZCzszNq1aqFbdu2WbedOHECSUlJSEpKAmAfxyc/Px+xsbG4evWq9TWUl5dXJJzaQx+3M5vN8Pf3R9u2ba1nThYuXIhx48bh//7v/7Bq1SoAtn/my1GPD+A4ryFH+pt6Ozc3NwwdOhTZ2dkAgLlz5+LJJ5/EM888g9deew1AwWvIlvtz5NcQ4BjHyFFfQ0IIeHt7o1+/ftDr9QCAefPm4emnn8YLL7yAuXPnArD943Mre/i9zdBtR/r164cmTZpALpdj2bJlmDBhArZv347evXvj22+/lfyH6W6YzeYinxQ2btwYV69excmTJzFv3jw0a9YMycnJaNGiBRITE+3mF4CLiwsee+wx6zCX2bNn48knn8SQIUPw1ltvWfezh15K0rlzZ7i7u0Mul2PRokV45pln8NFHH2HgwIHYuHGjXRyn/Px8KBQKJCQkICYmBrNnz4bJZMLu3bvx0EMPITs7GwqFwub7KEmrVq2g1WoBAPPnz8eoUaPwwgsvYPz48YiJibGL4+Pr64vHHnvMepnGl19+iWHDhmHMmDH4/PPPAdjXG4LCNzaFb679/f2xdu1a7NmzB1999RWee+45mEwm/PLLL/jmm2+kLLVcHO343M5eX0OO+jcVALKzs5Gammq97ebmhl9//RVnz55FZGQk5s2bhwkTJiAqKgpjxowBYNsfXjnia8gRjpEjv4Z0Oh3y8vKs33MPDw+sXLkSR44cwY8//oipU6eibdu2WLduHd577z0Atnd8ymLzv7cF2ayEhATx/fffizVr1oj09HRhNptFjx49xMqVK8VHH30k1q1bJ4QQ4o8//hA+Pj5izZo10hZcQXFxceK5554TQ4YMETt37hRCCLFq1Srx+uuvi99++008/vjj1n1Hjhwp2rRpI4xGo1Tl3lFWVpa4cuWK9fbKlStFv379RFJSkujWrZs4duyYiImJEb6+vuKFF16QsNKKS0xMFLt37xb79+8XQgiRn58vQkJCxJ49e8TQoUOt2xcsWCC8vb3Fnj17pCy3RCdPnhRvv/22mDhxojh16pQQQohvvvlGfPvtt2LFihXi+eeft+77+OOPi8cff9ymf95ulZSUJI4fPy5OnDghhBAiMzNT9O/fX8TFxYnevXuL48ePi0uXLolPP/1UtG3bVhw/flziikuWm5srsrKyrLdnz54tRowYIU6fPi3at28v9u7dK5YtWyYCAwPFBx98IGGlFRMXFyf69OkjRo8eLd58800hhBBbtmwREydOFImJiWLIkCHCYrGIzMxM8e2334pBgwaJpKQkiasuzlGPjxCO8RpytL+ptzp+/Lh48MEHxSOPPCK6dOkidDqdyMnJEaNHjxapqamibdu21r9DcXFx4oEHHhC7du2StugSOPJryBGOkaO/hh5++GExcOBAMWzYMCGEEGfPnhXjx48XKSkpIiQkxPoedsOGDaJjx44iLi5OypLvyN5+b/NMt42Kj49HmzZt8Oeff+Kll15CWFgY5HI5Hn/8cQgh4ObmhmnTpiE7OxuDBw/Giy++iBUrVtjNRA9JSUno168fnJ2doVar8cwzzyAmJgYNGjRAXFwcWrdujdTUVCxatAgAMGXKFGg0GmRmZkpbeCmOHz+Ozp07Y8CAARg4cCCuXbuGYcOGwcfHB/n5+bh8+TJ0Oh26deuGvXv3Yu3atdi+fbvUZZfLqVOn0LZtW3zwwQfo1KkTPvnkE6hUKtStWxcWiwX33Xcfli9fjpycHEyaNAn9+/fHrl27pC67iEuXLqFHjx64evUqEhMT0aFDB8THxyMoKAibNm1CgwYNcPHiRWzevBkA8Pzzz8NsNtvF8PKTJ08iODgYL730Enr27InFixfDy8sLZrMZ+fn5aNy4MXbu3InatWvjxRdfREBAAA4dOiR12cUcP34cPXv2xMCBAzFy5EgAwOjRo+Hu7g61Wg29Xo/AwECMHTsWK1aswOrVq3HkyBGJq76z69evY9CgQWjevDlat26NQ4cOYcaMGXjkkUdw9uxZnDlzBteuXcPatWvh7u6OgQMH4vz589YhmbbCUY8P4BivIUf7m3qrrKwsjB8/HgMHDsSCBQsghMCAAQPg7u6O+Ph4/PXXX+jZsyfWrl2Lw4cPo1mzZtBoNDb3+9uRX0OOcIwc+TWUlpaGYcOGoWvXrhg/fjzOnz+PcePGoVGjRjh16hQOHjwIf39/REZG4urVq+jSpQt0Oh3y8/OlLr1U9vh7m6HbRq1fvx6DBg3Chg0bEBUVhVWrVmH37t1o3LgxwsPDMXLkSDRr1gwjR47EsWPHkJKSAldXV6nLLrczZ87Ax8cHX331FZYtW4Zhw4Zh6dKleOSRR1C7dm1YLBY89dRT+P333zFu3Dj89NNPuHjxonVopi3RarV46aWXMGjQICxbtgxZWVl4/vnnoVQqcfbsWcTHx2PMmDH44YcfcPDgQdx///1o06aNXQxHMpvNmDNnDp566in89ddfiIqKwowZM5CYmIhOnTph+/bt6NKlC1JTU7Fw4UKkpqbCbDZbZ2S1FYcOHUJQUBB++eUX/PnnnxgyZAh+/vln9O7dG7Vr10b9+vXRokULLF++HB999BH++usvXLp0yTok2FYZDAZ88MEHCA0NxYYNGzB37lxMnz4daWlp6NatG+Li4tC8eXPs2bMHf/75J9zc3KBUKnHlyhWpSy8iLS0No0aNQteuXfHBBx/gn3/+wZgxYxAYGGid6OWRRx7B8uXLcf78eTzyyCPw9fW1i9eQTqdD7dq1MWnSJLzzzjsYO3YskpOToVKpoNPp0KRJE0yYMAEzZszA+PHjMX36dOh0Onh6ekpdupUjHx9HeQ050t/U2zk7O0Mul6Nly5Zo3749IiMjoVKpYDAY0LVrVyiVSowfPx7Xr1/HuHHjEBoairNnz6JRo0ZSl27lyK8hwDGOkSO/hoQQ8PT0xMCBAxEaGoqvvvrK+v6mTp068PPzw4QJE7Bp0yaMHTsWEydOxLVr1xAQECBx5SWz19/bDN02ytXVFX///TcuX76M9u3bY9CgQUhOTkbLli3h5OSE+vXr4/3334eLiwtGjx6NvXv3YsqUKSXOrmiLmjRpAjc3N3z55ZcACq5t0mg00Ol0SEpKwunTpzFhwgS8+uqr0Ol0uH79OtavXw+NRiNx5cXJ5XLodDq0aNECwcHBWL9+PbRaLUwmE0JDQ5Gbm4tRo0bBxcUFjz/+OEJDQ3HkyBE0btxY6tLvSKFQIDAwEKmpqUhJSUHv3r0xePBgpKeno1atWjhz5gyeeuopdO7cGVFRUejatSv27duH8ePHS116EfXr14ezszPCw8MBAIGBgXB3d4dMJsOJEydw7do1vP322+jatSsOHTqEo0ePYsWKFVCr1RJXXjZnZ2fUrl0bTk5OcHV1xdixY9GuXTvk5OTA3d0d+/btw+uvv466deti3rx56NixIw4dOoTQ0FCpSy+i8Pq5vn37YsCAAfjtt9+QlpYGmUyGHj16QK1WY9SoUTh9+jRCQ0Px8ssvIyEhAf7+/hJXfmdmsxlGoxHbt29HVlYWkpOTkZaWBldXV/Tp0wd79uzBc889h6lTp6JZs2ZwdnbGn3/+icDAQKlLt3Lk4+MoryFH+pt6O4vFgo4dO2LPnj3YuHEjVq5ciby8PDg7O6N169ZYtWoVWrVqhffeew+ffvopunXrhn379qFhw4ZSl27lyK8hwDGOkSO/hsxmM+rUqYPo6GjExcVh3759yMzMhJOTE7p164aNGzeif//+eP/99zFy5Eg0aNAAu3btQv369aUuvUT2+nvbPhJaDTRs2DAcPHgQ3bt3x2OPPYYdO3Zg1qxZCAgIQE5ODv7++2906dLFOhTEw8MDXl5eUpddpvj4eCgUCjRq1AgBAQEYP348Vq5ciaioKKSlpWH9+vVwc3PDsGHDkJSUBLVajUcffRSPPvooDAYDnJ2dpW6hRK6urhg0aBD++OMP+Pj4ICEhATKZDE5OTvDx8cGyZcuwfv16TJ06Ff3798eVK1fw6aef2tQnvLfLycmBh4cHAKB9+/Y4f/483nzzTdSvXx/Hjx9Ho0aN8OCDD2LFihUwm8145ZVXMGrUKFy+fBm1a9dGnTp1JO6gqMaNGyMkJARLlizBL7/8gsuXL2PDhg3w8PDAgAEDkJSUhIcffhgTJ07E5MmTodPp4ObmJnXZpcrLy7OObGnatCl27dqFOXPmQCaT4fz586hVqxZGjhxpnbTv888/R3x8PK5fv45GjRrhvvvuk7L8YpycnNChQwds2bIF/v7+2LFjB4QQkMlkqFevHsLDw/HNN99ArVbj2LFjuHjxImJiYmz2DcH58+dx9uxZDBw4EE2aNMHbb7+N+fPnY+vWrUhKSkJERAQAwMvLC7/99hvGjBmDYcOGAYC1b1shhIBCoUD79u2xefNmhzg+gGO8hhz1bypQMHQ0MjIS06ZNg7u7O5555hlERkZi/vz5AIClS5cCKPjdvmLFCgBA8+bN0bx5c6lKLpWjvoYc4Rg58mvo7NmziImJwYQJE1CrVi1MnDgRS5cuxTvvvIOcnBzr8aldu7b10rru3buje/fuElZ9ZxaLBXK5HI0bN0Z0dLR9/d6W4kJyKu7y5ctiy5YtYs2aNSIxMVEIUTBZ1aJFi8SSJUuskxtYLBYxYcIE8dNPP0lZboUdPXpUODs7i++++866LT8/X1y7dk38888/Ii0tzbo9PDxc9OrVSxiNRmEymaQo946OHTsmpk6dar195MgRERYWJnr06CEGDx4sEhIShBAFE1eEhoZKVeZdOXr0qOjcubM4d+6cddv69evFp59+Kt566y1x6dIlIUTBxHHBwcFi8+bNUpVaqvj4ePHll1+K//u//xN//vmnyMvLEwaDQZw6dUps2rRJXL9+3brvt99+K4YPHy7MZrMwGAwSVl0+x44dEyNGjLD+nhBCiHnz5ol3331XjB8/Xly8eFEIIURycrJo2rSp2Ldvn1SllunkyZNi/vz51tubN28WL730kujevbvo0aOH9TW0adMm66Qv9iA2NlbI5XKxZMmSItv//fdfkZycLDIyMqzbkpKSxDPPPFPNFZbP7a+FHTt2iAkTJtj98RHCMV5DjvY39VZHjx4VKpVKfPLJJ0W263Q6YTAYRE5OTpHtTz75pDh9+nR1llgujvwacoRj5MivodjYWOHk5CS+/vrrIttTUlJEVlaW0Gq11m16vV6EhoaK9PT06i6zQtLS0sTVq1etE+EKIcR3330n3n77bbv5vc3QbQOOHz8uatWqJR566CGhUChEu3btxKuvvmq9v3BmxML/rlu3TkyYMEGSWu9GbGyscHV1FW+//XaZ+xX+Irtx44YYMmRIdZR2V44ePSpcXV2LzSxqsViETqcTOp3Ous1kMolevXqJgwcPCrPZXN2lVljhH6EpU6YIIQp6ulXhMSp8MzFnzhyxfPnyEveVSlxcnNBoNKJr167ikUceESqVSgwfPlxs2bKlyH6Fr6fY2Fi7ecNz9OhRIZfLxYcffiiEEMV+pgp70uv1Qggh3njjjf9v784Dsij+P4C/F+S+FFBUUESRQwVEQcXbUEElL8wjNRXT8qjE+0AxNE/Mo0MrK48KTNOy8ipFJO3wAgXEAxRF8cL7Ap7nef/+4PdsPAKK9dVnH5rXP8ruPjD7zM7uzOzMZ+QIrEpy9OhRmpubc+HChTrb79y5w1u3bulUCO7fv88uXbowOztbPl+lXGuPS05OppWVFSdMmFDqfm26tedx8+ZNent786+//nphaSyPtLQ09ujRgx06dGCnTp24Y8cOPnr0iGRRxcdQ84esGGWooj1Ti9OWoaedm1qtplqtlp+xSlu9paKXIUPPo/9CGSrrOaSlVqvlemuTJk24a9euF5TCZ5eSksJGjRrR09OTtra27NevH5OTk+X92vu1ku/bpIherne3b9/G4MGDMWDAAPzyyy/Izs5Gr169sHv3bnTt2hVA0dBLtVotz9c2MjLC6dOnDSLAxunTpxEYGIhp06Zh8eLFUKlU2LlzJz777DMkJibK61MCf69fa25ujjt37iA3N1dx55iSkoJWrVph9OjRmDt3rs4+SZJgYWEBc3NzkIRarYaxsTFsbW1x7do1GBkpu7ilpqYiKCgIkydPxrx58wAUBau4dOmSfIyxsTE0Gg1MTEwAFM0TiouL00t6S/Pw4UNMmzYNgwYNwt69e/HHH3/g+++/x82bN7FgwQJs3rxZPlZbnmrUqIFr167h6tWrirveijt+/DiCgoIwbdo0zJkzB0DRveDGjRvyMZUqVQJJecibsbEx1q5dC0A5a8KnpKSgdevWGDNmDCZPnqyzz8bGBnZ2drCxsZHLkCRJuHv3Li5cuCCXISUNv9bKyMhA27ZtMWzYMMTGxsplIzY2FkuXLkVBQYGcbiMjI2g0Gtja2qJfv35wdnbWc+r/dvr0aQQFBcHOzg4hISFQqVQYN24cJk+ejAsXLsDe3h7W1tbQaDQGlT9AxShDFe2ZWpx2lYnw8HD53BYsWIA33ngD/fv3x/79+3Hv3j0ARfkmSRKMjY3xxhtvwM/PT8+p/1tFLkMVIY8qchnKzMxEu3bt8OqrryI2NhYqlQorV67EtGnTMGHCBJw/f14+VnutWVhYYPjw4XB3d9dXsp/o4sWL6Nq1K7p164ZVq1Zh48aNSE5OxtixY+V7s6mpKTQajWLv2zL9tPUFrezsbHp4ePDAgQPytrt37/Lbb7+lp6enzhu44j2fxdeDVqqCggJOnjyZ5ubm/Pnnn0mSXbp0YcOGDVmzZk2am5tz8ODBOkNAtOeoxHUPz58/TysrK3mN7fz8fL733nscPnw4+/fvzx07dvDWrVslPrd27VrFDat63NWrV+np6cnGjRvL20aMGMGgoCBWrVqVgwcP5uHDh+V92nzKy8vTGaKpbyqViv7+/iWGvP3+++/s3r07Q0ND5XVCiys+OkGJLl68yBo1ajA4OFjeNm7cOAYHB7NRo0acMmUKr127Ju/Tvi05fvy4ovInKyuL1tbWfPPNN0kW3SNWrFjBiRMn8p133mF6errcU13cihUrePLkyRed3Gcya9YsSpLEb7/9ltevX+dLL73EoKAguru7s3bt2qxTp458Hyj+dlVp196sWbPYo0cPnW3z5s1jixYtGBERwdzc3BKfMYT8qQhlqKI9Ux+3YcMGent7c/jw4UxPT2enTp3Ypk0bvvzyy2zVqhUdHR350UcfyW+MlaqiliGS3LRpk0HnUUUvQx9++CEdHBw4b948ZmdnMzg4WB7117BhQzo4OHDbtm0kdZ9DSj63bdu20dPTk9evX5e3Xbp0iWFhYWzdujU3bdokb1fifbs40ejWsxs3btDNzY2xsbE62x89esS1a9fS19eXq1atkrcrechRaY4fP8533nmHHh4erF27Nrt3785jx45RrVZz27ZtbNSoEd944w2Syj+3H3/8kX5+fuzZsyezs7MZGhrKli1bMjw8nM2aNaOnpyfnzZtXYi6TIbh8+TIjIiLYvn17zp07l23atGGXLl24ePFifvXVV6xTpw47dOggVwqUmFcajYb3799n165dOW7cOJLUmXu1b98+NmjQgFOnTiVZclipkl28eJHt2rVjWFgY161bx1atWrFz586cMmUK33vvPVpbW7NHjx68f/8+SWXmD0l++umnrFOnDidOnMjc3Fx27tyZQUFBbN++PevXr083NzfGxcUZxJw5rYcPH8r/HzNmDOvWrUtPT0+GhYUxKyuLN2/e5KVLl9ihQwc2bNhQ0ZUbkpw0aRKbNWtWotK8dOlSNm/enAsWLCi1Y0TpKkoZqkjPVK3bt2/L/1+7di3btWvHKlWqMDQ0lJcvX5bv1W+//Tbt7e154cIFfSW1XCpqGdIy9DyqiGWoeL1z3rx5bNy4MWvVqsWuXbsyJydH7tzt27cva9eurTO9Qem2bdvGWrVqMTMzk+TfQ8hzc3PZoUMHBgcHyw1ypeeXaHTr2aNHjzhkyBCGhoby2LFjOvvu37/P7t27s3///npK3f9GamoqR44cyS5duugEQCDJL774giYmJorskSrNli1b2K5dO5qamrJLly46AbnGjRtHV1dXZmVl6TGFzyYnJ0d+OObm5vKtt95izZo12blzZ51zy8nJYZUqVRgdHa2nlJbfypUraWpqyp07d5LUbVx//PHHtLGx4dWrV/WVvGemTf/Zs2cZFhZGR0dHhoWF6eTP8ePHaW5urhMQRqmWLVvGoKAgOjg4MDQ0lJcuXZIb2b1792bdunUNpkJw6tQpRkZG6lxPY8aMYUBAQIl73b59+2hvb8/9+/e/6GQ+k+XLl9PT01MO8lS8k+Dtt9+mm5tbqSN6lKyilaH09PQK80w9efIkW7duzYSEBHnbmjVr+Oqrr8qjkrT5p1araW1tXSJIodJUxDL0uC+++MKg86gilaFTp06xb9++TEtLk7fNmzePISEhPHr0qM6x586do7W1Nbds2fJiE/kvnDt3jjY2Npw9e7a8TRtXKDs7m9bW1iUCximVaHQrwPHjx+nk5MS+ffvKN2mtJUuWsEmTJnLvu9JdvHiRv/76K+Pi4nQanxkZGdyzZ49cULQ36O+++44NGjRQ/AOoeO/Zt99+yxEjRsiV5+KBT0xNTfnZZ5/pJY3PKj09na6urpw6dap8DpcvX2ZMTAy3b99eYkhVly5dFBtpmdTNo9dff502Njb87bffdI7ZtWsXfXx8dKKSKlVpUVLPnTvHUaNG6USM1+ZdQEAAIyMjX2gan0Xxzo8lS5awT58+PHTokM6+mzdv0tjYmN99951e0vgsUlJSaGZmRkmSSjSk9+7dK78B116Xv/76Kz08PErc45VGrVbTy8uLnTp1kq8/7T1ApVLR2tqaX3/9tT6TWG4VrQwVd+rUKYN+ppJFwaysrKwoSRI/+ugjnX0pKSkl3hRnZGSwUaNGTEpKepHJfGYVqQydOXOG7733Hl977TWuXbtWXr2ENOw8IitGGdIG9pUkiZs3b9bZ9/vvv8v5o30OHTx4kF5eXjpByJRMmyerV69mpUqV5Pq1RqORy9bLL7/MUaNG6S2Nz0Ks061nGo0GjRo1wg8//IDg4GBoNBqMHj0aHTp0AFAUnMfFxUUO+qRkx48fR8+ePeHk5ISDBw+iTZs2iIyMxMsvvwxPT094eHjoBBICgAMHDsDFxUUOVqFUkiTJa2q+8soraNCgAerXrw/g76BIWVlZ8PLygpeXl55T+3TJyclo2bIlTExMsGPHDsyfPx8A4OTkhPHjx8PExETOq0qVKqGgoAAk4ePjo89kl8BiaxoXDzyzYMECPHz4EJ07d8bKlSvRtm1b1KpVCzt37oSRkZHig9qdPHkSc+bMQU5ODiwtLTF//nz4+vrC1dUVCxYs0Fkb1MjICPfv34eNjY2irz1tOTEyMsL48ePx119/oVGjRvI+kjh79izq16+PevXq6Tm1T5aSkoKgoCCMGjUKFy9exJIlS+Dn5wcLCwsYGRnprHOqvS537tyJGjVqwN7eXl/Jfipt8Mf4+Hh06dIFPXr0QFxcHGxsbAAAN27cQN26dVG1alU9p/TpKmIZKq5+/fpwd3c32GdqSkoKWrZsiejoaKhUKsyZMwfdu3eHi4sLAMDX17fEZ9avX49KlSop+v5QkcpQamoqOnfuDH9/fxQUFGDMmDEYO3asXF8whDzSPnNKUxHKUFBQECZOnIibN29izpw5aNeunfyMadGihXys9hw3b94MOzs71KxZUy9pflbaPOnduzcyMzMxevRoFBQUYPTo0XL+FBYWwtbWVp/JLD/9tvn/O7TLJjy+jfx73umhQ4fYuHFjNmnShH5+fuzRowdtbW0NokfqzJkzrFWrFmfMmMHr168zOzubLVu25ODBg8s8fvr06axcuTKPHz/+glP7zz1pvsisWbPo6+vLixcvvsAUPbvk5GRaWloyJiaGV69eZbVq1bh06dInfmb69Ok6c2r0rfhax2V58OABp06dSnt7e9auXZsBAQF0cHDgkSNHnn8C/4XU1FQ6OjoyIiKCMTExDAwMpJeX1xMDbs2YMYN16tTh2bNnX1xCn6KsQDpPKkMzZsxgYGCgzrBfpTly5AhtbGzkJQMXLlxIZ2dn+bt/PFbA4cOHOXHiRNrZ2TElJeVFJ/cfS0pKYu3atRkYGMi4uDju27eP06dPp5OTk7weqlJVlDKkVZ74E4b0TNWu8axdmjIpKYn16tXj+vXrSbJEXenHH39kZGQkbW1tSwyXVTJDLkPnz5+nt7c3p0yZIm+Lj4+nhYUFT58+XeJ4JeZRVlYWP/nkE6pUqgpXho4ePUpLS0tOnz6dJBkXF0dnZ2d5dN/j57t3715OmDCBdnZ2im5TaDSaMusI2pGYkiSxf//+nDhxIkeNGkVra2vFByvWEo3uFyAtLY0DBw5kcHAw33zzTf7000/yPu3DRftvdnY2N2/ezLFjx3LhwoUGcSE9evSIEydO5MCBA3n//n35XH744QfWrFlTJ+IgWfR9dOrUiZ6enoq5Of8bP/30EyMjI2lnZ6f48zl+/LhOZefWrVvs2rUre/XqVerxGzZsYO/evVmtWjXFNFaPHj3K1q1bl7sBc+DAAW7cuJFff/21IivUxV2+fJktWrSQA8GRRQ8hFxcXrly5ssTxP/30EwcMGEBHR0fF5A9ZNMRwwoQJ8rC9p9m1axcnTZpEGxsbRVcIrl+/TkmSOHnyZHlbYWEh69atK69qUFxmZiYnTJhAb29vRZ8XWXpnSG5uLkNDQ+nl5UVXV1f6+vrqrGKgRBWhDF2+fJl//fUXt27dKm97UmdVamqqwTxT79y5w4CAALnTSis0NJTNmzcvcbxarea7777LgICAEnFv9Ck7O7tc9TNDLENqtZqffvopw8PDmZOTQ41GQ7VazZs3b9LLy6vECiBqtZqzZ89WVB6lpKTQ3NycNWvWlOukFaUM3bx5k9WqVZODwmr5+/szLCysxPF5eXmcPn06fX19Fdvxq+2kL0+dITExkWFhYQwODmbv3r0Ve06lEY3u5ywjI4N2dnbs378/p06dSj8/PwYEBOhUCLSR+JQeda8sDx8+5LRp0/j555/rbN+/fz+rVKnCS5culfhMUlKSztwgpcjKyuL777/P8ePHMz4+vtRjHs+nKVOmsFWrVop52DzJjBkzOGfOHJ1tO3bsoJGRkbyMRHGpqakcOnRoiUAj+pKcnEwTExNOmjSpxL7HR44YYnnasWMHW7RoIX/f2nmAbdu2LbHCQUFBAXfv3s2+ffsyNTX1hae1LNrKjiRJOh2MxRXPm4cPH/Ktt95SdIWguD///FP+v/Zai4mJob+/f4mRIAUFBTxz5kypSwTpy+nTpzl//nxOnTqV33zzDe/evSvv0+bL42Xn3LlzzMzMLNGBqkSGXoaOHTtGX19fNmjQgNbW1gwICJDf0GvzpbS3dkp9ppbm1KlT8v+1+bN//366uLiUOddZSXE4jhw5wqpVq3Ljxo1lHmPIZYgsikHxeF1BpVKxbt26Oks0FaeUPNKO5hs0aBDr16/P9957r8Qxhl6Gij8rtWVo/fr19PDwYGJiYonjb968qdgAsqmpqezVqxc7duzIkJAQJiYmlojurz1HbbnSxkxR6tJ0ZRGN7udIo9Fw+vTp7Nu3r7ztzp07nDt3Lhs3bswRI0boHP/9998reljlkxQvzNqb2blz5+jl5aUzFLj42odKc+zYMbq4uDA4OJgtW7akkZERFy1aVObxxfNKKQ+bZ6EdxnP37l2GhYVx0KBBfPDgQYnKglKWOEpNTaWFhQVnzZpFsij9eXl5ZUaLX7lypRyoy5B88MEH8v+1332fPn10IneSfzf4ii9ZpW/aoC5jxoxh//795dEvxa+p4v/XBoh8+PCh4u99TxqemJ6eTktLS528U6LU1FRWrlyZ7dq1Y9u2bVmpUiWGh4dzx44d8jHFh/YaShT5xxlqGTp16hSdnJw4ffp0pqen8+TJk/Ty8iozgOWKFSv47bffvuBU/nNP6gi9cuUKAwMDGRERUe7P6ENycjKtrKyeGHCv+L3CUMtQcdo8UKlUbNCggU7Aru+++44HDx7UV9JK0OaPdiRF//792alTpzLrMRWpDJ09e5a1atWSRzIaglOnTtHW1pYjR47kpEmT2KdPH0qSxOjo6FI7QLQjL7SUdn94GtHofs6GDh3Ktm3b6my7c+cOY2NjGRAQwPnz55MsGuLm4uLCGTNmGNT6wY8rXgBOnTrF6tWry2+6Z8yYwcaNGyuyt+3cuXN0d3fn5MmT5e//888/p5OTk06vvFZ0dDQjIiJK3adUT7o5LV68mJUrV2ZOTs5Tj9WH69ev093dnf7+/vK2YcOGsWnTpqxRowbbtm3Lo0ePyum+cuUKJUlijx49DKYntKyYD2TRUlpvvfWW/PPHH3/MLVu2KCqfDh8+rDPXedmyZbS1tZXn/z2e1sjISEZHRxvMm5+yaPNp4sSJ9PHxUeybkgcPHjAsLIxjxoyRtx0+fJgBAQHs2LFjici32vy5cePGi07qP2bIZej+/fscMmQI33jjDZ3zWLJkCVu1alXi+Ly8PLq5uTE0NFRntIIh27BhA01NTRUb+frEiRM682gLCwu5d+9ebtmyhfv27StxvCGWobKo1Wqq1WoGBQXx119/JUlOnTqVNjY2ipm2lZmZSUmSdKYuJCUlUZKkUlfEqEhlSHsfW758OR0dHRU/J10rKiqKnTt31tm2YsUKOjg4cMqUKbx8+bK8PTY2ljVq1DCIUaVlUXYIXwNGEgDQpEkTqNVqnDx5Ut5nY2ODiIgI+Pv748cff0RBQQG6deuGiIgIREREKD6y8pMUjyD96NEj3LlzByYmJoiJicGiRYvw2WefKS5qp0ajQXx8PNzd3TF9+nT5+w8MDISJiQk0Gk2Jz1haWmL//v2GEzERunmjpb1Ox48fj9q1ayM6OlonIrhSODg4IDQ0FFZWVpg9ezaaNWuG3NxcvPHGG/j4449RWFiInj17IjMzEwBQrVo1ZGRkYOHChTAzM9Nz6svn8UipRkZGUKvVAAATExNYWloCAGbNmoUxY8bA09NTMfl069YttGnTBiNGjMDcuXMBAKNHj4aHhwfmzJlT6jUlSRI++OADfST3f0p7v2jfvj2uX7+OEydO6DlFpbOwsMCNGzfg6OgIoOi+16RJE6xfvx4qlQqffvopUlJS5OO1+VPa/U+pDLkMmZubw9zcHO7u7jrn4efnh+zsbNy6dQuFhYUAiu7b9vb22LNnDz766CNYW1vrK9nlon3OPE1QUBBatGiBn376SXHXnVqtxvTp02FpaYnu3bsDKIqo/M477+DNN99EcHAwxo4di2vXrsmfMcQyVBbt6hN3795FQUEB3n33XaxYsQJ79uxBnTp19J08AEDdunXx+eefy88gtVqN5s2bo2fPnvjmm29w9+5d+VhtGUpISDCIMvQ02vtYq1at4ODggN9//13PKSqfhw8fyv9XqVQAgLfeegvvvfcePvzwQ2zZskXe/+qrr8LDw0O+jxskPTb4/xPOnDkjR1HV9qRpe6TOnz9PSZL4448/6jOJz+T+/fsl5lqUJTMzk/7+/hwxYgTNzMwUPdQ3MTGxRFAKtVrNOnXqMCEhodTPGFLv9dPe5qhUKkZFRdHb27vUOfj6VPxt1fjx4+nk5MRu3brp9ICSZMOGDTlkyBCSyhkS/29pz2PgwIFctGgRFy1aRAsLC0WWpeLBZ7RraE6fPp0NGzaUR7c8HplUiaNeSlPe0UctWrRgu3btnm9inpE27Xfu3GGHDh3k9UxVKpV8faWlpdHFxYXvvPOOzmcNJX+eROll6P79+/Lw9uLR1bXlJCEhge7u7jpvv3Nycsr9HNanx8tNeUYVjBw5km5ubooZ8k+SFy5c4NmzZ5mens6QkBCGhITQy8uLoaGhPHLkCLOzs/nzzz/T1NS0xNDeilCGtAoLC9m8eXN6e3vT3NxcMWXocY9fZx9//DHt7OyYkZFB8u/r0pBHlT7Jyy+/TF9f3xIjf5Ro+fLltLGxkVf9KX5fe/fdd2ltba0zeswQzulJRKP7BdizZw/NzMw4ZswYXrt2Td6em5tLPz8/HjhwQI+pK7/jx4+zW7duTExMLNeQ3RMnTlCSJNrZ2SkmKmxxZRXe4sFq3NzcuGvXLnnfr7/+KgdFUsqwxNLcv3+fN27ceKaKS1paGmvVqiUPMde3e/fu8c6dO7x9+7bO9tjYWH733Xc688xIMjw8nH369Hnh6fynnuX6GTZsGCVJopWVlaLmz+Xl5TE9PZ2nTp3SeVgWH+ZvY2PDmJgYnc9pKztKLUM5OTnctWsX16xZIzfanlRB0x6zffv2UpfT0ZejR48yLCyM9+7dI0lu3LhRZ6ilWq2Wo8V+8803rFKlCrOzs0sErVGizMzMUgMGlUWJZaj4M7V4g7v4tbZ3717Wq1dPzouJEyeyffv2cjwEpTp16hSjoqI4ZMgQrlu37qnTSLTX3O3btxW1nFZqaipdXFzk4LcHDx5kq1at2KlTpxLDqj/88EM6OjrywoULBlGGnqV+oNFoePv2bTZq1IiVK1dWzBDfCxcucMOGDfzuu+9KpKn4d9+8eXP2799f0fnxuOzsbG7cuJFLlizhhQsXnnp88U5UpSzv+jT5+fls27YtW7RoId8jtNdlbm4ua9WqVerUAEMlGt0vyNatW2lmZsbevXszPj6e6enpnDp1KmvUqFGuwqRv2gA8b7zxBs+fP19iv3ZJieJyc3MZHh6uyGXPTp48ydjYWJ23usVvxoWFhbx37x7d3d3l5TGmTZtGSZIUvw53amoqw8LC6O3tzZ49e5YaQfrxvNI2XJUyryktLY2dO3emv78/a9asya+++kqnk+TxtzwajYZ9+vTRCbKmVMUrn2Wl8/Eo0kOGDKEkSYqJIk8WNRj8/f3p4+NDMzMzzpkzRyePtP+fOHEiW7Zsqdi5zo87duwYPTw82KRJE1pZWbFJkyYlljExhN52bVC74uvsFhYWcuzYsTQzM9NZjookt23bRm9vb4OYY5+SksIaNWpw+PDhZQbgU3oZKs8zlSxa8rBmzZosLCzk9OnTaWFhUWLJJqU5duwYHRwc2K9fPzZv3pyNGjUqcb2V1omltDeP2ijYbm5udHJykusLGRkZ3LRpU4mG9YcffkgfHx9FvaUvS2pqKlu0aMG9e/eWeUxp+REfH8+0tLTnmbRyO3bsGF1dXRkQEEAnJye+/PLLPHPmTInjNBoN58yZw0aNGinmpcLTHDt2jHXq1GHLli1pb29PNze3EqtgGMJzqLiTJ09y8uTJHDp0KJctWybHRNq9ezebNWvG4OBgnaDEN27coJeXl0GNBn4a0eh+gQ4fPsx27drR1dWV9erVo4eHhyLfAD/u3r177Ny5szwskSx6i3306NFSK9JffPGF3AOsxCFwp0+fpr29PSVJ4rRp03RGH2ip1Wo+fPiQ9erV46FDhxgTE0MrKyv+9ddfekhx+aWlpbFKlSocM2YMV61axVatWvHVV1/VOab4jVqJDaG0tDQ6ODgwMjKSX3/9NcePH08TE5My184sLCxkVFQUa9Sooai3jKVJS0ujsbGxTjCrp0X01VJSz7U2jyZOnMi0tDTGxsZSkqRSGw+7du2ijY0Nt2zZ8uIT+oxOnDhBR0dHRkVFMTs7m1lZWXR0dCxz6bOVK1cqcs3dlJQUWllZlVhaT6VS8fr16xwzZgxNTEy4cuVK5ubm8uHDh/KSlkqfNpOVlcXq1atz0qRJZZad4tNLlFiGnuWZ+ttvv9HPz4+RkZE0NTVV5PVW3NWrV9m4cWNGRUXJ29q3b8+5c+eWevz777/PxYsXv6jklZu202r69Om8du0aGzZsyJiYmCcuSfnOO+8wPDxc8aMQzp07R09PT5qamtLZ2fmpgevef/99Lly48AWlrnzOnTtHZ2dnTp06lffu3eO2bdtYvXp1nSUdyb/rOzdv3qQkSSWWQVOijIwMVqtWjTNnzuSNGzeo0WhYs2ZNrlu3rtTjV6xYUeYybkqRlpZGOzs7hoaGMjw8nHZ2dnzppZfkc/rxxx/ZrFkzurm5cefOndyzZw+joqJYvXp1RdZT/ynR6H7Bbt++zbNnz/LYsWOlNvaU6NGjR2zdujWPHDlClUrFkJAQBgYG0sbGhi1atODq1avlY/ft28f69etz4MCBLCwsVNwbx3v37jEiIoJDhw7lRx99REmSOGnSpDLzwt/fn4GBgTQ1NVXMkMSyPHjwgD179tSZl/nDDz+wV69evHLlSon1eGNjY9mxY0dFzcvKy8tj586d+fbbb+tsb9++vRx5uPg1tWvXLr788susXr264juwLl68yGbNmjEgIIDW1tY6kZRLKyfaCPnaeWhKce3aNbZt21bnOtNoNAwNDeWBAwd49OjREo3vLl26sE2bNlSr1Yq7J2jdvHmTXbt2lYeRaoWEhPCzzz7j+++/z/T0dHkY8OXLlxUZIT83N5fVq1dnSEgIyaJK57hx49ilSxc2aNCAH3zwARMSErhixQqamprSzc2Nvr6+rFq1quLLEEmuW7eOvXv3Jlm0zvaCBQsYERHBqKgo7tmzR+dYpZahZ3mm7tq1i5Ik0cHBQfENbrLoDZ2np6fOc2XYsGEcMmQIw8LCGB0dLefHzZs32bFjR7Zv315RnT0pKSk0MzOTo5Sr1Wr26dOHgYGB8jHF3wJnZmZy5syZrFy5smLWey9LQUEBY2Nj2aNHDx47dox9+vSho6NjmQ3v27dvKzKPPvnkE7Zv317nedK1a1d+8sknXLt2rc69QDtSacGCBYoceVnc3bt3OXjwYL711ltUqVTy+XXv3p3z58/nlClT+Ouvv8ojkq5fv674COz5+fkcNGiQzjLJp0+fZr9+/RgYGMhPPvmEZNHSmwMGDGDVqlXp4eHBhg0bGsQ971lU0ncgt/8aW1tbg4p4DRRFJj558iSuX7+OSZMmAQBWr16NS5cuYc+ePYiKioKdnR369OmDNm3aYPLkyQgODkalSsq7vIyMjNC0aVM4ODigX79+cHR0RP/+/QEAkydPliP7qtVq3L59G1lZWbh37x6OHj0KHx8ffSb9qczMzJCXlwd/f395W1JSEo4ePYomTZrA09MTzZo1w/z58yFJEhwcHFBQUAAnJyc9plpXYWEhbt26hT59+gAoirBsZGQENzc33LhxA8DfUTpJws3NDQ0aNMCiRYvg5eWlt3Q/jUajwd69e+Hq6opx48YhJycHQ4cOBQCsWLECkiTJ56plYWGB/fv3o3LlyvpJdBkkSUJoaKicRwAwd+5c7Ny5E5cvX8b169fRsGFDREVFoXXr1gCAkSNHwsfHR9ErM1SuXBlhYWFo3LixvG3u3LnYvXs3CgoKcPnyZSxcuBArVqxA37594eTkhIyMDJBUXIT8oKAgXLhwAT/88ANWrVqFwsJCNG7cGG5ubli2bBk6dOiAZcuWoV27dvI5tGjRAq6urvpO+lMdPXpUjnjbuXNnFBQUwNXVFRs3bkRCQgIGDRqEN998E8Dfq0worQw9yzO1WbNm6Ny5MxYtWgRfX189p/zpHj58CJVKhT///BOurq747LPP8NVXX2HatGmQJAkHDhzA4cOH8fnnn6NatWpy9PwqVaroO+my/Px8TJ48GTExMfJ9ee7cuWjevDlWrlyJUaNGyfey9PR0TJ8+HSkpKUhISEDDhg31nPonMzExgZ+fH9zc3ODj44Nvv/0Wr7zyCnr16oUtW7bI92yg6Blra2uLNWvWgKSi8ogkzp8/j+TkZPj7++O9997D9u3bUVBQgNu3byM7OxsLFy7E0KFDYWJiAgCYMGGCIuulxVlbW6Nr166oU6eOvIrBnDlzsG3bNpiYmCAnJwcbN27EyJEjMWHCBDg4OCAhIQFqtVqxEdhNTU1x5coVuLm5ASjKO3d3dyxatAjR0dFYt24datWqhS5duuCbb75BRkYGbG1tYWpqKtfJKww9NvgFA6HRaNi/f3+OHTuWYWFh3LFjh7zvwoULHDRoEN98801FDiUvjTaokFZ8fDwlSeLEiRPl3sPCwkJeu3aNO3bsUHzPNVnU63779m2GhISwV69e/Oijjzht2jRaWFjwyy+/5Pbt2/nuu++ySZMmOsN879y5o79El6H42ufaHuqoqCgOHjxY5zjtED5Dmdd0/vx5nXmNcXFxtLCwKPHGu/gblOLzm5Sk+HUTFxdHSZK4YcMG5uXlMTExkYGBgZw9e7YeU1h+ly5dKjUo0L59+1ivXj1u3bpVvta6d+/OgIAAksq+7i5dusTXXnuNFhYW7NSpk8487a+++op2dnYGNU+u+Hf95ZdfyrFROnbsKK9ikJubyyFDhrBjx446EaOV9HZO61mfqY/HFFCax1c1GTp0KN3d3RkcHExLS0ud+97XX39NZ2dng3qDpdFoeOvWLfbs2ZN9+/alSqWS79P5+flMSEhQzFrV/0RhYaH8xvu3336Tt/3yyy+Kjb6elZXFli1b0t3dneHh4ZQkid9//z01Gg2vXLnCt99+m+3bt+f169cVFyugNMWDWhaXkpJCb29v/vjjj/K0mTfffJOenp68d++eYkeOaalUKhYUFHDYsGHs06cPHz16pFPPyczMZFBQEPv27St/Runn9G+IRrdQLgcPHqSVlRUlSSoREGXChAls27atwRWU4kN3tA2HSZMm8eLFi4yMjGSvXr0UPzfr8Yr/H3/8wdDQUL766qv09PTk559/Lu+7fPkya9euzfnz57/oZP4jxR+UM2bMkIfLkuS8efMYGxur+KXBtNG9T548WWKfSqVifHy8TsNbpVJx/fr18vx1QyhT586dK1GB7tatG19++WU9paj8cnJy6ODgwF69epWI15CTkyPPAdZeZ4sXL2bz5s0V3wgii6YzTJs2jbt37yapey25u7tz4sSJ+kraM9FGYNfeiw8ePEhzc3P6+/vLw8y1MjIyKEkSd+7cKW9Tahl62jNVOx1D6YpHYC/eoZ2amsqkpCR6eXnpRCNPT09n/fr1FR8fpTTfffcdJUmSG6aGoniE7+TkZHl78bJRUFAgN7wTEhL4xhtv0MvLq8xAhUqQlZXFDRs2MDo6usTKJQsWLKCfn59BBLVLS0vj4MGD2aFDB77++uuMi4uT9+Xl5cnBe7XPnS+//JJ+fn6KfGmi9XjddO/evTQ2Nuby5ctLHLN3714aGRkZxAuuf0u5Y/0ERQkICMD27dsBAJ9++inS0tLkfYWFhfDw8JAXtjcU2qE7Go0G/fv3R1xcHJYtW4aXXnoJK1aswKxZs2BpaannVJbt1KlTWLZsGXJzc+VtzZs3x6ZNm7Bu3TrY29vrDDeyt7eHp6enwUxvMDIyAkmdnwFg1qxZmDFjBjp27KjooWKpqano2LEj+vXrBx8fH8ydOxcqlUo+J2NjY4SHh2PNmjVYvXo13n77bYwfPx7Dhg2Dvb09gL+H0iuZq6srmjRpAqCoLD169AjW1tZo0aKFnlP2dKdPn8bt27dx+/ZtfPjhhzhy5Ii8z9nZWR5urb3OMjIy0LBhQ4PIl5o1a2Lq1KnycFFJkkASeXl5qFq1qs40FKVKSUlBy5Yt0bBhQ1haWoIkAgICsGzZMhw/fhyZmZnIysqSj3d0dERQUJBcfgDllqGnPVM9PT2hVqv1lbxySUtLQ5s2beDi4gI3NzdYWVnJ+7R5VqlSJZ08WLduHczNzVGnTh09pPjfCQsLQ6dOnbBy5Up5ioPSHT9+HK1bt8bixYsxevRozJw5E5mZmQB0y4aJiQni4uLQoUMHvPTSS/j666+xfv16VKtWTV9Jfyo3Nzf07dsXLi4uePjwIQoKCuR9V65cQZ06dRRfhjIyMtC6dWuYmpoiLCwMFy5cwKxZs/DWW28BKKq3Va1aFQDkYfIHDx6Eh4eH/LPSlFY3bdeuHRYuXIjIyEisXr0awN91cBsbG3h6eurcPyos/bb5BUOTmJjImjVrslmzZhw+fDgHDx5MOzs7Hj9+XN9J+8c0Go3c4/vSSy/R3t5eMWtQluVJEdjVajXv3bvH5s2bc+bMmbx58ybv3r3LmTNnskaNGszKytJjyp+N9k1PdHQ0R44cycWLF9PMzEzxQxOfJbq3SqXiN998Q0mSWKVKFUUFtvsnZs6cydq1a+tME1CqvLw8du/enZ988gmbNGnCgQMHyr3txd8yFhQUMCoqio6OjooPxPM0s2bNYv369RW1FnJpyorAnp+fT41Gw6VLl9LIyIivvfYa9+3bx8uXLzMqKop16tRR/LKOxRnqM7U8Edg1Gg09PDzo7e3NiIgIDho0iA4ODmWuRGEI5s+fT1tb2xLLNylReSN8a+91KpWKI0eOpL29vWKWBSsPbWTsRYsWcd26dZw8ebKi1hIvy6NHjzhw4ECdwLEPHz6kv78/JUnigAEDdI5/8OCB/BxSav48qW56//59vvvuu5QkiVFRUTxy5Ajz8vI4depUuru7K3Yqw/+SaHQLzywjI4NRUVHs2LEjR40apfjKQXmoVCpGRkZSkiSmpKToOzlPVN4I7Bs2bKAkSfTw8GDz5s3p6upqENGJSzN37lxKkkQ7OzvFR5F/1ujeKpWKw4cPp42NjWLWEP4nvv32W44ZM4YODg4GcZ2pVCpevXqVHh4ezMnJ4ebNmxkYGMgRI0awZcuWDA8PJ1kUPTo8PJwuLi4GcV5liYuL48iRI1mlShXFn0dZEdi7du1Kb29vLlu2jGlpafzhhx/o7OzM6tWr09vb22DvcYb4TH1aBPZVq1aRLKpo9+vXj926dePw4cMN9h6n7Zi/ceMGmzZtahBzuJ8lwrdKpeIXX3xBSZIMsuN3z549rFevHuvXr8/27dsrvh6nFRwcLMc/0Q6Fnzx5MsPDw9mkSRN5Ob0dO3awW7duir7HlVU3Ld6YVqvVXLt2LatXr05nZ2d6eXmxZs2ain+R8r8iGt3CP6ZWqw1izll5qFQqrl692iB64B88eMCPPvqI8fHxJP9uXJfW8P7tt984d+5crlq1yiAqCWU5ePAgJUlSbO9ucdevX+e8efN03vTGxMRQkiQ2btyYLi4uDAkJkZdo2b59O+vVq6f4zoSnSU1NZd++fQ2mUq2tiA4cOFAOZPXzzz/T0dGRNjY2/PLLL0kWBXopvsyRoUpJSWG3bt0MYt5cbm4ue/XqxYCAAH7//fcMDQ1lcHAwJ0yYwNGjR9PNzY0REREsLCxkdnY2f//9dyYmJvLSpUv6Tvq/YkjP1MuXL7Nq1arctWsXIyMjGRISwpSUFG7fvp2TJk1i9erVdeamklR8DI7y0Gg0JYKxKtWqVatYt25duZGm7bzu2LEjAwMDWa1aNfk+R5KHDx826HpCXl4eL1++zJs3b+o7KU+l0Wh4//59tmnThoMHD5bLRk5ODl1dXfnFF19w0KBB7NChA8miet+SJUsUPYLsSXXTx99inz17lomJidy+fTtzcnL0kVy9EI1uQfh/Sg24U5onRWDXNrwLCgoMZi348jCUig75bNG9L1++bBBDFcvDEAKMPe61117j1KlTSZLDhw9nlSpV2KBBA0ZERMjBngylIfQ0hrLCBPn0COy2trb86aef9JjC/7ZnicCufbYa0jO2IihvhO+KVE8wNL/99huNjIzYtm1bDh48mFZWVnz99ddJFgUptLa2NpiObLJ8dVNtZ+l/kXKjEAnCC6bUgDul0QacUKvVMDIyQr9+/UASr776KiRJwrhx4xAbG4tz585h/fr1sLS0NKjzK40hBdmwsbGR/x8UFIRDhw7Jwcbatm2LatWq4dChQwCgqHXS/y2lBnYpDUlIkoSXXnoJZ8+exejRo7Ft2zYcPnwYycnJmDRpEkxMTODj4wNzc3N9J/d/wtTUVN9JKLcaNWpg/vz5cHZ2RseOHeHg4CDn2cCBAzF79mwkJiaiW7du+k7qf5IkSZgwYQLat2+PBw8eYOTIkfI+FxcXODk54eDBgzAxMZGfPYb+DDI0bm5u+Oqrr3Dw4EGkp6dDkiT06NEDAFCtWjXUrFkTiYmJil3f+b+gVatW+OOPP7BixQqYmZlh0aJFGD16NAAgKysLtWrVQs2aNfWcyvIrb900Ozsb69atqxB102chGt2CYMCMjY1BUo7ALkkSBg8ejK1btyIzMxMHDx40qMZqReTq6ipHwdZoNCgoKIC1tTV8fX31nLL/Nu2D3s3NDcOGDYOTkxN++uknuLm5wc3NDZIkwc/Pr8I0uA2RNgK7Ng+0Edhv3LiBqlWronHjxvpN4H+cNgJ7u3bt8Omnn6Ju3bpo2LAhAN1VTQypM66i0d7PVq9ejUOHDqGgoEDufDOUCN8VXWBgINatW1ei8ZmUlAQnJyeDbJSKumnpJLLYmjyCIBgkbTGWJAnBwcFITk7G3r174ePjo+eUCY+bNWsW1q5di19//RX169fXd3L+8woLC7F+/XoEBATA19dXfpsqKFd0dDTi4uLwyy+/yB1agv7s27cPAwYMgIuLC3x8fFBQUICtW7fit99+Q6NGjfSdPAFAeno6WrZsiRkzZqB69epITU3Fp59+in379ol6gsIcP34cq1atwldffYV9+/bBz89P30n6x0TdVJd40y0IFYAkSVCr1Zg0aRISEhKQnJz8n72pKdXGjRuRmJiI+Ph4/PLLL6LBrRAmJiYYOnSovA68aHArV3x8PBISErBx40bs3r1bNLgVom3bttizZw+++uor/PHHH6hfv75ocCtMgwYNsGXLFowYMQJGRkZwdnZGYmKiqCcoTH5+Ps6cOYMbN24gKSnJ4EfEibqpLvGmWxAqCLVajTVr1qBp06Zi2KUCpaWlISYmBrNnz4a3t7e+kyMIBufYsWOYPn06Fi5cKA9jFpRFo9EAgNyJJSjLjRs3UFhYCDMzM1SuXFnfyRFKkZ+fD5VKVWGGX4u66d9Eo1sQKhAxNFbZCgsLxfxGQfgXis9JFQRBEJRP1E2LiEa3IAiCIAiCIAiCIDwnYvyPIAiCIAiCIAiCIDwnotEtCIIgCIIgCIIgCM+JaHQLgiAIgiAIgiAIwnMiGt2CIAiCIAiCIAiC8JyIRrcgCIIgCIIgCIIgPCei0S0IgiAIgiAIgiAIz4lodAuCIAiCwg0dOhQ9e/bUdzIEQRAEQfgHRKNbEARBEABcu3YNo0aNQu3atWFmZobq1asjJCQE+/fv13fSsHz5cqxZs0b+uX379hg3bty//r2zZ8+GJEmQJAmVKlVCnTp1EBkZiXv37v3r3/281alTB8uWLdN3MgRBEAThqSrpOwGCIAiCoATh4eEoKCjA2rVrUbduXVy5cgW7d+9GXl7ec/27BQUFMDU1feIxdnZ2z+3vN2zYEL/++itUKhX279+PiIgIPHjwAJ988skz/y6SUKvVqFRJVC8EQRAEQUu86RYEQRD+827duoWkpCQsXLgQHTp0gKurK5o1a4Zp06ahe/fu8nGSJGHlypXo0qULLCwsULduXWzatEnnd02ZMgUeHh6wtLRE3bp1MXPmTBQWFsr7Z8+ejcaNG2P16tVwc3ODubk5AGDTpk3w8fGBhYUFHBwc0LFjR9y/fx+A7vDyoUOHIjExEcuXL5ffUp89exbu7u6IjY3VSUtycjIkScKZM2fKPPdKlSqhevXqcHFxQb9+/TBw4EBs3boVAKDRaDB//ny4ubnBwsICfn5+Oue7d+9eSJKE7du3o2nTpjAzM8Nvv/0GjUaDRYsWwd3dHWZmZqhduzbee+89+XMXLlxA3759UblyZdjb26NHjx44d+6cvF97vrGxsahRowYcHBwwZswY+Xts3749srOzERkZKX8HAJCXl4cBAwbA2dkZlpaW8PHxQVxcnM753r17FwMHDoSVlRVq1KiBpUuXlhg5kJ+fj4kTJ8LZ2RlWVlZo3rw59u7dW+Z3KAiCIAhPIhrdgiAIwn+etbU1rK2t8f333yM/P/+Jx86cORPh4eFISUnBwIED0b9/f5w4cULeb2NjgzVr1iA9PR3Lly/HZ599hqVLl+r8jjNnzuC7777D5s2bkZycjNzcXAwYMAARERE4ceIE9u7di969e4Nkib+/fPlyBAUFYcSIEcjNzUVubi5q166NiIgIfPnllzrHfvnll2jbti3c3d3L/V1YWFigoKAAADB//nysW7cOq1atQlpaGiIjIzFo0CAkJibqfGbq1KlYsGABTpw4AV9fX0ybNg0LFizAzJkzkZ6ejm+++QZOTk4AgMLCQoSEhMDGxgZJSUnYv38/rK2tERoaKv9dAEhISEBmZiYSEhKwdu1arFmzRh5iv3nzZri4uCAmJkb+DgDg0aNHaNq0KX7++WekpqZi5MiRGDx4MP766y/5944fPx779+/H1q1b8csvvyApKQlHjhzROZ+xY8fi999/R3x8PI4dO4ZXXnkFoaGhOH36dLm/R0EQBEGQURAEQRAEbtq0iVWqVKG5uTlbtmzJadOmMSUlRecYAHzzzTd1tjVv3pyjRo0q8/cuXryYTZs2lX+Ojo6miYkJr169Km87fPgwAfDcuXOl/o4hQ4awR48e8s/t2rXjO++8o3PMxYsXaWxszD///JMkWVBQQEdHR65Zs6bMtEVHR9PPz0/++dChQ3R0dGSfPn346NEjWlpa8sCBAzqfGT58OAcMGECSTEhIIAB+//338v47d+7QzMyMn332Wal/c/369fT09KRGo5G35efn08LCgjt37pTP19XVlSqVSj7mlVdeYb9+/eSfXV1duXTp0jLPTatbt26cMGGCnDYTExNu3LhR3n/r1i1aWlrK32d2djaNjY158eJFnd8THBzMadOmPfXvCYIgCMLjxKQrQRAEQUDRnO5u3bohKSkJf/zxB7Zv345FixZh9erVGDp0qHxcUFCQzueCgoKQnJws/7xhwwasWLECmZmZuHfvHlQqFWxtbXU+4+rqiqpVq8o/+/n5ITg4GD4+PggJCUHnzp3Rp08fVKlSpdzpr1mzJrp164YvvvgCzZo1w48//oj8/Hy88sorT/zc8ePHYW1tDbVajYKCAnTr1g0ffvghzpw5gwcPHqBTp046xxcUFMDf319nW0BAgPz/EydOID8/H8HBwaX+vZSUFJw5cwY2NjY62x89eoTMzEz554YNG8LY2Fj+uUaNGjh+/PgTz0WtVmPevHn49ttvcfHiRRQUFCA/Px+WlpYAgKysLBQWFqJZs2byZ+zs7ODp6anzfajVanh4eOj87vz8fDg4ODzx7wuCIAhCaUSjWxAEQRD+n7m5OTp16oROnTph5syZeP311xEdHa3T6H6S33//HQMHDsS7776LkJAQ2NnZIT4+HkuWLNE5zsrKSudnY2Nj/PLLLzhw4AB27dqFDz74ADNmzMCff/4JNze3cqf/9ddfx+DBg7F06VJ8+eWX6Nevn9zgLIunpye2bt2KSpUqoWbNmnJQN+0c659//hnOzs46nzEzMyvzfCwsLJ749+7du4emTZvi66+/LrGveEeEiYmJzj5JkqDRaJ74uxcvXozly5dj2bJl8PHxgZWVFcaNG6czbP1p7t27B2NjYxw+fFin0Q8UTUMQBEEQhGclGt2CIAiCUIYGDRrg+++/19n2xx9/4LXXXtP5Wfvm98CBA3B1dcWMGTPk/dnZ2eX6W5IkoVWrVmjVqhVmzZoFV1dXbNmyBePHjy9xrKmpKdRqdYntXbt2hZWVFVauXIkdO3Zg3759T/27pqampc75btCgAczMzHD+/Hm0a9euXOcAAPXr14eFhQV2796N119/vcT+Jk2aYMOGDahWrVqJEQDPorTvYP/+/ejRowcGDRoEoCgQ3KlTp9CgQQMAQN26dWFiYoKDBw+idu3aAIDbt2/j1KlTaNu2LQDA398farUaV69eRZs2bf5x+gRBEARBSzS6BUEQhP+8vLw8vPLKK4iIiICvry9sbGxw6NAhLFq0CD169NA5duPGjQgICEDr1q3x9ddf46+//sLnn38OoKjBef78ecTHxyMwMBA///wztmzZ8tS//+eff2L37t3o3LkzqlWrhj///BPXrl2Dt7d3qcfXqVMHf/75J86dOwdra2vY29vDyMgIxsbGGDp0KKZNm4b69euXGAr/LGxsbDBx4kRERkZCo9GgdevWuH37Nvbv3w9bW1sMGTKk1M+Zm5tjypQpmDx5MkxNTdGqVStcu3YNaWlpGD58OAYOHIjFixejR48eiImJgYuLC7Kzs7F582ZMnjwZLi4u5UpfnTp1sG/fPvTv3x9mZmZwdHRE/fr1sWnTJhw4cABVqlTB+++/jytXrsiNbhsbGwwZMgSTJk2Cvb09qlWrhujoaBgZGckR0D08PDBw4EC89tprWLJkCfz9/XHt2jXs3r0bvr6+6Nat2z/+TgVBEIT/JhG9XBAEQfjPs7a2RvPmzbF06VK0bdsWjRo1wsyZMzFixAh8+OGHOse+++67iI+Ph6+vL9atW4e4uDi5Ude9e3dERkZi7NixaNy4MQ4cOICZM2c+9e/b2tpi37596Nq1Kzw8PBAVFYUlS5agS5cupR4/ceJEGBsbo0GDBqhatSrOnz8v7xs+fDgKCgowbNiwf/GNFJkzZw5mzpyJ+fPnw9vbG6Ghofj555+fOuR95syZmDBhAmbNmgVvb2/069cPV69eBQBYWlpi3759qF27Nnr37g1vb28MHz4cjx49eqY33zExMTh37hzq1asnD0uPiopCkyZNEBISgvbt26N69eryUmta77//PoKCghAWFoaOHTuiVatW8Pb2lpduA4qivr/22muYMGECPD090bNnT52344IgCILwLCSylPVIBEEQBEEoQZIkbNmypURDTkmSkpIQHByMCxcuyMt0CWW7f/8+nJ2dsWTJEgwfPlzfyREEQRAqIDG8XBAEQRAqgPz8fFy7dg2zZ8/GK6+8IhrcZTh69CgyMjLQrFkz3L59GzExMQBQYhqBIAiCIPyviOHlgiAIglABxMXFwdXVFbdu3cKiRYv0nRxFi42NhZ+fHzp27Ij79+8jKSkJjo6O+k6WIAiCUEGJ4eWCIAiCIAiCIAiC8JyIN92CIAiCIAiCIAiC8JyIRrcgCIIgCIIgCIIgPCei0S0IgiAIgiAIgiAIz4lodAuCIAiCIAiCIAjCcyIa3YIgCIIgCIIgCILwnIhGtyAIgiAIgiAIgiA8J6LRLQiCIAiCIAiCIAjPiWh0C4IgCIIgCIIgCMJzIhrdgiAIgiAIgiAIgvCc/B+OjwF2omifAQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "\n",
    "# Convert keys and values into usable format\n",
    "x_labels = list(loss_dict.keys())\n",
    "x_values = [float(k.strip('%')) for k in x_labels]\n",
    "y_values = [v for v in loss_dict.values()]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(x_values, y_values, marker='o', linestyle='-', color='royalblue')\n",
    "plt.xticks(x_values, x_labels, rotation=45)\n",
    "plt.xlabel('Sparsity Percentage')\n",
    "plt.ylabel('NLL Loss')\n",
    "plt.title('Loss vs Sparsity Level')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cad248a-af06-411f-a9e3-2b49f77f67cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298742c0-9f24-4ad1-be11-9d28d27fd485",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d49c142-55be-4b11-8fd0-b57a4e2948a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63234cfa-8ceb-4e06-a1f0-f5f990e4d4d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167b551a-1caf-4363-9f91-bc2e14b2d555",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e57b3c6d-d559-4e31-82af-0a70ad42b0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "class TopKGate(nn.Module):\n",
    "    \"\"\"\n",
    "    Gating network with capacity constraints, next-best assignment, and auxiliary loss\n",
    "    to minimize token drops during training.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, num_experts, top_k=2, capacity_factor=1.5):\n",
    "        super().__init__()\n",
    "        self.num_experts = num_experts\n",
    "        self.top_k = top_k\n",
    "        self.capacity_factor = capacity_factor  # 1.5 is reasonable - not too wasteful\n",
    "        self.gate = nn.Linear(input_dim, num_experts, bias=True)\n",
    "        nn.init.zeros_(self.gate.bias)  # Start with equal expert probability\n",
    "        self.noise_epsilon = 1e-2\n",
    "        \n",
    "        # Auxiliary loss weights\n",
    "        self.load_balance_weight = 0.01\n",
    "        self.drop_penalty_weight = 1.0  # Strong penalty for drops\n",
    "        self.router_balance_weight = 1e-4\n",
    "        \n",
    "    def calculate_expert_capacity(self, num_tokens):\n",
    "        \"\"\"Calculate per-expert capacity based on total tokens\"\"\"\n",
    "        base_capacity = num_tokens // self.num_experts\n",
    "        return int(base_capacity * self.capacity_factor)\n",
    "    \n",
    "    def assign_tokens_with_capacity(self, gate_probs, expert_capacity):\n",
    "        \"\"\"\n",
    "        Assign tokens to experts with capacity constraints using next-best strategy\n",
    "        \n",
    "        Returns:\n",
    "            assignments: tensor of shape (num_tokens,) with expert indices (-1 for dropped)\n",
    "            expert_masks: tensor of shape (num_tokens, num_experts) for routing\n",
    "            assignment_probs: tensor of shape (num_tokens,) with routing probabilities\n",
    "        \"\"\"\n",
    "        num_tokens, num_experts = gate_probs.shape\n",
    "        expert_counts = torch.zeros(num_experts, device=gate_probs.device, dtype=torch.long)\n",
    "        assignments = torch.full((num_tokens,), -1, device=gate_probs.device, dtype=torch.long)\n",
    "        assignment_probs = torch.zeros(num_tokens, device=gate_probs.device)\n",
    "        \n",
    "        # Sort all tokens by their maximum gate probability (process confident assignments first)\n",
    "        max_probs, _ = gate_probs.max(dim=1)\n",
    "        sorted_token_indices = torch.argsort(max_probs, descending=True)\n",
    "        dropped_tokens_count = 0\n",
    "        \n",
    "        for token_idx in sorted_token_indices:\n",
    "            token_probs = gate_probs[token_idx]\n",
    "            # Get all experts sorted by preference (not just top-k)\n",
    "            expert_preferences = torch.argsort(token_probs, descending=True)\n",
    "            \n",
    "            assigned = False\n",
    "            \n",
    "            # First, try the top-k experts\n",
    "            for rank, expert_idx in enumerate(expert_preferences[:self.top_k]):\n",
    "                if expert_counts[expert_idx] < expert_capacity:\n",
    "                    assignments[token_idx] = expert_idx\n",
    "                    assignment_probs[token_idx] = token_probs[expert_idx]\n",
    "                    expert_counts[expert_idx] += 1\n",
    "                    assigned = True\n",
    "                    break\n",
    "            \n",
    "            # If top-k experts are all full, try next-best available experts\n",
    "            if not assigned:\n",
    "                dropped_tokens_count += 1\n",
    "                for expert_idx in expert_preferences[self.top_k:]:\n",
    "                    if expert_counts[expert_idx] < expert_capacity:\n",
    "                        assignments[token_idx] = expert_idx\n",
    "                        assignment_probs[token_idx] = token_probs[expert_idx]\n",
    "                        expert_counts[expert_idx] += 1\n",
    "                        assigned = True\n",
    "                        break\n",
    "            \n",
    "            # If still not assigned, token will be dropped (assignments[token_idx] remains -1)\n",
    "        \n",
    "        # Create expert masks for routing\n",
    "        expert_masks = torch.zeros(num_tokens, num_experts, device=gate_probs.device)\n",
    "        valid_assignments = assignments >= 0\n",
    "        expert_masks[valid_assignments, assignments[valid_assignments]] = 1.0\n",
    "        \n",
    "        return assignments, expert_masks, assignment_probs, expert_counts,dropped_tokens_count\n",
    "    \n",
    "    def calculate_auxiliary_losses(self, gate_probs, assignments, expert_counts,dropped_tokens_count):\n",
    "        \"\"\"Calculate all auxiliary losses\"\"\"\n",
    "        num_tokens = gate_probs.shape[0]\n",
    "        \n",
    "        # 1. Load Balance Loss (encourage uniform expert usage)\n",
    "        mean_gate = gate_probs.mean(dim=0)\n",
    "        cv_squared = gate_probs.var(dim=0) / (mean_gate + 1e-8)\n",
    "        load_balance_loss = cv_squared.mean()\n",
    "        \n",
    "        # 2. Drop Penalty Loss (heavily penalize dropped tokens)\n",
    "        num_dropped = (assignments == -1).sum().float()\n",
    "        drop_rate = dropped_tokens_count / num_tokens\n",
    "        drop_penalty_loss = drop_rate  # Linear penalty on drop rate\n",
    "        \n",
    "        # 3. Capacity Utilization Loss (encourage using available capacity)\n",
    "        target_capacity = num_tokens // self.num_experts\n",
    "        capacity_diff = expert_counts.float() - target_capacity\n",
    "        capacity_imbalance = (capacity_diff ** 2).mean()\n",
    "        \n",
    "        # 4. Router Z-Loss (from PaLM paper - prevents logit scale explosion)\n",
    "        router_z_loss = torch.logsumexp(gate_probs, dim=-1).pow(2).mean()\n",
    "        \n",
    "        return {\n",
    "            'load_balance_loss': load_balance_loss,\n",
    "            'drop_penalty_loss': drop_penalty_loss,\n",
    "            'capacity_imbalance': capacity_imbalance,\n",
    "            'router_z_loss': router_z_loss,  # Small weight\n",
    "            'drop_rate': drop_rate,\n",
    "            'num_dropped': num_dropped.item(),\n",
    "            'num_dropped_pre_next_best': dropped_tokens_count\n",
    "        }\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass with capacity constraints\n",
    "        \n",
    "        Returns:\n",
    "            expert_masks: (batch_size * seq_len, num_experts) - binary masks for routing\n",
    "            assignment_probs: (batch_size * seq_len,) - probabilities for assigned experts\n",
    "            aux_losses: dict of auxiliary losses and metrics\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, input_dim = x.shape\n",
    "        num_tokens = batch_size * seq_len\n",
    "        x_flat = x.view(-1, input_dim)\n",
    "        \n",
    "        # Calculate expert capacity\n",
    "        expert_capacity = self.calculate_expert_capacity(num_tokens)\n",
    "        \n",
    "        # Gate computation\n",
    "        gate_logits = self.gate(x_flat)\n",
    "        \n",
    "        # Add noise during training for exploration\n",
    "        if self.training:\n",
    "            gate_logits += torch.randn_like(gate_logits) * self.noise_epsilon\n",
    "        \n",
    "        gate_probs = F.softmax(gate_logits, dim=-1)\n",
    "        \n",
    "        # Assign tokens to experts with capacity constraints\n",
    "        assignments, expert_masks, assignment_probs, expert_counts, dropped_tokens_count = self.assign_tokens_with_capacity(gate_probs, expert_capacity)\n",
    "        \n",
    "        # Calculate auxiliary losses\n",
    "        aux_losses = self.calculate_auxiliary_losses(gate_probs, assignments, expert_counts,dropped_tokens_count)\n",
    "        effective_lb_weight = self.load_balance_weight * (self.top_k / 2) # Scale based on top_k\n",
    "\n",
    "        # Combine losses with weights\n",
    "        total_aux_loss = (\n",
    "            effective_lb_weight * aux_losses['load_balance_loss'] +\n",
    "            self.drop_penalty_weight * aux_losses['drop_penalty_loss'] +\n",
    "             self.router_balance_weight * aux_losses['router_z_loss']\n",
    "        )\n",
    "        aux_losses['total_aux_loss'] = total_aux_loss\n",
    "        # Monitor drop rates\n",
    "        # if aux_losses['drop_rate'] > 0.05:  # More than 5% drops\n",
    "        #     print(f\"Warning: High drop rate {aux_losses['drop_rate']:.2%}\")\n",
    "        \n",
    "        return expert_masks, assignment_probs, aux_losses\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# class SparseMoE(nn.Module):\n",
    "#     \"\"\"Sparse Mixture of Experts Layer\"\"\"\n",
    "#     def __init__(self, input_dim, head_size, output_dim, num_experts=8, top_k=2, \n",
    "#                  capacity_factor=1.5, dropout=0.1):\n",
    "#         super(SparseMoE, self).__init__()\n",
    "#         self.num_experts = num_experts\n",
    "#         self.top_k = top_k\n",
    "#         self.input_dim = input_dim\n",
    "#         self.output_dim = output_dim\n",
    "        \n",
    "#         # Create expert networks\n",
    "#         self.experts = nn.ModuleList([\n",
    "#             Expert(head_size, output_dim) \n",
    "#             for _ in range(num_experts)\n",
    "#         ])\n",
    "        \n",
    "#         # Gating network\n",
    "#         self.gate = TopKGate(input_dim, num_experts, top_k, capacity_factor)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         batch_size, seq_len, input_dim = x.shape\n",
    "#         x_flat = x.view(-1, input_dim)  # (batch_size * seq_len, input_dim)\n",
    "        \n",
    "#         # Get routing decisions from gate\n",
    "#         # expert_indices, expert_weights, balance_loss = self.gate(x)\n",
    "#         expert_masks, assignment_probs, aux_losses = self.gate(x)\n",
    "        \n",
    "#         # Initialize output\n",
    "#         output = torch.zeros(x_flat.shape[0], self.output_dim, \n",
    "#                            device=x.device, dtype=x.dtype)\n",
    "\n",
    "        \n",
    "#         # Process tokens through assigned experts\n",
    "#         for expert_idx, expert in enumerate(self.experts):\n",
    "#             # Find tokens assigned to this expert\n",
    "#             expert_tokens = expert_masks[:, expert_idx] > 0\n",
    "#             if expert_tokens.any():\n",
    "#                 # Get tokens for this expert\n",
    "#                 tokens = x_flat[expert_tokens]\n",
    "#                 # Process through expert\n",
    "#                 expert_output = expert(tokens)\n",
    "#                 # Weight by assignment probability\n",
    "#                 weights = assignment_probs[expert_tokens].unsqueeze(-1)\n",
    "#                 # Add to final output\n",
    "#                 output[expert_tokens] += expert_output * weights\n",
    "        \n",
    "#         return output.view(batch_size, seq_len, -1), aux_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7651987-a8d5-454e-a0d2-b5d1e2e38a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SparseMoE(nn.Module):\n",
    "    \"\"\"Sparse Mixture of Experts Layer\"\"\"\n",
    "    def __init__(self, input_dim, head_size, output_dim, num_experts=8, top_k=2, \n",
    "                 capacity_factor=1.5, dropout=0.1):\n",
    "        super(SparseMoE, self).__init__()\n",
    "        self.num_experts = num_experts\n",
    "        self.top_k = top_k\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        # Create expert networks\n",
    "        self.experts = nn.ModuleList([\n",
    "            Expert(head_size, output_dim) \n",
    "            for _ in range(num_experts)\n",
    "        ])\n",
    "        \n",
    "        # Gating network\n",
    "        self.gate = TopKGate(input_dim, num_experts, top_k, capacity_factor)\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        batch_size, seq_len, input_dim = x.shape\n",
    "        \n",
    "        \n",
    "        x_flat = x.view(-1, input_dim)  # (batch_size * seq_len, input_dim)\n",
    "        \n",
    "        # Get routing decisions from gate\n",
    "        expert_masks, expert_weights, aux_losses = self.gate(x)\n",
    "        \n",
    "        # Initialize output\n",
    "        output = torch.zeros(x_flat.shape[0], self.output_dim, \n",
    "                           device=x.device, dtype=x.dtype)\n",
    "        \n",
    "        # # Process each expert\n",
    "        # for i, expert in enumerate(self.experts):\n",
    "        #     # Find tokens assigned to this expert\n",
    "\n",
    "\n",
    "\n",
    "        for expert_idx, expert in enumerate(self.experts):\n",
    "            # Find tokens assigned to this expert\n",
    "            expert_tokens = expert_masks[:, expert_idx] > 0\n",
    "            # Create binary mask for the expert\n",
    "            binary_mask =expert_masks[:,expert_idx].float().unsqueeze(-1) # expert_masks[:, expert_idx] > 0 #(expert_indices == i).any(dim=-1).float().unsqueeze(-1)  # [128, 1]\n",
    "\n",
    "\n",
    "            # if expert_tokens.any():\n",
    "            #     # Get tokens for this expert\n",
    "            #     print(binary_mask.shape)\n",
    "            #     print(x_flat.shape)\n",
    "            #     tokens =  x_flat * binary_mask #x_flat[expert_tokens]\n",
    "            #     # Process through expert\n",
    "            #     expert_output = expert(tokens)\n",
    "            #     print(tokens.shape)\n",
    "            #     # Weight by assignment probability\n",
    "            #     weights = assignment_probs[expert_tokens].unsqueeze(-1)\n",
    "            #     # Add to final output\n",
    "            #     output[expert_tokens] += expert_output * weights\n",
    "        \n",
    "            \n",
    "            if expert_tokens.any():\n",
    "                # Get tokens for this expert\n",
    "                # expert_input = x_flat[expert_mask]\n",
    "                # Apply the mask to keep shape\n",
    "                expert_input = x_flat * binary_mask # [128, 64]\n",
    "                # print(expert_indices.shape)\n",
    "                # print(x_flat.shape)\n",
    "                # print(expert_input.shape)\n",
    "                \n",
    "                # Get weights for this expert\n",
    "                token_indices = torch.where(expert_tokens)[0]\n",
    "                # print(expert_weights.shape)\n",
    "                expert_output = expert(expert_input.view(batch_size, seq_len, self.output_dim))\n",
    "                expert_output = expert_output.view(-1, self.output_dim)\n",
    "                weights = (expert_weights.unsqueeze(-1) * binary_mask) * expert_output\n",
    "                output[expert_tokens] += weights[expert_tokens] \n",
    "                \n",
    "                \n",
    "                # # Weight the expert output\n",
    "                # for j, token_idx in enumerate(token_indices):\n",
    "                #     # Find which position this expert is in for this token\n",
    "                #     expert_positions = (expert_indices[token_idx] == i).nonzero(as_tuple=True)[0]\n",
    "                    \n",
    "                #     if len(expert_positions) > 0:\n",
    "                #         weight = expert_weights[token_idx, expert_positions[0]]\n",
    "                #         output[token_idx] += weight * expert_output[j]\n",
    "        \n",
    "        # Reshape back to original shape\n",
    "        output = output.view(batch_size, seq_len, self.output_dim)\n",
    "        \n",
    "        return output, aux_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b486110-79f1-4e0a-9c95-0692473bd70e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.213601 M parameters\n",
      "Gate parameters: 520\n",
      "Dense MultiHeadAttention parameters: 16,448\n",
      "Equivalent experts: 2.1\n",
      "Total MoE parameters from model: 17,416\n",
      "Parameters For all experts from model (layer 0): 16,896\n",
      "Parameters per expert (layer 0) from model: 2,112\n",
      "At Sparsity 100.0%, Loss at 2 is: 4.331852912902832\n",
      "Parameters of Top 0 out of 8 Experts with Drop Rate of 0.0: 0\n",
      "At Sparsity 87.5%, Loss at 2 is: 3.9932351112365723\n",
      "Parameters of Top 1 out of 8 Experts with Drop Rate of 0.0009765625: 2,112\n",
      "At Sparsity 75.0%, Loss at 2 is: 3.648714303970337\n",
      "Parameters of Top 2 out of 8 Experts with Drop Rate of 0.01220703125: 4,224\n",
      "At Sparsity 62.5%, Loss at 2 is: 3.580791473388672\n",
      "Parameters of Top 3 out of 8 Experts with Drop Rate of 0.01513671875: 6,336\n",
      "At Sparsity 50.0%, Loss at 2 is: 3.506544351577759\n",
      "Parameters of Top 4 out of 8 Experts with Drop Rate of 0.0166015625: 8,448\n",
      "At Sparsity 37.5%, Loss at 2 is: 3.3448872566223145\n",
      "Parameters of Top 5 out of 8 Experts with Drop Rate of 0.01513671875: 10,560\n",
      "At Sparsity 25.0%, Loss at 2 is: 3.1706526279449463\n",
      "Parameters of Top 6 out of 8 Experts with Drop Rate of 0.0166015625: 12,672\n",
      "At Sparsity 12.5%, Loss at 2 is: 3.2596991062164307\n",
      "Parameters of Top 7 out of 8 Experts with Drop Rate of 0.0302734375: 14,784\n",
      "At Sparsity 0.0%, Loss at 2 is: 3.1490249633789062\n",
      "Parameters of Top 8 out of 8 Experts with Drop Rate of 0.03369140625: 16,896\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "max_iters = 2#000\n",
    "eval_interval = 100\n",
    "learning_rate =  1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 50\n",
    "n_embd = 64\n",
    "num_experts = n_head = 8\n",
    "top_k =3\n",
    "#n_head = 2\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "\n",
    "\n",
    "training = True\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss,total_loss,_ = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "class Expert(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size,output_dim):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.proj = nn.Linear(head_size, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        out = self.proj(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "# class MultiHeadAttention(nn.Module):\n",
    "#     \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "#     def __init__(self, num_heads, head_size):\n",
    "#         super().__init__()\n",
    "#         self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "#         self.proj = nn.Linear(n_embd, n_embd)\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "#         out = self.dropout(self.proj(out))\n",
    "#         return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = SparseMoE(n_embd, head_size, n_embd, num_experts, top_k, dropout=dropout)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        moe_out, aux_metrics = self.sa(self.ln1(x))\n",
    "        x = x + moe_out\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x, aux_metrics\n",
    "\n",
    "\n",
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.ModuleList([Block(n_embd, n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        \n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        total_aux_loss = 0.0\n",
    "        aux_metrics = {'drop_rate': 0.0, 'load_balance': 0.0, 'total_aux_loss': 0.0}\n",
    "\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            x, block_metrics = block(x)\n",
    "            layer_weight = (i + 1) / len(self.blocks)\n",
    "            total_aux_loss += (block_metrics['total_aux_loss'] * layer_weight)\n",
    "            \n",
    "            # Optionally track per-block metrics\n",
    "            aux_metrics['drop_rate'] += block_metrics['drop_rate']\n",
    "            aux_metrics['load_balance'] += block_metrics['load_balance_loss']\n",
    "        \n",
    "        # Average metrics across blocks\n",
    "        aux_metrics = {k: v / len(self.blocks) for k, v in aux_metrics.items()}\n",
    "        aux_metrics['total_aux_loss'] = total_aux_loss\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "            total_loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            total_loss = loss + total_aux_loss #aux_metrics['total_aux_loss']\n",
    "\n",
    "        return logits, loss,total_loss,aux_metrics\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss,total_loss,_ = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "    # Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters()) / 1e6\n",
    "\n",
    "expert_params_model = sum(p.numel() for expert in model.blocks[0].sa.experts for p in expert.parameters()) \n",
    "total_moe_params_model = sum(p.numel() for p in model.blocks[0].sa.parameters()) \n",
    "gate_params = sum(p.numel() for p in model.blocks[0].sa.gate.parameters()) \n",
    "\n",
    "\n",
    "# Dense FFN equivalent\n",
    "dense_ffn_params = n_embd + (4 * (n_embd**2))\n",
    "\n",
    "print(f\"Gate parameters: {gate_params:,}\")\n",
    "\n",
    "print(f\"Dense MultiHeadAttention parameters: {dense_ffn_params:,}\")\n",
    "print(f\"Equivalent experts: {total_moe_params / dense_ffn_params:.1f}\")\n",
    "\n",
    "print(f\"Total MoE parameters from model: {total_moe_params_model:,}\")\n",
    "print(f\"Parameters For all experts from model (layer 0): {expert_params_model:,}\")\n",
    "print(f\"Parameters per expert (layer 0) from model: {expert_params_model // num_experts:,}\")\n",
    "\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "loss_dict = {}\n",
    "for i in range(num_experts+1):\n",
    "    top_k = i\n",
    "    \n",
    "    for iter in range(max_iters):\n",
    "    \n",
    "        # every once in a while evaluate the loss on train and val sets\n",
    "        if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "            losses = estimate_loss()\n",
    "            # print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "    \n",
    "        # sample a batch of data\n",
    "        xb, yb = get_batch('train')\n",
    "    \n",
    "        # evaluate the loss\n",
    "        logits, loss, total_loss,aux_metrics = model(xb, yb)\n",
    "        \n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    sparsity = round(((num_experts-top_k)/num_experts) * 100, 2)\n",
    "    drop_rate = aux_metrics['drop_rate']\n",
    "    print(f\"At Sparsity {sparsity}%, Loss at {max_iters} is: {loss}\")\n",
    "    expert_params = sum(p.numel() for expert in model.blocks[0].sa.experts for p in expert.parameters()) \n",
    "    print(f\"Parameters of Top {top_k} out of {num_experts} Experts with Drop Rate of {drop_rate}: {(expert_params // num_experts)*top_k:,}\")\n",
    "\n",
    "    loss_dict[f\"{sparsity}%\"] = loss.item()\n",
    "\n",
    "# generate from the model\n",
    "training = False\n",
    "# context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "# print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fbe99eba-21b8-4829-baf3-0ab2ccc2d26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum(p.numel() for expert in model.blocks[0].sa.experts for p in expert.parameters()) "
   ]
  },
  {
   "cell_type": "raw",
   "id": "9f1da581-0e7c-4a8d-80eb-ac3045cfb70a",
   "metadata": {},
   "source": [
    "step 4999: train loss 1.7855, val loss 1.9239\n",
    "\n",
    "when i used 16, 32 for batch, block and top_k for n_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14b48a72-25aa-463a-b6c0-3c9d6de581db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 4999: train loss 1.6876, val loss 1.8470\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbd6bc1-1586-4e58-9315-332c70435e24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51648a9b-434b-425f-9b7c-527e7126f00e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "f554dba1-3e38-45bf-870e-7eb96dec1098",
   "metadata": {},
   "source": [
    "0.209729 M parameters\n",
    "step 0: train loss 4.4322, val loss 4.4217\n",
    "step 100: train loss 2.6608, val loss 2.6740\n",
    "step 200: train loss 2.5182, val loss 2.5130\n",
    "step 300: train loss 2.4444, val loss 2.4571\n",
    "step 400: train loss 2.3800, val loss 2.3874\n",
    "step 500: train loss 2.3222, val loss 2.3391\n",
    "step 600: train loss 2.2744, val loss 2.2834\n",
    "step 700: train loss 2.2368, val loss 2.2462\n",
    "step 800: train loss 2.1992, val loss 2.2211\n",
    "step 900: train loss 2.1566, val loss 2.1773\n",
    "step 1000: train loss 2.1302, val loss 2.1523\n",
    "step 1100: train loss 2.1031, val loss 2.1443\n",
    "step 1200: train loss 2.0650, val loss 2.1034\n",
    "step 1300: train loss 2.0516, val loss 2.0864\n",
    "step 1400: train loss 2.0233, val loss 2.0625 \n",
    "step 1500: train loss 2.0015, val loss 2.0590\n",
    "step 1600: train loss 1.9828, val loss 2.0666\n",
    "step 1700: train loss 1.9715, val loss 2.0396\n",
    "step 1800: train loss 1.9397, val loss 2.0315\n",
    "step 1900: train loss 1.9304, val loss 2.0042\n",
    "step 2000: train loss 1.9170, val loss 2.0222\n",
    "step 2100: train loss 1.8990, val loss 2.0080\n",
    "step 2200: train loss 1.8903, val loss 1.9822\n",
    "step 2300: train loss 1.8812, val loss 1.9796\n",
    "step 2400: train loss 1.8659, val loss 1.9604\n",
    "step 2500: train loss 1.8386, val loss 1.9613\n",
    "step 2600: train loss 1.8481, val loss 1.9597\n",
    "step 2700: train loss 1.8395, val loss 1.9540\n",
    "step 2800: train loss 1.8335, val loss 1.9526\n",
    "step 2900: train loss 1.8272, val loss 1.9535\n",
    "step 3000: train loss 1.8169, val loss 1.9323\n",
    "step 3100: train loss 1.7951, val loss 1.9434\n",
    "step 3200: train loss 1.7780, val loss 1.9211\n",
    "step 3300: train loss 1.7752, val loss 1.9180\n",
    "step 3400: train loss 1.7771, val loss 1.9159\n",
    "step 3500: train loss 1.7656, val loss 1.9041\n",
    "step 3600: train loss 1.7489, val loss 1.9075\n",
    "step 3700: train loss 1.7526, val loss 1.9034\n",
    "step 3800: train loss 1.7475, val loss 1.9081\n",
    "step 3900: train loss 1.7466, val loss 1.8903\n",
    "step 4000: train loss 1.7247, val loss 1.8687\n",
    "step 4100: train loss 1.7319, val loss 1.8830\n",
    "step 4200: train loss 1.7245, val loss 1.8810\n",
    "step 4300: train loss 1.7286, val loss 1.8705\n",
    "step 4400: train loss 1.7258, val loss 1.8805\n",
    "step 4500: train loss 1.7152, val loss 1.8680\n",
    "step 4600: train loss 1.7032, val loss 1.8496\n",
    "step 4700: train loss 1.7033, val loss 1.8611\n",
    "step 4800: train loss 1.6913, val loss 1.8515\n",
    "step 4900: train loss 1.6910, val loss 1.8610\n",
    "step 4999: train loss 1.6876, val loss 1.8470\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1501daf7-ef20-4415-9979-f2d1dbe4065d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a53443-7ee4-4d83-82c2-b91c4b776f2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2a32d9-8554-4a97-a6cc-3c2c891e8caa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a047548e-5f10-4e29-99d8-b0224cc4ccc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b0379b-be2d-4386-8d8b-4e4f0db00842",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c8586b-c511-468b-b62e-ede78951ba21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba4faa7-4450-4ce2-9f3e-5af9dd4be12c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bd273f-93ce-474a-b1b2-c3ca9a3aa303",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
